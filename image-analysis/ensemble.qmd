---
format: 
  html: default
title: "Ensemble Image Classification"
date: 2024-1-22
author:
  - name: Michael Achmann-Denkler
    id: ma
    orcid: 0000-0002-4754-7842
    email: michael.achmann@informatik.uni-regensburg.de
license: CC BY
bibliography: ../literature.bib
notebook-view:
  - notebook: ../notebooks/2024_01_22_Ensemble_Classification.ipynb
    title: "Ensemble Classification Notebook"
citation:
  type: webpage
  doi: 10.5281/zenodo.10039756
---

In this chapter, we explore an approach known as [ensemble classification](https://en.wikipedia.org/wiki/Ensemble_learning), which I have just been experimenting with. At this stage, the methodology is in an early phase, with comprehensive evaluations and literature comparisons yet to be conducted.


The core concept of our ensemble method is to synthesize a rich set of inputs for the GPT model, specifically tailored to enhance its ability to classify images without direct visual analysis. In the example below we achieve this by integrating *Captions*, *Objects*, and *OCR* (Optical Character Recognition) data, each element extracted and processed by distinct, specialized models. This preparatory processing transforms the raw image into a multi-faceted textual representation, compatible with LLMs. The power of our ensemble approach comes from combining different types of data. By doing so, we leverage the specific strengths of each model used in the process.

The idea behind our ensemble approach here is to provide the GPT model with *Captions*, *Objects*, and *OCR* detected and generated by different models. The final classification using the GPT prompt below relies solely on the outputs of the previous models. In other words: The final classification takes places without the actual image. This approach complements image classification approaches based on GPT and CLIP introduced in the [image classification](classification.qmd) chapter.

The process is illustrated in the following schematic, detailing the sequential flow from image to classification:

![A schematic overview of the ensemble classification process.](../images/Ensemble-Classification.png)

In practice, this approach has shown promising consistency, particularly when compared to methods relying solely on CLIP for image classification, as discussed in the [image classification](classification.qmd) chapter. Nevertheless, as the example below illustrates, the technique is not perfect. My first trials with GPT-3.5 yielded suboptimal results, thus I recommend employing GPT-4 for its advanced capabilities.

An advantage -- especially in contrast to multimodal LLMs -- of this ensemble method is the control it gives over the input variables. Depending on the specific requirements of your analysis, you can fine-tune the focus of the ensemble. For instance, if textual content is important, emphasizing OCR might be beneficial. Conversely, if the context provided by objects within the image is more relevant, then prioritizing object detection would be advantageous. This flexibility allows for tailored, context-sensitive classifications.

To explore the application of this technique, the following notebook will guide you through the steps necessary to extract *Captions*, *Objects*, and *OCR* using Google Vision APIs. This hands-on approach aims not just to illustrate the theory but also to equip you with practical skills in implementing this innovative classification strategy.

## Notebook
{{< embed ../notebooks/2024_01_22_Ensemble_Classification.ipynb  echo=true >}}

## Conclusion

In wrapping up this chapter, we see that ensemble classification offers a new perspective in analyzing visual social media. By combining *Captions*, *Objects*, and *OCR*, we create a more versatile tool for image classification with GPT models, especially useful when we can't rely on direct image analysis. A key advantage of this approach is its flexibility. You can adjust the focus to suit your project's needs, whether that's on text with OCR or objects in the image.