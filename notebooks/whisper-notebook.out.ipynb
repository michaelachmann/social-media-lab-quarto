{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \n",
        "\n",
        "**Extract Audio from Video File**\n",
        "\n",
        "After loading the metadta and media files from the Google Drive, we\n",
        "extract the audio from each video file to prepare the automated\n",
        "transcription."
      ],
      "id": "370ee175-d780-4bfd-8d8a-b2bb642f7c3c"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q moviepy"
      ],
      "id": "cell-1"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set audio directory path\n",
        "audio_path = \"media/audio/\"\n",
        "\n",
        "# Check if the directory exists\n",
        "if not os.path.exists(audio_path):\n",
        "    # Create the directory if it does not exist\n",
        "    os.makedirs(audio_path)"
      ],
      "id": "cell-2"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "outputId": "5dcf27b3-c6d4-4548-f9e4-657635c9bac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in media/audio/CzD93SEIi-E.mp3"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ],
      "source": [
        "from moviepy.editor import *\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if row['video_file'] != \"\":\n",
        "        # Load the video file\n",
        "        video = VideoFileClip(row['video_file'])\n",
        "        filename = row['video_file'].split('/')[-1]\n",
        "\n",
        "        # Extract the audio from the video file\n",
        "        audio = video.audio\n",
        "\n",
        "        if audio is not None:\n",
        "            sampling_rate = audio.fps\n",
        "            current_suffix = filename.split(\".\")[-1]\n",
        "            new_filename = filename.replace(current_suffix, \"mp3\")\n",
        "\n",
        "            # Save the audio to a file\n",
        "            audio.write_audiofile(\"{}{}\".format(audio_path, new_filename))\n",
        "        else:\n",
        "            new_filename = \"No Audio\"\n",
        "            sampling_rate = -1\n",
        "\n",
        "        # Update DataFrame inplace\n",
        "        df.at[index, 'audio_file'] = new_filename\n",
        "        df.at[index, 'duration'] = video.duration\n",
        "        df.at[index, 'sampling_rate'] = sampling_rate\n",
        "\n",
        "        df.at[index, 'video_file'] = row['video_file'].split('/')[-1]\n",
        "\n",
        "        # Close the video file\n",
        "        video.close()\n"
      ],
      "id": "cell-3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We’ve extracted the audio content of each video file to a `mp3` file in\n",
        "the `media/audio` folder. The files keep the name of the video file. We\n",
        "added new columns to the metadata for audio duration and sampling_rate.\n",
        "In case the video did not include an audio file, `smapling_rate`is set\n",
        "to `-1`, which we use to filter the `df` when transcribing the files."
      ],
      "id": "fb15876e-d3d1-4771-b406-aa4537c23bf7"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "outputId": "46950938-f26d-4c6d-b933-dcefdcc23c26"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>1 rows × 24 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8aec1eb9-74d8-4d05-a892-6603f34aa7e1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8aec1eb9-74d8-4d05-a892-6603f34aa7e1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8aec1eb9-74d8-4d05-a892-6603f34aa7e1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>"
            ]
          }
        }
      ],
      "source": [
        "df[df['video_file'] != \"\"].head()"
      ],
      "id": "cell-5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s update the `ZIP`ed folder to include the audio files."
      ],
      "id": "54426645-c3e7-4e7e-8373-1f3dcf497cea"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "outputId": "d5a1ced4-f1aa-4bab-fdb7-1363c61cfcdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: media/ (stored 0%)\n",
            "updating: media/videos/ (stored 0%)\n",
            "updating: media/videos/CzD93SEIi-E.mp4 (deflated 0%)\n",
            "  adding: media/audio/ (stored 0%)\n",
            "  adding: media/audio/CzD93SEIi-E.mp3 (deflated 1%)"
          ]
        }
      ],
      "source": [
        "!zip -r /content/drive/MyDrive/2023-11-24-4CAT-Images-Clean.zip media"
      ],
      "id": "cell-7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And save the updated metadata file. **Change filename when importing\n",
        "stories here!**"
      ],
      "id": "e1045722-1d05-47ff-b5db-e22be4bb6677"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(four_cat_file_path)"
      ],
      "id": "cell-9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Transcriptions using Whisper**\n",
        "\n",
        "> The Whisper model was proposed in Robust Speech Recognition via\n",
        "> Large-Scale Weak Supervision by Alec Radford, Jong Wook Kim, Tao Xu,\n",
        "> Greg Brockman, Christine McLeavey, Ilya Sutskever.\n",
        "\n",
        "> The abstract from the paper is the following:\n",
        "\n",
        "> > We study the capabilities of speech processing systems trained\n",
        "> > simply to predict large amounts of transcripts of audio on the\n",
        "> > internet. When scaled to 680,000 hours of multilingual and multitask\n",
        "> > supervision, the resulting models generalize well to standard\n",
        "> > benchmarks and are often competitive with prior fully supervised\n",
        "> > results but in a zeroshot transfer setting without the need for any\n",
        "> > finetuning. When compared to humans, the models approach their\n",
        "> > accuracy and robustness. We are releasing models and inference code\n",
        "> > to serve as a foundation for further work on robust speech\n",
        "> > processing.\n",
        "\n",
        "– https://huggingface.co/docs/transformers/model_doc/whisper"
      ],
      "id": "add6cc6c-c755-4e36-82e7-75bc56bee83e"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ],
      "id": "cell-11"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next code snippet initializes the Whisper model. The\n",
        "`transcribe_aduio` method is applied to each row of the dataframe where\n",
        "`sampling_rate` \\> `0`, thus only to those lines with referencees to\n",
        "audio files. Each audio file is transcribed using Whisper, the result,\n",
        "one text string, is saved to the `transcript` column.\n",
        "\n",
        "**Adjust the language variable according to your needs!** The model is\n",
        "also capable of automated translation, e.g. setting `language` to\n",
        "english when processing German content results in an English translation\n",
        "of the speech. (Additionally, the `task` variable accepts `translate`)."
      ],
      "id": "a09bccbc-67ec-40b2-a5ad-99334e747bd0"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "outputId": "8400ca21-c56c-422f-c4ae-c1f036647d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import pipeline, WhisperProcessor, WhisperForConditionalGeneration\n",
        "import librosa\n",
        "\n",
        "# Set device to GPU if available, else use CPU\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize the Whisper model pipeline for automatic speech recognition\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-large\",\n",
        "    chunk_length_s=30,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# Load model and processor for multilingual support\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n",
        "\n",
        "# Function to read, transcribe, and handle longer audio files in different languages\n",
        "def transcribe_audio(filename, language='german'):\n",
        "    try:\n",
        "        # Load and resample audio file\n",
        "        audio_path = f\"{audio_folder}/{filename}\"\n",
        "        waveform, original_sample_rate = librosa.load(audio_path, sr=None, mono=True)\n",
        "        waveform_resampled = librosa.resample(waveform, orig_sr=original_sample_rate, target_sr=16000)\n",
        "\n",
        "        # Get forced decoder IDs for the specified language\n",
        "        forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=\"transcribe\")\n",
        "\n",
        "        # Process the audio file in chunks and transcribe\n",
        "        transcription = \"\"\n",
        "        for i in range(0, len(waveform_resampled), 16000 * 30):  # 30 seconds chunks\n",
        "            chunk = waveform_resampled[i:i + 16000 * 30]\n",
        "            input_features = processor(chunk, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
        "            predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
        "            chunk_transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "            transcription += \" \" + chunk_transcription\n",
        "\n",
        "        return transcription.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {filename}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# Filter the DataFrame (sampling_rates < 0 identify items without audio)\n",
        "filtered_index = df['sampling_rate'] > 0\n",
        "\n",
        "# Apply the transcription function to each row in the filtered DataFrame\n",
        "df.loc[filtered_index, 'transcript'] = df.loc[filtered_index, 'audio_file'].apply(transcribe_audio)"
      ],
      "id": "cell-13"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "outputId": "6ba00b74-ad5e-4def-9bbb-d5d69ac399be"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>1 rows × 25 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f0098e61-844a-4b08-aeba-50efc2e0231d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f0098e61-844a-4b08-aeba-50efc2e0231d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f0098e61-844a-4b08-aeba-50efc2e0231d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>"
            ]
          }
        }
      ],
      "source": [
        "df[df['video_file'] != \"\"].head()"
      ],
      "id": "cell-14"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "outputId": "547cdf2d-3e04-441c-d99f-4ea291390324"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'Ich bitte auf den abgelagerten Vortrag der Maaßen-Söder-Entfühlen ein.  Erfüllung meiner Amtspflichten, so wahr mir Gott helfe. Ich schwöre Treue der Verfassung des Freistaates Bayern, Gehorsam den Gesetzen und gewissenhafte Erfüllung meiner Amtspflichten, so wahr mir Gott helfe. Herr Ministerpräsident, ich darf Ihnen im Namen des ganzen Hauses ganz persönlich die herzlichsten Glückwünsche aussprechen und wünsche Ihnen viel Erfolg und gute Nerven auch bei Ihrer Aufgabe. Herzlichen Dank.  Applaus'"
            ]
          }
        }
      ],
      "source": [
        "df.loc[4, 'transcript']"
      ],
      "id": "cell-15"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overall, the transcriptions work well. The first sentence above,\n",
        "however, shows that we still can expect misinterpretations."
      ],
      "id": "0394b63c-9681-444a-90b2-6ffdd4c11ba6"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python"
    }
  }
}