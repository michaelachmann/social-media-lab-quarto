[
  {
    "objectID": "notebooks/firebase-interface-notebook.html",
    "href": "notebooks/firebase-interface-notebook.html",
    "title": "Project Creation",
    "section": "",
    "text": "The following lines of code assume that the firebase Credential File has been downloaded from GRIPS and uploaded to Colab / your Jupyter project path. First of all install the necessary packages:\n\n!pip -q install firebase-admin\n\nNext, we connect to our firebase project. Please update the credentials_path variable with the path to your credentials file (see above).\n\nimport firebase_admin\nfrom firebase_admin import credentials, firestore\n\ncredentials_path = '/content/XXXX-adminsdk-YYYYYY.json' \n\ncred = credentials.Certificate(credentials_path)\nfirebase_admin.initialize_app(cred)\ndb = firestore.client()\n\n\nProject Creation\nPlease provide an alert_email and project_name to create a new project on the backend. The backend checks hourly when the last stories have been uploaded to a project. If no story has been uploaded for more than 12 hours, an email alert will be triggered.\nRun the cell to create the new project on the backend. When successfull, the project id and api key will be displayed.\n\nfrom IPython.display import display, Markdown\nimport pandas as pd\n\nalert_email = 'michael@achmann.me'\nproject_name = 'Forschungsseminar23 Test'\n\n# Create Project\nimport uuid\n\n# Generate a UUID for the document\nproject_id = str(uuid.uuid4())\napi_key = str(uuid.uuid4())\n\n# Your data\ndata = {\n    \"api_key\": api_key,\n    \"email\": alert_email,\n    \"name\": project_name\n}\n\n# Add a new document with a UUID as the document name (ID)\ndoc_ref = db.collection('projects').document(project_id)\ndoc_ref.set(data)\n\ndisplay(Markdown(\"### Project Created:\"))\ndisplay(Markdown(f\"**Project Name:** {project_name}\"))\ndisplay(Markdown(f\"**Alert Email:** {alert_email}\"))\ndisplay(Markdown(f\"**Project ID:** {project_id}\"))\ndisplay(Markdown(f\"**API-Key:** {api_key}\"))\n\n\nProject Created:\nProject Name: Forschungsseminar23 Test\nAlert Email: michael@achmann.me\nProject ID: 959466fe-4088-4099-a6b2-3cbe058889d3\nAPI-Key: 554fbce8-fb15-44f1-bb4d-54cdc57554f2\n\n\n\nConfigure the Plugin\nConfigure Zeeschuimer-F using the above information after creating a project. In order to access the settings of Firefox plugins click on the puzzle tile on the top right of the browser. Click on Zeeschuimer F and the settings open.\n\n\n\nScreenshot of Firefox with open extensions menu\n\n\nFill in the Firebase Project field with the project id and aFirebase API Key with the api key provided after running the Project Creation. The Firebase Endopint URL will be provided via GRIPS (unless you’ve installed your own instance).\n\n\n\nScreenshot of the Settings for Zeeschuimer-F\n\n\n1) Turn the IG Stories Switch on, 2) restart your browser for the values to be loaded correctly. Once the browser has started again, you’re ready to collect you first stories! Open the Instagram website and open any story.\n\n\n\nScreenshot of the switch\n\n\nCheck the extension settings page to see whether it is collecting stories while browsing. The counter should increase with each story visit. The remote collection process can currently only be checked through the Firebase Interface notebook. Follow the next steps to download the collected data.\n\n\nProject Export\nThe following code downloads all stories in JSON format and saves it locally (i.e. on your colab instance). Provide the PROJECT_ID variable and an export_path to download all stories.\n\nfrom tqdm.auto import tqdm\nimport os\nimport json\n\nPROJECT_ID = '959466fe-4088-4099-a6b2-3cbe058889d3'\nexport_path = '/content/export' \n\n\ndef fetch_stories(project_id):\n    stories_ref = db.collection('projects').document(project_id).collection('stories')\n    docs = stories_ref.stream()\n\n    stories = []\n    for doc in docs:\n        stories.append(doc.to_dict())\n\n    return stories\n\ndb = fetch_stories(PROJECT_ID)\n\nif not os.path.exists('export'):\n    os.makedirs('export')\n\n# Iterate over each element in the database\nfor element in tqdm(db, desc='Exporting elements'):\n    # Serialize the element to JSON\n    element_json = json.dumps(element, indent=4)\n\n    # Write to a file named {id}.json\n    with open(os.path.join('export', f\"{element['id']}.json\"), 'w') as f:\n        f.write(element_json)\n\n\n\n\n\n\nConvert to DataFrame\nNext, we convert the exported JSON files to a pandas DataFrame and save the table as CSV. Provide the df_export_path variable for the location where to save the exported CSV file.\n\n\n\n\n\n\nWork-In-Progress\n\n\n\nThe DataFrame in the current version has a different structure than the one we created when downloading Instagram Posts.. In order to compare stories with posts we will might want to use the same data structure.\n\n\n\n\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n\ndf_export_path = '/content/2022-11-09-Stories-Exported.csv' \n\ndef process_instagram_story(data):\n\n    # Extract relevant information\n    story_info = {\n        'ID': data.get(\"id\"),\n        'Time of Posting': datetime.utcfromtimestamp(data['taken_at']).strftime('%Y-%m-%d %H:%M:%S'),\n        'Type of Content': 'Video' if 'video_duration' in data else 'Image',\n        'video_url': None,\n        'image_url': None,\n        'Username': data['user']['username'],\n        'Video Length (s)': data.get('video_duration', None),\n        'Expiration': (datetime.utcfromtimestamp(data['taken_at']) + timedelta(hours=24)).strftime('%Y-%m-%d %H:%M:%S'),\n        'Caption': data.get('caption', None),\n        'Is Verified': data['user']['is_verified'],\n        'Stickers': data.get('story_bloks_stickers', []),\n        'Accessibility Caption': data.get('accessibility_caption', ''),\n        'Attribution URL': data.get('attribution_content_url', '')\n    }\n\n    return story_info\n\nrows = []\nfor element in db:\n  rows.append(process_instagram_story(element))\n\ndf = pd.DataFrame(rows)\ndf.to_csv(df_export_path)\nprint(f\"Successfully exported {len(df)} rows as CSV.\")\n\nSuccessfully exported 22 rows as CSV.\n\n\nNow let’s take a look at the structure of the exported data:\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nID\nTime of Posting\nType of Content\nvideo_url\nimage_url\nUsername\nVideo Length (s)\nExpiration\nCaption\nIs Verified\nStickers\nAccessibility Caption\nAttribution URL\n\n\n\n\n0\n3231585718932790545_1483455177\n2023-11-08 14:50:59\nImage\n&lt;NA&gt;\nhttps://storage.googleapis.com/zeeschuimer-fb-...\nrmf24.pl\nNaN\n2023-11-09 14:50:59\nNone\nFalse\n[]\nPhoto by Fakty RMF FM | Rozmowy | Podcasty on ...\n\n\n\n1\n3231585778860997221_1483455177\n2023-11-08 14:51:06\nImage\n&lt;NA&gt;\nhttps://storage.googleapis.com/zeeschuimer-fb-...\nrmf24.pl\nNaN\n2023-11-09 14:51:06\nNone\nFalse\n[]\nPhoto by Fakty RMF FM | Rozmowy | Podcasty on ...\n\n\n\n2\n3231750838597692854_1349651722\n2023-11-08 20:19:00\nVideo\nhttps://storage.googleapis.com/zeeschuimer-fb-...\n&lt;NA&gt;\ntagesschau\n13.300\n2023-11-09 20:19:00\nNone\nTrue\n[]\n\n\n\n\n3\n3231750989408058657_1349651722\n2023-11-08 20:19:18\nVideo\nhttps://storage.googleapis.com/zeeschuimer-fb-...\n&lt;NA&gt;\ntagesschau\n15.267\n2023-11-09 20:19:18\nNone\nTrue\n[]\n\n\n\n\n4\n3231751135118088390_1349651722\n2023-11-08 20:19:35\nVideo\nhttps://storage.googleapis.com/zeeschuimer-fb-...\n&lt;NA&gt;\ntagesschau\n17.000\n2023-11-09 20:19:35\nNone\nTrue\n[]\n\n\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nDownload Images and Videos\nAll videos and images for our Instagram stories have been downloaded by our firebase backend. They are saved in a Cloud Bucket. The following cell helps with these two steps:\n\nCreate a signed link to each video and image\nDownload each file and saves it in the following structure: {media_export_path}/{image|video}/{username}/{ID.jpg|mp4}. It is important to keep a unique identifier (here ID) to map metadata and images for future data analysis.\n\nPlease provide a storage_bucket and media_export_path.\n\nstorage_bucket = \"XXXX.appspot.com\"  \nmedia_export_path =  '/content/media/'\n\nfrom firebase_admin import storage\nimport os\nimport requests\n\nbucket = storage.bucket(storage_bucket)\n\ndef generate_signed_url(username, content_id, file_type):\n    if file_type not in ['images', 'videos']:\n        raise ValueError(\"Invalid file type specified\")\n\n    ext = 'jpeg' if file_type == 'images' else 'mp4'\n    blob_path = f\"projects/{PROJECT_ID}/stories/{file_type}/{username}/{content_id}.{ext}\"\n    blob = bucket.blob(blob_path)\n    # Set the expiration of the link. Here, it's set to 24 hours.\n    return blob.generate_signed_url(expiration=timedelta(hours=24), method='GET')\n\n# Create a function to be applied across DataFrame rows\ndef apply_generate_signed_url(row):\n    image_url = generate_signed_url(row['Username'], row['ID'], 'images')\n    video_url = generate_signed_url(row['Username'], row['ID'], 'videos') if row['Type of Content'] == 'Video' else pd.NA\n    return pd.Series({'image_url': image_url, 'video_url': video_url})\n\n# Apply the function along the axis=1 (row-wise)\ndf[['image_url', 'video_url']] = df.apply(apply_generate_signed_url, axis=1)\n\n# Now, creating the lists for images and videos can be done more efficiently\ndata_images = df.loc[df['image_url'].notna(), ['ID', 'image_url', 'Username', 'Time of Posting']] \\\n               .rename(columns={'image_url': 'url', 'Time of Posting': 'datetime'}) \\\n               .to_dict('records')\n\ndata_videos = df.loc[df['video_url'].notna(), ['ID', 'video_url', 'Username', 'Time of Posting']] \\\n               .rename(columns={'video_url': 'url', 'Time of Posting': 'datetime'}) \\\n               .to_dict('records')\n\n\ndef create_directories(base_path, entries, subdir):\n    usernames = set(entry['Username'] for entry in entries)\n    for username in usernames:\n        os.makedirs(os.path.join(base_path, subdir, username), exist_ok=True)\n\ndef download_file(entry, media_type, media_export_path, session):\n    directory = os.path.join(media_export_path, media_type, entry['Username'])\n    ext = 'jpg' if media_type == 'images' else 'mp4'\n    filename = os.path.join(directory, f\"{entry['ID']}.{ext}\")\n\n    with session.get(entry['url'], stream=True) as response:\n        if response.status_code == 200:\n            with open(filename, 'wb') as file:\n                for chunk in response.iter_content(8192):\n                    file.write(chunk)\n        else:\n            print(f\"Failed to download {entry['url']}. Status code: {response.status_code}\")\n\nsession = requests.Session()\n# Pre-create directories\ncreate_directories(media_export_path, data_images, 'images')\ncreate_directories(media_export_path, data_videos, 'videos')\n\n# Download images\nfor entry in tqdm(data_images, desc=\"Downloading Images\", unit=\"file\"):\n    download_file(entry, 'images', media_export_path, session)\n\n# Download videos\nfor entry in tqdm(data_videos, desc=\"Downloading Videos\", unit=\"file\"):\n    download_file(entry, 'videos', media_export_path, session)\n\nprint(\"Download complete!\")\n\n\n\n\n\n\n\nDownload complete!\n\n\n\n\nPrepare Downloadable ZIP\nRun the following to ZIP all files. Optionally copy them to Google Drive.\n\n!zip -r 2023-11-09-Story-Media-Export.zip media/*\n\n\n!cp 2023-11-09-Story-Media-Export.zip /content/drive/MyDrive/"
  },
  {
    "objectID": "notebooks/corpus-analysis-notebook.html",
    "href": "notebooks/corpus-analysis-notebook.html",
    "title": "Social Media Lab",
    "section": "",
    "text": "Among a variety of possibilities, we can, for example, look at the frequencies of the words contained in the corpus or examine the corpus for recurring themes it contains.\nFirst we need to import all the required libraries once again. The Natural Language Toolkit (NLTK) gives us access to a variety of natural language processing functions (e.g. tokenisation, stop word removal, part-of-speech tagging, …).\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport requests\nimport pandas as pd\n\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n\n\nWhen analysing word frequencies, we can use stop word lists to ignore words that occur frequently but are not relevant to us. We can easily download such a list. However, this can also be individually adapted to the purpose.\n\n# Retrieve Stopwords from Github\nsw_json = requests.get('https://github.com/stopwords-iso/stopwords-de/raw/master/stopwords-de.json')\n\nNow we can tokenise the existing text, remove the stop words or punctuation marks they contain, convert the words to lower case, or use bi-grams in addition to single-word tokens.\nWe then sum up the occurrences of the individual words and make the results available in a DataFrame.\n\ndef word_freq(text, punctuation=False, stop_words = False, lowercasing = False, bigrams = False):\n\n    if punctuation:\n        # Tokenizing, removing punctuation\n        tokens = RegexpTokenizer(r'\\w+').tokenize(text) # https://regexr.com/\n    else:\n        # Tokenizing, w/o removing punctuation\n        # tokens = text.split()\n        tokens = word_tokenize(text)\n\n    if stop_words:\n        # Removing Stopwords\n        tokens = [w for w in tokens if not w.lower() in stop_words]\n\n    if lowercasing:\n        # Lower-Casing\n        tokens = [w.lower() for w in tokens]\n\n    if bigrams:\n        # Converting text tokens into bigrams\n        tokens = nltk.bigrams(tokens)\n\n    # Creating Data Frame\n    freq = nltk.FreqDist(tokens) # display(freq)\n    df = pd.DataFrame.from_dict(freq, orient='index')\n    df.columns = ['Frequency']\n    df.index.name = 'Term'\n\n    # Here we calculate the total number of tokens in our Frequency List\n    total_tokens = sum(freq.values()) # sum([2,3,4,5,6])\n\n    # Here we add a new column `Relative` (*100 for percentage)\n    df['Relative'] = (df['Frequency'] / total_tokens) * 100\n\n    return df\n\n\nfrom pathlib import Path\nimport os\n\n#@markdown Do you want bigrams included?\nbigrams = True #@param {type:\"boolean\"}\n\n#@markdown Should all words get lower cased before counting the occurances?\nlowercasing = True #@param {type:\"boolean\"}\n\n#@markdown Do you want to exclude stopwords in your result list?\nstopwords = True #@param {type:\"boolean\"}\n\n#@markdown Do you want to remove punctuation before counting the occurances?\npunctuation = True #@param {type:\"boolean\"}\n\n\n# Load stopwords file if necessary\nif stopwords:\n    stopwords = sw_json.json()\n\n# Read source file and concat all texts\ntext = ' '.join(list(df[text_column]))\n\n# Call word_freq() with specified parameters\ndf_freq = word_freq(text, punctuation = punctuation, stop_words = stopwords, lowercasing = lowercasing, bigrams = bigrams)\n\n# Sort results for descending values\ndf_freq = df_freq.sort_values(\"Relative\", ascending = False)\n\ndisplay(df_freq[0:10])\n\n\n  \n    \n\n\n\n\n\n\nFrequency\nRelative\n\n\nTerm\n\n\n\n\n\n\n(jüdisches, leben)\n5\n1.259446\n\n\n(allerheiligen, allerseelen)\n4\n1.007557\n\n\n(ilse, aigner)\n3\n0.755668\n\n\n(bayerischer, landtag)\n3\n0.755668\n\n\n(klare, haltung)\n2\n0.503778\n\n\n(wünschen, einfach)\n2\n0.503778\n\n\n(vaters, freundschaftliche)\n2\n0.503778\n\n\n(tod, vaters)\n2\n0.503778\n\n\n(günter, tod)\n2\n0.503778\n\n\n(schwiegervater, günter)\n2\n0.503778\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWordcloud\nOne way to visualise word frequencies and recurring themes of texts are word clouds. These basically show the most frequently occurring words in the text (similar to the table created earlier), but more frequently occurring words are depicted larger than less frequently occurring words.\nFirst, we have to install the necessary library wordcloud.\n\n!pip install -q wordcloud\n\nThe actual implementation of this approach is relatively simple. We need to combine all the texts into a single text, as we did in the previous step with the frequency analysis, and pass it to the imported library.\n\nfrom wordcloud import WordCloud, STOPWORDS\n\ndef generate_wordcloud(text, path):\n\n    text = ' '.join(list(text))\n\n    # Generate a word cloud image\n    wordcloud = WordCloud(background_color=\"white\",width=1920, height=1080).generate(text)\n\n    # Dazugehörige Grafik erstellen\n    plt.imshow(wordcloud, interpolation=\"bilinear\") # Auflösung/Interpolation der Grafik\n    plt.axis(\"off\")\n    plt.figtext(0.5, 0.1, wordcloud_subcaption, wrap=True, horizontalalignment='center', fontsize=12)\n    plt.savefig(path, dpi=300)\n    plt.show()\n\nOnce again, we have the option of adjusting various parameters. Remember to specify the right file path, file name and column of your text data!\n\n#@markdown Input for additional stopwords; whitespace separated\nstopwords_extension_wc = '' #@param {type: \"string\"}\n\n#@markdown Subcaption for the wordcloud, leave blank to ignore\nwordcloud_subcaption = 'Markus S\\xF6der' #@param {type: \"string\"}\n\nNow all we have to do is load the stop word file, add our own additions and then trigger the creation of the word cloud using the function we created at the beginning.\nThe result image is saved in the defined data_path.\n\nimport matplotlib.pyplot as plt\nimport requests\n\n# Retrieve Stopwords from Github\nr = requests.get('https://github.com/stopwords-iso/stopwords-de/raw/master/stopwords-de.json')\nstop_words = r.json()\n\n# Convert input into list\nstopwords_extension_wc_list = stopwords_extension_wc.split(' ')\nstop_words.extend(stopwords_extension_wc_list)\n\n# Stopwörter in die WordCloud laden\nSTOPWORDS.update(stop_words)\n\n\ngenerate_wordcloud(df[text_column], 'wordcloud.png')"
  },
  {
    "objectID": "notebooks/whisper-notebook.html",
    "href": "notebooks/whisper-notebook.html",
    "title": "Social Media Lab",
    "section": "",
    "text": "Extract Audio from Video File\nAfter loading the metadta and media files from the Google Drive, we extract the audio from each video file to prepare the automated transcription.\n\n!pip install -q moviepy\n\n\nimport os\n\n# Set audio directory path\naudio_path = \"media/audio/\"\n\n# Check if the directory exists\nif not os.path.exists(audio_path):\n    # Create the directory if it does not exist\n    os.makedirs(audio_path)\n\n\nfrom moviepy.editor import *\n\nfor index, row in df.iterrows():\n    if row['video_file'] != \"\":\n        # Load the video file\n        video = VideoFileClip(row['video_file'])\n        filename = row['video_file'].split('/')[-1]\n\n        # Extract the audio from the video file\n        audio = video.audio\n\n        if audio is not None:\n            sampling_rate = audio.fps\n            current_suffix = filename.split(\".\")[-1]\n            new_filename = filename.replace(current_suffix, \"mp3\")\n\n            # Save the audio to a file\n            audio.write_audiofile(\"{}{}\".format(audio_path, new_filename))\n        else:\n            new_filename = \"No Audio\"\n            sampling_rate = -1\n\n        # Update DataFrame inplace\n        df.at[index, 'audio_file'] = new_filename\n        df.at[index, 'duration'] = video.duration\n        df.at[index, 'sampling_rate'] = sampling_rate\n\n        df.at[index, 'video_file'] = row['video_file'].split('/')[-1]\n\n        # Close the video file\n        video.close()\n\nMoviePy - Writing audio in media/audio/CzD93SEIi-E.mp3\nMoviePy - Done.\n\n\n                                                                      \n\n\nWe’ve extracted the audio content of each video file to a mp3 file in the media/audio folder. The files keep the name of the video file. We added new columns to the metadata for audio duration and sampling_rate. In case the video did not include an audio file, smapling_rateis set to -1, which we use to filter the df when transcribing the files.\n\ndf[df['video_file'] != \"\"].head()\n\n\n  \n    \n\n\n\n\n\n\nid\nthread_id\nparent_id\nbody\nauthor\nauthor_fullname\nauthor_avatar_url\ntimestamp\ntype\nurl\n...\nnum_comments\nnum_media\nlocation_name\nlocation_latlong\nlocation_city\nunix_timestamp\nvideo_file\naudio_file\nduration\nsampling_rate\n\n\n\n\n4\nCzD93SEIi-E\nCzD93SEIi-E\nCzD93SEIi-E\nMitzuarbeiten für unser Land, Bayern zu entwic...\nmarkus.soeder\nMarkus Söder\nhttps://scontent-fra3-1.cdninstagram.com/v/t51...\n2023-10-31 12:06:23\nvideo\nhttps://www.instagram.com/p/CzD93SEIi-E\n...\n227\n1\nNaN\nNaN\nNaN\n1698753983\nCzD93SEIi-E.mp4\nCzD93SEIi-E.mp3\n67.89\n44100.0\n\n\n\n\n\n1 rows × 24 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\nLet’s update the ZIPed folder to include the audio files.\n\n!zip -r /content/drive/MyDrive/2023-11-24-4CAT-Images-Clean.zip media\n\nupdating: media/ (stored 0%)\nupdating: media/videos/ (stored 0%)\nupdating: media/videos/CzD93SEIi-E.mp4 (deflated 0%)\n  adding: media/audio/ (stored 0%)\n  adding: media/audio/CzD93SEIi-E.mp3 (deflated 1%)\n\n\nAnd save the updated metadata file. Change filename when importing stories here!\n\ndf.to_csv(four_cat_file_path)\n\nTranscriptions using Whisper\n\nThe Whisper model was proposed in Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.\n\n\nThe abstract from the paper is the following:\n\n\n\nWe study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.\n\n\n– https://huggingface.co/docs/transformers/model_doc/whisper\n\n!pip install -q transformers\n\nThe next code snippet initializes the Whisper model. The transcribe_aduio method is applied to each row of the dataframe where sampling_rate &gt; 0, thus only to those lines with referencees to audio files. Each audio file is transcribed using Whisper, the result, one text string, is saved to the transcript column.\nAdjust the language variable according to your needs! The model is also capable of automated translation, e.g. setting language to english when processing German content results in an English translation of the speech. (Additionally, the task variable accepts translate).\n\nimport torch\nfrom transformers import pipeline, WhisperProcessor, WhisperForConditionalGeneration\nimport librosa\n\n# Set device to GPU if available, else use CPU\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize the Whisper model pipeline for automatic speech recognition\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=\"openai/whisper-large\",\n    chunk_length_s=30,\n    device=device,\n)\n\n# Load model and processor for multilingual support\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n\n# Function to read, transcribe, and handle longer audio files in different languages\ndef transcribe_audio(filename, language='german'):\n    try:\n        # Load and resample audio file\n        audio_path = f\"{audio_folder}/{filename}\"\n        waveform, original_sample_rate = librosa.load(audio_path, sr=None, mono=True)\n        waveform_resampled = librosa.resample(waveform, orig_sr=original_sample_rate, target_sr=16000)\n\n        # Get forced decoder IDs for the specified language\n        forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=\"transcribe\")\n\n        # Process the audio file in chunks and transcribe\n        transcription = \"\"\n        for i in range(0, len(waveform_resampled), 16000 * 30):  # 30 seconds chunks\n            chunk = waveform_resampled[i:i + 16000 * 30]\n            input_features = processor(chunk, sampling_rate=16000, return_tensors=\"pt\").input_features\n            predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n            chunk_transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n            transcription += \" \" + chunk_transcription\n\n        return transcription.strip()\n    except Exception as e:\n        print(f\"Error processing file {filename}: {e}\")\n        return \"\"\n\n\n# Filter the DataFrame (sampling_rates &lt; 0 identify items without audio)\nfiltered_index = df['sampling_rate'] &gt; 0\n\n# Apply the transcription function to each row in the filtered DataFrame\ndf.loc[filtered_index, 'transcript'] = df.loc[filtered_index, 'audio_file'].apply(transcribe_audio)\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\ndf[df['video_file'] != \"\"].head()\n\n\n  \n    \n\n\n\n\n\n\nid\nthread_id\nparent_id\nbody\nauthor\nauthor_fullname\nauthor_avatar_url\ntimestamp\ntype\nurl\n...\nnum_media\nlocation_name\nlocation_latlong\nlocation_city\nunix_timestamp\nvideo_file\naudio_file\nduration\nsampling_rate\ntranscript\n\n\n\n\n4\nCzD93SEIi-E\nCzD93SEIi-E\nCzD93SEIi-E\nMitzuarbeiten für unser Land, Bayern zu entwic...\nmarkus.soeder\nMarkus Söder\nhttps://scontent-fra3-1.cdninstagram.com/v/t51...\n2023-10-31 12:06:23\nvideo\nhttps://www.instagram.com/p/CzD93SEIi-E\n...\n1\nNaN\nNaN\nNaN\n1698753983\nCzD93SEIi-E.mp4\nCzD93SEIi-E.mp3\n67.89\n44100.0\nIch bitte auf den abgelagerten Vortrag der Maa...\n\n\n\n\n\n1 rows × 25 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\n\ndf.loc[4, 'transcript']\n\n'Ich bitte auf den abgelagerten Vortrag der Maaßen-Söder-Entfühlen ein.  Erfüllung meiner Amtspflichten, so wahr mir Gott helfe. Ich schwöre Treue der Verfassung des Freistaates Bayern, Gehorsam den Gesetzen und gewissenhafte Erfüllung meiner Amtspflichten, so wahr mir Gott helfe. Herr Ministerpräsident, ich darf Ihnen im Namen des ganzen Hauses ganz persönlich die herzlichsten Glückwünsche aussprechen und wünsche Ihnen viel Erfolg und gute Nerven auch bei Ihrer Aufgabe. Herzlichen Dank.  Applaus'\n\n\nOverall, the transcriptions work well. The first sentence above, however, shows that we still can expect misinterpretations."
  },
  {
    "objectID": "getting-started/literature-review-assistant.html",
    "href": "getting-started/literature-review-assistant.html",
    "title": "GPT Literature Review Assistant",
    "section": "",
    "text": "This notebook demonstrates how to work with the GPT-API based on a simple use case. First, we are going to import search results of our literature review with Publish or Perish. Next, we are going to explore how we could use python in combination with Publish or Perish to speed up our review process: We will manually code the relevance, using the Jupyter notebook as our labelling interface. Afterwards we are going to add a GPT-API call to extract features from the abstract, the first step towards our assistant guiding our literature review process.\nOverall, this notebooks is a simple implementation demonstrating how prompts work and how easy it is to use GPT in Jupyter notebooks. The notebook is available in the supplement repository, you can clone the notebook to your Colab account with one click.\n\n\nAt first we need to install necessary packages. Hit run and wait.\n\nprint(\"Install Packages\")\n!pip install -q openai crossref-commons\n\n\n\n\nIf this is the start of your review process, upload the csv file exported from Publish or Perish in the left-hand Files pane. Enter the filename in publish_or_perish_file_name. Define the output name in file_name. If you want to save the imported file in the google drive add /content/drive/MyDrive/ to the path.  Skip this cell if you want to work with a file that has been imported in the past.\n\n\n\n\n\n\nWarning\n\n\n\nWe delete rows with missing DOIs. Without a DOI our code cannot retrieve abstracts. When importing the Publish or Perish file, the following code will display the number of rows that have been deleted due to missing DOIs. When using this notebook for real-world projects, you should be aware of the missing rows and manually review them!\n\n\n\n#@title Import from Publish or Perish Data.\n#@markdown If this is the start of your review process, upload the `csv` file exported from [Publish or Perish](https://harzing.com/resources/publish-or-perish) in the left-hand *Files* pane. Enter the filename in `publish_or_perish_file_name`. Define the output name in `file_name`. If you want to save the imported file in the google drive add `/content/drive/MyDrive/` to the path. &lt;br/&gt; **Skip this cell if you want to work with a file that has been imported in the past.**\n\nimport pandas as pd\nimport numpy as np\nimport io\n\npublish_or_perish_file_name = \"scholar.csv\" # @param {type: \"string\"}\nfile_name = \"2023-10-31-Literature-Review.csv\" # @param {type: \"string\"}\n\n# Initialize empty DataFrame\nall_data = pd.DataFrame()\n\n\ntry:\n    all_data = pd.read_csv(publish_or_perish_file_name)\n\n    # Remove Duplicates\n    initial_len = len(all_data)\n    all_data = all_data.drop_duplicates(subset='DOI', keep='first')\n    removed_len = initial_len - len(all_data)\n    print(f'Removed {removed_len} duplicates based on DOI.')\n\n    # Remove missing DOIs\n    initial_len = len(all_data)\n    all_data = all_data[~pd.isna(all_data['DOI'])]\n    removed_len = initial_len - len(all_data)\n    print(f'Removed {removed_len} rows without DOI.')\n\n    all_data = all_data.sort_values(by='Cites', ascending=False).reset_index(drop=True)\n\n    print('Sorted Table by Cites.')\n\n    # Create empty columns for Literature Review\n    all_data[\"Relevant\"] = \"\"\n    all_data[\"Notes\"] = \"\"\n    all_data[\"Checked\"] = False\n\n    print('Initialized Columns')\n\n    all_data.to_csv(file_name)\n    print(f\"Success: Saved data to {file_name}\")\n\n    print(f'Success: Data loaded from File \"{file_name}\".')\nexcept Exception as e:\n    print(f\"Error: Failed to load data from File. {str(e)}\")\n\nRemoved 172 duplicates based on DOI.\nRemoved 1 rows without DOI.\nSorted Table by Cites.\nInitializes Columns\nSuccess: Saved data to 2023-10-31-Literature-Review.csv\nSuccess: Data loaded from File \"2023-10-31-Literature-Review.csv\".\n\n\n\n\n\nIf you want to keep going with a former review process, we can read an uploaded file / a file from google drive. Only run one cell, this one or the above.\n\n#@title Read previously imported File\n#@markdown If you want to keep going with a former review process, we can read an uploaded file / a file from google drive. **Only run one cell, this one or the above.**\nimport pandas as pd\nimport numpy as np\nimport io\n\nfile_name = \"2023-10-31-Literature-Review.csv\" # @param {type: \"string\"}\n\ntry:\n    all_data = pd.read_csv(file_name)\n\n    print(f'Success: Data loaded from File \"{file_name}\".')\nexcept Exception as e:\n    print(f\"Error: Failed to load data from File. {str(e)}\")\n\nSuccess: Data loaded from File \"2023-10-31-Literature-Review.csv\".\n\n\n\nIn this example we’ve saved the file locally. When working with Colab, the file will be deleted when we disconnect. For colab you should link your google drive (open the files pane on the left, click the Google Drive button). Once connected, save the file in the folder /content/drive/MyDrive/YOUR-FILENAME.csv. It will be accessible through Drive, and Colab is from now on going to connect automatically to drive.\nCheck the imported data. We’re using pandas, the imported data is saved in the all_datavariable. head(2)displays the two top rows of the table. Additionally, we have added three columns: Relevant, Notes, and Checked. We are going to make use of them to keep track of our progress.\n\n# Check the structure (and content) of the file\nall_data.head(2)\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0.2\nUnnamed: 0.1\nUnnamed: 0\nCites\nAuthors\nTitle\nYear\nSource\nPublisher\nArticleURL\n...\nAge\nAbstract\nFullTextURL\nRelatedURL\nbabbage_similarity\nbabbage_search\nsimilarities\nRelevant\nNotes\nChecked\n\n\n\n\n0\n0\n746\n844\n21\nFlorian Arendt\nSuicide on Instagram – Content Analysis of a G...\n2019.0\nCrisis\nHogrefe Publishing Group\nhttp://dx.doi.org/10.1027/0227-5910/a000529\n...\n3.0\nAbstract. Background: Suicide is the second le...\nhttps://econtent.hogrefe.com/doi/pdf/10.1027/0...\nNaN\n[-0.0018475924152880907, 0.022463073953986168,...\n[-0.014954154379665852, 0.026176564395427704, ...\n-1\nNaN\nNaN\nFalse\n\n\n1\n1\n770\n868\n4\nPaloma de H. Sánchez-Cobarro, Francisco-Jose M...\nThe Brand-Generated Content Interaction of Ins...\n2020.0\nJournal of Theoretical and Applied Electronic ...\nMDPI AG\nhttp://dx.doi.org/10.3390/jtaer16030031\n...\n2.0\nThe last decade has seen a considerable increa...\nhttps://www.mdpi.com/0718-1876/16/3/31/pdf\nNaN\n[-0.0029447057750076056, 0.01190990675240755, ...\n[-0.01012819167226553, 0.02539714053273201, -0...\n-1\nNaN\nNaN\nFalse\n\n\n\n\n\n2 rows × 35 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nIn the next step we are going to start our literature review:\n\nWe filter for the first unchecked row, ordered by the cite count.\nWe retrieve the abstract from CrossRef API using the DOI.\nWe display all information\nWe answer whether the paper appear to be relevant by entering y or n for yes or no.\n\nFor our session, the cell only runs through one row and finishes afterwards. For a real world application you’d probably like to add some kind of loop.\n\nfrom crossref_commons.retrieval import get_publication_as_json\nimport json\nimport openai\nimport textwrap\nimport IPython\nimport re\n\n# Get one row: Not checked, highest Citation count.\nhighest_cites_unchecked = all_data[all_data['Checked'] == False].sort_values(by=\"Cites\", ascending=False).iloc[0]\nindex = highest_cites_unchecked.name\n\n# Retrieve Abstract from Crossref\nresponse = get_publication_as_json(highest_cites_unchecked['DOI'])\nabstract = response.get(\"abstract\", \"\")\n\n# Remove XML\nabstract = re.sub(r'&lt;[^&gt;]+&gt;', '', abstract)\nall_data.loc[index, 'Abstract'] = abstract\n\n\n# Display all information\nIPython.display.clear_output(wait=True)\ntitle_disp = IPython.display.HTML(\"&lt;h2&gt;{}&lt;/h2&gt;\".format(highest_cites_unchecked['Title']))\nauthors_disp = IPython.display.HTML(\"&lt;p&gt;{}&lt;/p&gt;\".format(highest_cites_unchecked['Authors']))\ndoi_disp = IPython.display.HTML(\"&lt;p&gt;&lt;a target='_blank' href='https://doi.org/{}'&gt;{}&lt;/a&gt;&lt;/p&gt;\".format(highest_cites_unchecked['DOI'],highest_cites_unchecked['DOI']))\ndisplay(title_disp, authors_disp, doi_disp)\nprint(textwrap.fill(abstract, 80))\nrelevant_input = input('Relevant? (y/n): ').lower().strip() == 'y'\n\n# Save user input\nall_data.loc[index, 'Checked'] = True\nall_data.loc[index, 'Relevant'] = relevant_input\n\n\n\n\nFlorian Arendt\n\n\n10.1027/0227-5910/a000529\n\n\nAbstract. Background: Suicide is the second leading cause of death among\n15–29-year-olds globally. Unfortunately, the suicide-related content on\nInstagram, a popular social media platform for youth, has not received the\nscholarly attention it deserves. Method: The present study provides a content\nanalysis of posts tagged as #selbstmord, a German suicide-related hashtag. These\nposts were created between July 5 and July 11, 2017. Results: Approximately half\nof all posts included words or visuals related to suicide. Cutting was by far\nthe most prominent method. Although sadness was the dominant emotion, self-hate\nand loneliness also appeared regularly. Importantly, inconsistency – a gap\nbetween one's inner mental state (e.g., sadness) and one's overtly expressed\nbehavior (e.g., smiling) – was also a recurring theme. Conversely, help-seeking,\ndeath wishes, and professional awareness–intervention material were very rare.\nAn explorative analysis revealed that some videos relied on very fast cutting\ntechniques. We provide tentative evidence that users may be exposed to\npurposefully inserted suicide-related subliminal messages (i.e., exposure to\ncontent without the user's conscious awareness). Limitations: We only\ninvestigated the content of posts on one German hashtag, and the sample size was\nrather small. Conclusion: Suicide prevention organizations may consider posting\nmore awareness–intervention materials. Future research should investigate\nsuicide-related subliminal messages in social media video posts. Although\ntentative, this finding should raise a warning flag for suicide prevention\nscholars.\nRelevant? (y/n): y\n\n\nNext, we check whether our input has been saved:\n\n# Check the result\nall_data.iloc[index]\n\nUnnamed: 0.2                                                          0\nUnnamed: 0.1                                                        746\nUnnamed: 0                                                          844\nCites                                                                21\nAuthors                                                  Florian Arendt\nTitle                 Suicide on Instagram – Content Analysis of a G...\nYear                                                             2019.0\nSource                                                           Crisis\nPublisher                                      Hogrefe Publishing Group\nArticleURL                  http://dx.doi.org/10.1027/0227-5910/a000529\nCitesURL                                                            NaN\nGSRank                                                               26\nQueryDate                                           2022-09-08 10:44:44\nType                                                    journal-article\nDOI                                           10.1027/0227-5910/a000529\nISSN                                                          0227-5910\nCitationURL                                                         NaN\nVolume                                                             40.0\nIssue                                                               1.0\nStartPage                                                          36.0\nEndPage                                                            41.0\nECC                                                                  21\nCitesPerYear                                                        7.0\nCitesPerAuthor                                                       21\nAuthorCount                                                           1\nAge                                                                 3.0\nAbstract              Abstract. Background: Suicide is the second le...\nFullTextURL           https://econtent.hogrefe.com/doi/pdf/10.1027/0...\nRelatedURL                                                          NaN\nbabbage_similarity    [-0.0018475924152880907, 0.022463073953986168,...\nbabbage_search        [-0.014954154379665852, 0.026176564395427704, ...\nsimilarities                                                         -1\nRelevant                                                           True\nNotes                                                               NaN\nChecked                                                            True\nName: 0, dtype: object\n\n\n\n\n\nNow for the fun part: Is it possible to use GPT to help us during the review process? We are going to try and extract text features automatically. For the moment we are going to use gpt3.5-turbo.\nNote: Please feel free to test different prompts and questions. The Promptingguide is a good resource to learn more about different prompting techniques. Use the ChatGPT interface to cheaply test prompts prior to using them with the API. Use the OpenAI Playground to optimize your prompts with a visual user interface for different settings and a prompting history (trust me, this can save your life!).\nPrompts: We’re going to use the system prompt for our instructions, and the user prompt to send our content.\n\n\n\n\n\n\nCaution\n\n\n\nA word of warning: You should not trust the quality of the GPT output at this stage. The prompt has not been evaluated, overall LLMs produce output that appears meaningful most of the times. Sometimes, however, it is Hallucinations. Thus, before using prompts and LLMs for production, we have to make sure we can trust their outputs. We will dive deeper into this topic in the classification sessions.\n\n\n\nsystem_prompt = \"\"\"\nYou're an advanced AI research assistant. Your task is to extract **research questions**, **operationalization**, **data sources**, **population**, and **scientific disciplines** from user input. Return \"None\" if you can't find the information in user input.\n\n**Formatting**\nReturn a markdown table, one row for each extracted feature: **research questions**, **operationalization**, **data sources**, **population**, and **scientific disciplines**.\n\"\"\"\n\nPlease enter your API-Code in the next code cell for the openai.api_key variable. We have changed the cell to include the gpt_prompt variable, which sends the title and abstract as a user prompt. We’re using the openai.ChatCompletion.create() method to send our request to the API. We expect the response in api_response['choices'][0]['message']['content'] to be markdown (see prompt above), as such we display the markdown in our notebook.\n\nfrom crossref_commons.retrieval import get_publication_as_json\nimport json\nimport openai\nimport textwrap\nimport IPython\nimport re\n\n# Enter OpenAI API-Code\nopenai.api_key = \"sk-XXXXXXXXX\"\n\n# Get one row: Not checked, highest Citation count.\nhighest_cites_unchecked = all_data[all_data['Checked'] == False].sort_values(by=\"Cites\", ascending=False).iloc[0]\nindex = highest_cites_unchecked.name\n\n# Retrieve Abstract from Crossref\nresponse = get_publication_as_json(highest_cites_unchecked['DOI'])\nabstract = response.get(\"abstract\", \"\")\n\n# Remove XML\nabstract = re.sub(r'&lt;[^&gt;]+&gt;', '', abstract)\n\nall_data.loc[index, 'Abstract'] = abstract\n\n# Display all information (before we send the request to OpenAI)\nIPython.display.clear_output(wait=True)\ntitle_disp = IPython.display.HTML(\"&lt;h2&gt;{}&lt;/h2&gt;\".format(highest_cites_unchecked['Title']))\nauthors_disp = IPython.display.HTML(\"&lt;p&gt;{}&lt;/p&gt;\".format(highest_cites_unchecked['Authors']))\ndoi_disp = IPython.display.HTML(\"&lt;p&gt;&lt;a target='_blank' href='https://doi.org/{}'&gt;{}&lt;/a&gt;&lt;/p&gt;\".format(highest_cites_unchecked['DOI'],highest_cites_unchecked['DOI']))\ndisplay(title_disp, authors_disp, doi_disp)\nprint(textwrap.fill(abstract, 80))\n\ngpt_prompt = f\"\"\"\n**Title**: {highest_cites_unchecked['Title']}\n**Abstract**: {abstract}\n\"\"\"\n\n# Sending request, takes a moment. In the meantime you may read the abstract.\nmessages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": abstract}\n]\n\ntry:\n  api_response = openai.ChatCompletion.create(\n      model=\"gpt-3.5-turbo\",\n      messages=messages,\n      temperature=0,\n      timeout=30\n    )\n\n  gpt_result = api_response['choices'][0]['message']['content']\n\n  # Display the GPT result\n  display(IPython.display.HTML(f\"&lt;h3&gt;GPT Extracted Data&lt;/h3&gt;\"))\n  display(IPython.display.Markdown(gpt_result))\nexcept:\n  print(\"GPT API Error\")\n\nrelevant_input = input('Relevant? (y/n): ').lower().strip() == 'y'\n\n# Save user input\nall_data.loc[index, 'Checked'] = True\nall_data.loc[index, 'Relevant'] = relevant_input\n\n\n\n\nPaloma de H. Sánchez-Cobarro, Francisco-Jose Molina-Castillo, Cristina Alcazar-Caceres\n\n\n10.3390/jtaer16030031\n\n\nThe last decade has seen a considerable increase in entertainment-oriented\ncommunication techniques. Likewise, the rise of social networks has evolved,\noffering different formats such as publication and stories. Hence, there has\nbeen a growing interest in knowing which strategies have the greatest social\nimpact to help position organizations in the mind of the consumer. This research\naims to analyze the different impact that stories and publications can have on\nthe Instagram social network as a tool for generating branded content. To this\nend, it analyses the impact of the different Instagram stories and publications\nin various sectors using a methodology of structural equations with composite\nconstructs. The results obtained, based on 800 stories and publications in four\ndifferent companies (retailers and manufacturers), show that the reach of the\nstory generally explains the interaction with Instagram stories. In contrast, in\nthe case of publications, impressions are of greater importance in explaining\nthe interaction with the publication. Among the main contributions of the work,\nwe find that traditional pull communication techniques have been losing\neffectiveness in front of new formats of brand content generation that have been\noccupying the time in the relationship between users and brands.\nRelevant? (y/n): y\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nValue\n\n\n\n\nResearch questions\n- What strategies have the greatest social impact on Instagram?- How do stories and publications on Instagram impact the consumer’s perception of brands?- What is the relationship between reach and interaction with Instagram stories?- What is the relationship between impressions and interaction with Instagram publications?\n\n\nOperationalization\n- Analyzing the impact of Instagram stories and publications in various sectors- Using a methodology of structural equations with composite constructs\n\n\nData sources\n- 800 stories and publications on Instagram\n\n\nPopulation\n- Four different companies (retailers and manufacturers)\n\n\nScientific disciplines\n- Marketing- Communication\n\n\n\n\n\n\nThe above output shows a formatted table listing all extracted features. In this short warm-up session on GPT we have seen one use case of the LLM: The extraction of text feautures. In future sessions we are going to dive deeper into this topic.\n\n\n\n\n\n\nNote\n\n\n\nDid you create an excellent prompt? Share it with us! Enter your prompt into this Excel Sheet\n\n\n\n\n\nThe following line saves all progress to file_name. If file_name is a path to Google Drive you will be able to pick up your work later on.\n\nall_data.to_csv(file_name)\n\n\nSource: GPT Literature Review Assistant"
  },
  {
    "objectID": "getting-started/literature-review-assistant.html#setup",
    "href": "getting-started/literature-review-assistant.html#setup",
    "title": "GPT Literature Review Assistant",
    "section": "",
    "text": "At first we need to install necessary packages. Hit run and wait.\n\nprint(\"Install Packages\")\n!pip install -q openai crossref-commons"
  },
  {
    "objectID": "getting-started/literature-review-assistant.html#import-publish-or-perish-data.",
    "href": "getting-started/literature-review-assistant.html#import-publish-or-perish-data.",
    "title": "GPT Literature Review Assistant",
    "section": "",
    "text": "If this is the start of your review process, upload the csv file exported from Publish or Perish in the left-hand Files pane. Enter the filename in publish_or_perish_file_name. Define the output name in file_name. If you want to save the imported file in the google drive add /content/drive/MyDrive/ to the path.  Skip this cell if you want to work with a file that has been imported in the past.\n\n\n\n\n\n\nWarning\n\n\n\nWe delete rows with missing DOIs. Without a DOI our code cannot retrieve abstracts. When importing the Publish or Perish file, the following code will display the number of rows that have been deleted due to missing DOIs. When using this notebook for real-world projects, you should be aware of the missing rows and manually review them!\n\n\n\n#@title Import from Publish or Perish Data.\n#@markdown If this is the start of your review process, upload the `csv` file exported from [Publish or Perish](https://harzing.com/resources/publish-or-perish) in the left-hand *Files* pane. Enter the filename in `publish_or_perish_file_name`. Define the output name in `file_name`. If you want to save the imported file in the google drive add `/content/drive/MyDrive/` to the path. &lt;br/&gt; **Skip this cell if you want to work with a file that has been imported in the past.**\n\nimport pandas as pd\nimport numpy as np\nimport io\n\npublish_or_perish_file_name = \"scholar.csv\" # @param {type: \"string\"}\nfile_name = \"2023-10-31-Literature-Review.csv\" # @param {type: \"string\"}\n\n# Initialize empty DataFrame\nall_data = pd.DataFrame()\n\n\ntry:\n    all_data = pd.read_csv(publish_or_perish_file_name)\n\n    # Remove Duplicates\n    initial_len = len(all_data)\n    all_data = all_data.drop_duplicates(subset='DOI', keep='first')\n    removed_len = initial_len - len(all_data)\n    print(f'Removed {removed_len} duplicates based on DOI.')\n\n    # Remove missing DOIs\n    initial_len = len(all_data)\n    all_data = all_data[~pd.isna(all_data['DOI'])]\n    removed_len = initial_len - len(all_data)\n    print(f'Removed {removed_len} rows without DOI.')\n\n    all_data = all_data.sort_values(by='Cites', ascending=False).reset_index(drop=True)\n\n    print('Sorted Table by Cites.')\n\n    # Create empty columns for Literature Review\n    all_data[\"Relevant\"] = \"\"\n    all_data[\"Notes\"] = \"\"\n    all_data[\"Checked\"] = False\n\n    print('Initialized Columns')\n\n    all_data.to_csv(file_name)\n    print(f\"Success: Saved data to {file_name}\")\n\n    print(f'Success: Data loaded from File \"{file_name}\".')\nexcept Exception as e:\n    print(f\"Error: Failed to load data from File. {str(e)}\")\n\nRemoved 172 duplicates based on DOI.\nRemoved 1 rows without DOI.\nSorted Table by Cites.\nInitializes Columns\nSuccess: Saved data to 2023-10-31-Literature-Review.csv\nSuccess: Data loaded from File \"2023-10-31-Literature-Review.csv\"."
  },
  {
    "objectID": "getting-started/literature-review-assistant.html#read-previously-imported-file",
    "href": "getting-started/literature-review-assistant.html#read-previously-imported-file",
    "title": "GPT Literature Review Assistant",
    "section": "",
    "text": "If you want to keep going with a former review process, we can read an uploaded file / a file from google drive. Only run one cell, this one or the above.\n\n#@title Read previously imported File\n#@markdown If you want to keep going with a former review process, we can read an uploaded file / a file from google drive. **Only run one cell, this one or the above.**\nimport pandas as pd\nimport numpy as np\nimport io\n\nfile_name = \"2023-10-31-Literature-Review.csv\" # @param {type: \"string\"}\n\ntry:\n    all_data = pd.read_csv(file_name)\n\n    print(f'Success: Data loaded from File \"{file_name}\".')\nexcept Exception as e:\n    print(f\"Error: Failed to load data from File. {str(e)}\")\n\nSuccess: Data loaded from File \"2023-10-31-Literature-Review.csv\".\n\n\n\nIn this example we’ve saved the file locally. When working with Colab, the file will be deleted when we disconnect. For colab you should link your google drive (open the files pane on the left, click the Google Drive button). Once connected, save the file in the folder /content/drive/MyDrive/YOUR-FILENAME.csv. It will be accessible through Drive, and Colab is from now on going to connect automatically to drive.\nCheck the imported data. We’re using pandas, the imported data is saved in the all_datavariable. head(2)displays the two top rows of the table. Additionally, we have added three columns: Relevant, Notes, and Checked. We are going to make use of them to keep track of our progress.\n\n# Check the structure (and content) of the file\nall_data.head(2)\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0.2\nUnnamed: 0.1\nUnnamed: 0\nCites\nAuthors\nTitle\nYear\nSource\nPublisher\nArticleURL\n...\nAge\nAbstract\nFullTextURL\nRelatedURL\nbabbage_similarity\nbabbage_search\nsimilarities\nRelevant\nNotes\nChecked\n\n\n\n\n0\n0\n746\n844\n21\nFlorian Arendt\nSuicide on Instagram – Content Analysis of a G...\n2019.0\nCrisis\nHogrefe Publishing Group\nhttp://dx.doi.org/10.1027/0227-5910/a000529\n...\n3.0\nAbstract. Background: Suicide is the second le...\nhttps://econtent.hogrefe.com/doi/pdf/10.1027/0...\nNaN\n[-0.0018475924152880907, 0.022463073953986168,...\n[-0.014954154379665852, 0.026176564395427704, ...\n-1\nNaN\nNaN\nFalse\n\n\n1\n1\n770\n868\n4\nPaloma de H. Sánchez-Cobarro, Francisco-Jose M...\nThe Brand-Generated Content Interaction of Ins...\n2020.0\nJournal of Theoretical and Applied Electronic ...\nMDPI AG\nhttp://dx.doi.org/10.3390/jtaer16030031\n...\n2.0\nThe last decade has seen a considerable increa...\nhttps://www.mdpi.com/0718-1876/16/3/31/pdf\nNaN\n[-0.0029447057750076056, 0.01190990675240755, ...\n[-0.01012819167226553, 0.02539714053273201, -0...\n-1\nNaN\nNaN\nFalse\n\n\n\n\n\n2 rows × 35 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nIn the next step we are going to start our literature review:\n\nWe filter for the first unchecked row, ordered by the cite count.\nWe retrieve the abstract from CrossRef API using the DOI.\nWe display all information\nWe answer whether the paper appear to be relevant by entering y or n for yes or no.\n\nFor our session, the cell only runs through one row and finishes afterwards. For a real world application you’d probably like to add some kind of loop.\n\nfrom crossref_commons.retrieval import get_publication_as_json\nimport json\nimport openai\nimport textwrap\nimport IPython\nimport re\n\n# Get one row: Not checked, highest Citation count.\nhighest_cites_unchecked = all_data[all_data['Checked'] == False].sort_values(by=\"Cites\", ascending=False).iloc[0]\nindex = highest_cites_unchecked.name\n\n# Retrieve Abstract from Crossref\nresponse = get_publication_as_json(highest_cites_unchecked['DOI'])\nabstract = response.get(\"abstract\", \"\")\n\n# Remove XML\nabstract = re.sub(r'&lt;[^&gt;]+&gt;', '', abstract)\nall_data.loc[index, 'Abstract'] = abstract\n\n\n# Display all information\nIPython.display.clear_output(wait=True)\ntitle_disp = IPython.display.HTML(\"&lt;h2&gt;{}&lt;/h2&gt;\".format(highest_cites_unchecked['Title']))\nauthors_disp = IPython.display.HTML(\"&lt;p&gt;{}&lt;/p&gt;\".format(highest_cites_unchecked['Authors']))\ndoi_disp = IPython.display.HTML(\"&lt;p&gt;&lt;a target='_blank' href='https://doi.org/{}'&gt;{}&lt;/a&gt;&lt;/p&gt;\".format(highest_cites_unchecked['DOI'],highest_cites_unchecked['DOI']))\ndisplay(title_disp, authors_disp, doi_disp)\nprint(textwrap.fill(abstract, 80))\nrelevant_input = input('Relevant? (y/n): ').lower().strip() == 'y'\n\n# Save user input\nall_data.loc[index, 'Checked'] = True\nall_data.loc[index, 'Relevant'] = relevant_input\n\n\n\n\nFlorian Arendt\n\n\n10.1027/0227-5910/a000529\n\n\nAbstract. Background: Suicide is the second leading cause of death among\n15–29-year-olds globally. Unfortunately, the suicide-related content on\nInstagram, a popular social media platform for youth, has not received the\nscholarly attention it deserves. Method: The present study provides a content\nanalysis of posts tagged as #selbstmord, a German suicide-related hashtag. These\nposts were created between July 5 and July 11, 2017. Results: Approximately half\nof all posts included words or visuals related to suicide. Cutting was by far\nthe most prominent method. Although sadness was the dominant emotion, self-hate\nand loneliness also appeared regularly. Importantly, inconsistency – a gap\nbetween one's inner mental state (e.g., sadness) and one's overtly expressed\nbehavior (e.g., smiling) – was also a recurring theme. Conversely, help-seeking,\ndeath wishes, and professional awareness–intervention material were very rare.\nAn explorative analysis revealed that some videos relied on very fast cutting\ntechniques. We provide tentative evidence that users may be exposed to\npurposefully inserted suicide-related subliminal messages (i.e., exposure to\ncontent without the user's conscious awareness). Limitations: We only\ninvestigated the content of posts on one German hashtag, and the sample size was\nrather small. Conclusion: Suicide prevention organizations may consider posting\nmore awareness–intervention materials. Future research should investigate\nsuicide-related subliminal messages in social media video posts. Although\ntentative, this finding should raise a warning flag for suicide prevention\nscholars.\nRelevant? (y/n): y\n\n\nNext, we check whether our input has been saved:\n\n# Check the result\nall_data.iloc[index]\n\nUnnamed: 0.2                                                          0\nUnnamed: 0.1                                                        746\nUnnamed: 0                                                          844\nCites                                                                21\nAuthors                                                  Florian Arendt\nTitle                 Suicide on Instagram – Content Analysis of a G...\nYear                                                             2019.0\nSource                                                           Crisis\nPublisher                                      Hogrefe Publishing Group\nArticleURL                  http://dx.doi.org/10.1027/0227-5910/a000529\nCitesURL                                                            NaN\nGSRank                                                               26\nQueryDate                                           2022-09-08 10:44:44\nType                                                    journal-article\nDOI                                           10.1027/0227-5910/a000529\nISSN                                                          0227-5910\nCitationURL                                                         NaN\nVolume                                                             40.0\nIssue                                                               1.0\nStartPage                                                          36.0\nEndPage                                                            41.0\nECC                                                                  21\nCitesPerYear                                                        7.0\nCitesPerAuthor                                                       21\nAuthorCount                                                           1\nAge                                                                 3.0\nAbstract              Abstract. Background: Suicide is the second le...\nFullTextURL           https://econtent.hogrefe.com/doi/pdf/10.1027/0...\nRelatedURL                                                          NaN\nbabbage_similarity    [-0.0018475924152880907, 0.022463073953986168,...\nbabbage_search        [-0.014954154379665852, 0.026176564395427704, ...\nsimilarities                                                         -1\nRelevant                                                           True\nNotes                                                               NaN\nChecked                                                            True\nName: 0, dtype: object"
  },
  {
    "objectID": "getting-started/literature-review-assistant.html#using-gpt-to-extract-information-from-abstracts",
    "href": "getting-started/literature-review-assistant.html#using-gpt-to-extract-information-from-abstracts",
    "title": "GPT Literature Review Assistant",
    "section": "",
    "text": "Now for the fun part: Is it possible to use GPT to help us during the review process? We are going to try and extract text features automatically. For the moment we are going to use gpt3.5-turbo.\nNote: Please feel free to test different prompts and questions. The Promptingguide is a good resource to learn more about different prompting techniques. Use the ChatGPT interface to cheaply test prompts prior to using them with the API. Use the OpenAI Playground to optimize your prompts with a visual user interface for different settings and a prompting history (trust me, this can save your life!).\nPrompts: We’re going to use the system prompt for our instructions, and the user prompt to send our content.\n\n\n\n\n\n\nCaution\n\n\n\nA word of warning: You should not trust the quality of the GPT output at this stage. The prompt has not been evaluated, overall LLMs produce output that appears meaningful most of the times. Sometimes, however, it is Hallucinations. Thus, before using prompts and LLMs for production, we have to make sure we can trust their outputs. We will dive deeper into this topic in the classification sessions.\n\n\n\nsystem_prompt = \"\"\"\nYou're an advanced AI research assistant. Your task is to extract **research questions**, **operationalization**, **data sources**, **population**, and **scientific disciplines** from user input. Return \"None\" if you can't find the information in user input.\n\n**Formatting**\nReturn a markdown table, one row for each extracted feature: **research questions**, **operationalization**, **data sources**, **population**, and **scientific disciplines**.\n\"\"\"\n\nPlease enter your API-Code in the next code cell for the openai.api_key variable. We have changed the cell to include the gpt_prompt variable, which sends the title and abstract as a user prompt. We’re using the openai.ChatCompletion.create() method to send our request to the API. We expect the response in api_response['choices'][0]['message']['content'] to be markdown (see prompt above), as such we display the markdown in our notebook.\n\nfrom crossref_commons.retrieval import get_publication_as_json\nimport json\nimport openai\nimport textwrap\nimport IPython\nimport re\n\n# Enter OpenAI API-Code\nopenai.api_key = \"sk-XXXXXXXXX\"\n\n# Get one row: Not checked, highest Citation count.\nhighest_cites_unchecked = all_data[all_data['Checked'] == False].sort_values(by=\"Cites\", ascending=False).iloc[0]\nindex = highest_cites_unchecked.name\n\n# Retrieve Abstract from Crossref\nresponse = get_publication_as_json(highest_cites_unchecked['DOI'])\nabstract = response.get(\"abstract\", \"\")\n\n# Remove XML\nabstract = re.sub(r'&lt;[^&gt;]+&gt;', '', abstract)\n\nall_data.loc[index, 'Abstract'] = abstract\n\n# Display all information (before we send the request to OpenAI)\nIPython.display.clear_output(wait=True)\ntitle_disp = IPython.display.HTML(\"&lt;h2&gt;{}&lt;/h2&gt;\".format(highest_cites_unchecked['Title']))\nauthors_disp = IPython.display.HTML(\"&lt;p&gt;{}&lt;/p&gt;\".format(highest_cites_unchecked['Authors']))\ndoi_disp = IPython.display.HTML(\"&lt;p&gt;&lt;a target='_blank' href='https://doi.org/{}'&gt;{}&lt;/a&gt;&lt;/p&gt;\".format(highest_cites_unchecked['DOI'],highest_cites_unchecked['DOI']))\ndisplay(title_disp, authors_disp, doi_disp)\nprint(textwrap.fill(abstract, 80))\n\ngpt_prompt = f\"\"\"\n**Title**: {highest_cites_unchecked['Title']}\n**Abstract**: {abstract}\n\"\"\"\n\n# Sending request, takes a moment. In the meantime you may read the abstract.\nmessages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": abstract}\n]\n\ntry:\n  api_response = openai.ChatCompletion.create(\n      model=\"gpt-3.5-turbo\",\n      messages=messages,\n      temperature=0,\n      timeout=30\n    )\n\n  gpt_result = api_response['choices'][0]['message']['content']\n\n  # Display the GPT result\n  display(IPython.display.HTML(f\"&lt;h3&gt;GPT Extracted Data&lt;/h3&gt;\"))\n  display(IPython.display.Markdown(gpt_result))\nexcept:\n  print(\"GPT API Error\")\n\nrelevant_input = input('Relevant? (y/n): ').lower().strip() == 'y'\n\n# Save user input\nall_data.loc[index, 'Checked'] = True\nall_data.loc[index, 'Relevant'] = relevant_input\n\n\n\n\nPaloma de H. Sánchez-Cobarro, Francisco-Jose Molina-Castillo, Cristina Alcazar-Caceres\n\n\n10.3390/jtaer16030031\n\n\nThe last decade has seen a considerable increase in entertainment-oriented\ncommunication techniques. Likewise, the rise of social networks has evolved,\noffering different formats such as publication and stories. Hence, there has\nbeen a growing interest in knowing which strategies have the greatest social\nimpact to help position organizations in the mind of the consumer. This research\naims to analyze the different impact that stories and publications can have on\nthe Instagram social network as a tool for generating branded content. To this\nend, it analyses the impact of the different Instagram stories and publications\nin various sectors using a methodology of structural equations with composite\nconstructs. The results obtained, based on 800 stories and publications in four\ndifferent companies (retailers and manufacturers), show that the reach of the\nstory generally explains the interaction with Instagram stories. In contrast, in\nthe case of publications, impressions are of greater importance in explaining\nthe interaction with the publication. Among the main contributions of the work,\nwe find that traditional pull communication techniques have been losing\neffectiveness in front of new formats of brand content generation that have been\noccupying the time in the relationship between users and brands.\nRelevant? (y/n): y\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nValue\n\n\n\n\nResearch questions\n- What strategies have the greatest social impact on Instagram?- How do stories and publications on Instagram impact the consumer’s perception of brands?- What is the relationship between reach and interaction with Instagram stories?- What is the relationship between impressions and interaction with Instagram publications?\n\n\nOperationalization\n- Analyzing the impact of Instagram stories and publications in various sectors- Using a methodology of structural equations with composite constructs\n\n\nData sources\n- 800 stories and publications on Instagram\n\n\nPopulation\n- Four different companies (retailers and manufacturers)\n\n\nScientific disciplines\n- Marketing- Communication\n\n\n\n\n\n\nThe above output shows a formatted table listing all extracted features. In this short warm-up session on GPT we have seen one use case of the LLM: The extraction of text feautures. In future sessions we are going to dive deeper into this topic.\n\n\n\n\n\n\nNote\n\n\n\nDid you create an excellent prompt? Share it with us! Enter your prompt into this Excel Sheet"
  },
  {
    "objectID": "getting-started/literature-review-assistant.html#save-your-progress",
    "href": "getting-started/literature-review-assistant.html#save-your-progress",
    "title": "GPT Literature Review Assistant",
    "section": "",
    "text": "The following line saves all progress to file_name. If file_name is a path to Google Drive you will be able to pick up your work later on.\n\nall_data.to_csv(file_name)"
  },
  {
    "objectID": "getting-started/theory.html",
    "href": "getting-started/theory.html",
    "title": "Introduction to SMA",
    "section": "",
    "text": "Social Media Analyses (SMA) are used both, in academia and in professional settings. Depending on the research agenda, different methodologies may be applied (Kanthawala et al. 2022; Rejeb et al. 2022). In our course, we focus on the academic exploration of Social Media. We place particular emphasis on questions related to media, politics, and society. This represents a confluence of communication science and political science, intertwined with computational methods."
  },
  {
    "objectID": "getting-started/theory.html#social-media-analyses-in-different-contexts",
    "href": "getting-started/theory.html#social-media-analyses-in-different-contexts",
    "title": "Introduction to SMA",
    "section": "Social Media Analyses in different contexts",
    "text": "Social Media Analyses in different contexts\nBridging this discussion, there are several disciplines pivotal to the academic analysis of social media data at this intersection: Lazer et al. (2009) outlined in an influencial article computational social science as an emerging field that built on the ability to collect and analyze vast amounts of data. The goal of the computational social science, according to this article, is to reveal patterns in human interactions, benefiting from various data sources such as emails, phone records, online social networks, and other digital traces left by individuals. We are going to concentrate on social media data, a type of data described by Quan-Haase and Sloan (2022a) as incidental, since the data exists and is being created, no matter the researchers observing them – or not. One special type of data, Instagram stories, even have an ephemeral character. 24 hours after posting the story expires – becoming invisible for followers and researchers alike (see also Leaver, Highfield, and Abidin 2020 on the importance of stories). Atteveldt and Peng (2018) noted a surge in the use of computational methods in communication science, attributing it to three primary factors: the availability of digital data, sophisticated data analysis tools, and the emergence of cost-effective, potent processing capabilities complemented by accessible computing infrastructure. Building on this perspective, Haim (2023) sees the computational communication science as a sub-discipline of communication science that addresses digitally altered objects of research, which require computational approaches to tackle to amount and complexity of this special type of data.\nIn the realm of digital humanities, computational approaches to text analysis have a long history, influenced by concepts such as distant reading (Moretti 2000) and macroanalysis (Jockers 2013). Manovich picks up these concepts in his cultural analytics, see below. Lately also distant viewing has been outlined, as “a methodological and theoretical framework for the study of large collections of visual materials” (Arnold and Tilton 2019). I see potential in integrating approaches and methods from the digital humanities into social media analysis. Vice versa, there’s also potential in utilizing methods used for social media analysis to address questions in the humanities.\nChallenges for social media analyses have been outlined by Quan-Haase and Sloan (2022a): the role of theory, representativeness of data, scale, multimodality, data accessability, and legal and ethical considerations. Through our semester we are going to work on several of those challenges: In the Operationalization session we are going to talk about data-driven approaches (bearing in mind Anderson et al. 2008), as well as theories as basis for your research questions and operationalizations. The representativeness of data will be the challenge for our data collection sessions: We will not just answer how to collect data, but also what data to collect. The two challenges left are at the centre of our seminar: Our answer for the challenge of scale is to apply computational methods for data analysis, to process data at scale. Multimodality is another key issues of this seminar: We want to computationally process visual (or multimodal) data. We will talk about accessability problems throughout our data collection classes, and talk about legal and ethical issues on this page.\nKeeping these introductory considerations in mind, we immerse into a short outline of two theories: Cultural Analytics and Digital Methods, as foundational elements for social media research. Subsequently, we’ll address the ethical and legal challenges associated with analyzing social media. We’ll conclude the chapter by presenting an array of methodologies. In the related work chapter, you’ll find an overview of research on Instagram and TikTok content, even extending beyond our primary topics of interest.\n\n\n\n\n\n\nNote\n\n\n\nThe intent of this article is to provide a brief introduction to the field of computational social media analysis, tailored for my Winter 2023/24 seminar. It offers only a cursory glance at various theories and methodologies. As such, please do not regard the content of this page as a definitive scientific piece. Instead, view it as a compass to guide and inspire your own research endeavors. For a deeper dive into the theory of Digital Media in Politics and Society see the lecture by Prof. Jungherr."
  },
  {
    "objectID": "getting-started/theory.html#cultural-analytics",
    "href": "getting-started/theory.html#cultural-analytics",
    "title": "Introduction to SMA",
    "section": "Cultural Analytics",
    "text": "Cultural Analytics\nCultural analytics, as explained in the introductory chapter of the book “Cultural Analytics” by Lev Manovich, is a field that uses computers to analyze and understand large amounts of cultural information or “big cultural data”. This might include exploring big collections of images, videos, or other media data to see patterns and trends that are happening in digital culture. Manovich talks about some key questions and challenges in cultural analytics. For example, one big question is whether we should focus on finding common themes and patterns in our data, or whether we should pay more attention to things that are unusual or rare. Also, while cultural analytics can be a powerful tool for understanding aspects of culture, especially in the digital world, Manovich tells us to be aware of its limits. He says that computers and data analysis can tell us a lot, but they can’t understand culture in the rich and deep way that humans can, especially when it comes to understanding things like aesthetics (beauty, style, etc.). So, while cultural analytics can help us see large scale patterns and trends in culture, Manovich advises us to also appreciate and be aware of what it can’t see or understand. The field of cultural analytics then becomes a space where we use computational tools to explore and question culture, while also being mindful of the limitations and challenges of using these tools (Manovich 2020)."
  },
  {
    "objectID": "getting-started/theory.html#digital-methods",
    "href": "getting-started/theory.html#digital-methods",
    "title": "Introduction to SMA",
    "section": "Digital Methods",
    "text": "Digital Methods\n“Digital Methods,” as introduced by Rogers (2013), proposes a paradigm wherein the internet is both a site and a source for research, especially for social media studies. Unlike conventional research approaches that see the internet merely as a tool or data source, Rogers advocates for a methodology that is intrinsically web-centric, understanding and employing the unique dynamics and mechanics of the digital medium itself. An example for a digital methods research project is understanding algorithmic operations, especially of search engines like Google, and comprehending their impact on digital culture, information accessibility, and user engagement. This perspective is important to explore the foundations of how information is organized, ranked, and accessed online. Studying the digital medium itself means to study web-native phenomena such as hyperlink networks, search engine behaviors, and social media activities to uncover patterns, tendencies, and hierarchical structures within digital cultures and societies.\nThe concepts of cultural analytics and digital methods will guide us through our semester and our projects: We borrow the idea to use computational methods in order to understand “big cultural data” form Manovich and the concept of studying the digital medium itself from Rogers. Throughout the semester will enrich our projects through your own literature and theory based on the research interests. Beyond these foundations, we will borrow from i.e. the Computational Social Sciences (Lazer et al. 2009), the concept of Distant Viewing (Arnold and Tilton 2019), or Grammars of Action (Agre 1994; Gerlitz and Rieder 2018; Bainotti, Caliandro, and Gandini 2020; Omena, Rabello, and Mintz 2020), and Platform Vernaculars (Gibbs et al. 2015)."
  },
  {
    "objectID": "getting-started/theory.html#legal-ethical-challenges",
    "href": "getting-started/theory.html#legal-ethical-challenges",
    "title": "Introduction to SMA",
    "section": "Legal & Ethical Challenges",
    "text": "Legal & Ethical Challenges\n\n\n\n\n\n\nWarning\n\n\n\nThis subchapter scratches the surface. Recommended reading: Haim (2023) pp. 62–69; 126–128.\n\n\nWhen working with social media data, we’re dealing with personal information. As such we need to take into account legal and ethical considerations. From the legal perspective we need to focus on two aspects: The ownership of the data, and – when dealing with personal data – the GDPR. For the latter we need to take into account consent and should think about pseudonymisation or anonymisation of our data (Haim 2023). Further, the German Urheberrecht, the equivalent of the anglo-saxon copyright law (there are important differences, see Bundeszentrale für politische Bildung for a synopsis), defines exceptions for scientific research: I recommend the publication by Rat für Sozial- und Wirtschaftsdaten (RatSWD) (2019) which takes a closer look at the database law and provides some practical guidance (more in our slides).\nThe importance of the legal perspective social media research grew recently: Following the Cambridge Analytica scandal Meta platforms (like Instagram) started closing down on APIs, which would have offered a legal and accepted (by the plattform) point of access for researchers. I recommend to read McCrow-Young’s (2021) article, as she demonstrates how academic research may be interrupted by platform changes, like the closure of the Instagram-API in the wake of above incident. Post-API social media research found creative ways to access the data: Bainotti, Caliandro, and Gandini (2020), for example, took a unique approach for data collection by capturing Instagram content through YouTube videos. Recent publications on Instagram analyses, and most approaches in our future session, rely on crawling and scraping. Venturini and Rogers (2019) see a chance in the API-closure and argue that these techniques are “more than a ‘necessary evil’”, as it might force researchers to come back to (digital) field work.\nFinally a word about reserach ethics. While the GDPR provides a rigid legal framework for dealing with personal information, I’d like to recommend the article “But the Data is Already Public” by Zimmer (2010). The article documents how, in a matter of days, an anonymous dataset of 1700 facebook profiles became (partly) deanonymized. Based on this case study, the author compiles ethical concerns for future research, which we should also incorporate into our work."
  },
  {
    "objectID": "getting-started/theory.html#methodology",
    "href": "getting-started/theory.html#methodology",
    "title": "Introduction to SMA",
    "section": "Methodology",
    "text": "Methodology\nIn this chapter we are going to take a look at different methods for use with social media research, and particularly, with our projects. We are going to use (Visual) Content Analysis to understand the content of posts and stories. The concept of Plattform Affordances will help us understand these posts and stories as embedded in the platform and its available functions and options. Finally, the idea of Platform Vernaculars & Grammars serves as a guide to wire everything up, to discover patterns and trends in how users communicate and engage on these platforms.\n\n(Visual) Content Analysis\nWe are going to apply quantitative content analyses to our corpora. For a quantitative approach we are going to operationalize our theory-based interests and questions using formal and / or content features. Next, we need to apply the operationalization to the documents, in form of human annotations or computational coding (see Döring and Bortz 2016). Döring and Bortz (2016) outline a general approach to content analysis, Rose (2016) in contrast concentrates on visual content analyses. She suggests four steps:\n\n\n“Finding your Images.\nDevising your categories for coding.\nCoding the images.\nAnalysing the results.” – (Rose 2016 ch. 5)\n\n\nThe challenge of the first step is the sampling: Even with computational approaches, is it feasible to collect everything? The cultural analytics approach suggests such a goal, e.g. in order to obtain data and traces of subcultures. Due to practical limitations also Manovich’s works use an approach to break the large amount of available data into a smaller portion (see Hochman and Manovich 2013). This approach is called sampling, Rose (2016) introduces several sampling approaches like random, stratified, systematic, or cluster sampling. Döring and Bortz (2016) provide a deeper look into sampling strategies.\nThe codes, for the second step, may be devised from a qualitative exploration of the data or theories and related work. In context of our projects we are going to use both approaches: We will annotate a subset of our data as ground-truth while coding the total data using computational approaches. On code development there exists another large body of literature, like the Grounded Theory (e.g. Corbin and Strauss 2008) and Ethnic Coding Approach (Altheide 1987).\nFor the final analysis we are going to apply statistical data analyses. For an initial understanding of our data we will start with some exploratory analyses, e.g. plotting the data. In combination with the two approaches below, the platform affordances and platform vernaculars & grammars we may discover patterns of social media use. In most cases, our projects will compare different groups: These groups might be different user types (e.g. Politician Accounts vs. Party Accounts), or different Posts types (e.g. Posts vs. Stories), or different platforms (e.g. Instagram vs. TikTok).\n\n\nPlatform Affordances\nBossetta (2018) provides an overview of the concept of affordances and their application in social media analyses. He traces the term back to boyd and Papacharissi & Yuan who argued “that digital communication tech- nologies provide structural affordances to agents” (p. 473 Bossetta 2018). There are two important take-aways from his work: 1) The concept of affordances is not used consistently, and 2) the platforms shape affordances and thereby how users interact with the platform. Bainotti, Caliandro, and Gandini (2020) used the “Instagram-specific digital objects” as codes for their analysis of stories, linking the concept of affordances in the context of Instagram to the use of stickers.\nIn the context of our seminar we might consider the following elements as platform affordances:\n\n\n\nTikTok\nIG – Posts\nIG – Stories\n\n\n\n\nLikes\nLikes\nSliders\n\n\nComments\nComments\nVotes\n\n\nShares\nViews\nQuestions\n\n\nMusic\nMentions\nMentions\n\n\nHashtags\nHashtags\nHashtags\n\n\n…\n…\nLocations\n\n\n\n\n…\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDid you spot the difference between some of the listed affordances? Likes and comments, for instance, are reactions to posts. Would you consider these features as affordances? Let’s discuss this is in class!\n\n\n\n\nPlatform Vernaculars & Grammars\nPrevious studies have looked into ‘grammars’ in Instagram stories. Originally linked to research on privacy (Agre 1994), grammars classify activities using specific types, making data collection and analysis easier. This uncovers patterns in user behavior, beneficial for purposes such as advertising. To the best of my knowledge, this concept was first used for social media data by Gerlitz and Rieder (2018) in a Twitter study.\nOmena, Rabello, and Mintz (2020) discussed a “grammar of hashtags”, referring to the rules of hashtag use and how they’re organized on platforms. They suggest that hashtags, content visibility, and the nature of the content itself are essential in understanding hashtag use. Meanwhile, Bainotti, Caliandro, and Gandini (2020) used grammars to understand Instagram Stories, focusing on visual elements and their cultural meanings.\nLastly, Gibbs et al. (2015) examined the unique styles and logics of social media, termed “platform vernaculars”. These are influenced both by platform features and user habits."
  },
  {
    "objectID": "getting-started/theory.html#summary",
    "href": "getting-started/theory.html#summary",
    "title": "Introduction to SMA",
    "section": "Summary",
    "text": "Summary\nIn this chapter we have positioned ourselves between several disciplines: The computational social science, computational communication science, and digital humanities. In this position, we see social media data as trace data of human and social behaviour. The digitalness of our subject is, however, just one side of the coin: Follwing the theoretical frameworks of Digital Methods and Cultural Analytics, we want to conduct our analyses computationally with the aim to uncover patterns and trends of user behaviour on social media plattforms. Methodologically we can draw from quantitative content analysis, and the concept of platform affordances as features, and apply the concept of platform vernaculars and grammars to make sense of these features."
  },
  {
    "objectID": "getting-started/theory.html#additional-resources",
    "href": "getting-started/theory.html#additional-resources",
    "title": "Introduction to SMA",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nConferences\n\nInternational Conference on Social Media & Society\nIC²S² 2022\nICWSM\nAoIR\nWebSci\nInternational Conference on CMC and Social Media Corpora for the Humanities\n\n\n\nJournals\n\nNew Media & Society\nBig Data & Society\n\n\n\nTextbooks\n\nRose (2016): Visual Methodologies: An Introduction to Researching with Visual Materials.\nHaim (2023): Computational Communication Science: Eine Einführung.\nQuan-Haase and Sloan (2022b): The SAGE handbook of social media research methods.\n\n\n\nOnline Resources\n\nRichard Rogers: Social Media Research with Digital Methods (YouTube)\n\n\n\n\n\n\n\nNote\n\n\n\nDo you know of any ressources to be added to this list? Drop me a line: michael.achmann@ur.de.\n\n\n\n\n\nReferences\n\n\nAgre, Philip E. 1994. “Surveillance and capture: Two models of privacy.” The Information Society 10 (2): 101–27. https://doi.org/10.1080/01972243.1994.9960162.\n\n\nAltheide, David L. 1987. “Reflections: Ethnographic content analysis.” Qualitative Sociology 10 (1): 65–77. https://doi.org/10.1007/BF00988269.\n\n\nAnderson, Chris, Medea Giordano, Matt Jancer, Philip Ball, Will Knight, Sassafras Lowrey, and Laurence Scott. 2008. “The End of Theory: The Data Deluge Makes the Scientific Method Obsolete.” Wired, June. https://www.wired.com/2008/06/pb-theory/.\n\n\nArnold, Taylor, and Lauren Tilton. 2019. “Distant viewing: analyzing large visual corpora.” Digital Scholarship in the Humanities 34 (Supplement_1): i3–16. https://doi.org/10.1093/llc/fqz013.\n\n\nAtteveldt, Wouter van, and Tai-Quan Peng. 2018. “When Communication Meets Computation: Opportunities, Challenges, and Pitfalls in Computational Communication Science.” Communication Methods and Measures 12 (2-3): 81–92. https://doi.org/10.1080/19312458.2018.1458084.\n\n\nBainotti, Lucia, Alessandro Caliandro, and Alessandro Gandini. 2020. “From archive cultures to ephemeral content, and back: Studying Instagram Stories with digital methods.” New Media & Society, September, 1461444820960071. https://doi.org/10.1177/1461444820960071.\n\n\nBossetta, Michael. 2018. “The Digital Architectures of Social Media: Comparing Political Campaigning on Facebook, Twitter, Instagram, and Snapchat in the 2016 U.S. Election.” Journalism & Mass Communication Quarterly 95 (2): 471–96. https://doi.org/10.1177/1077699018763307.\n\n\nCorbin, Juliet M, and Anselm L Strauss. 2008. Basics of qualitative research: techniques and procedures for developing grounded theory. Sage Publications, Inc.\n\n\nDöring, Nicola, and Jürgen Bortz. 2016. Forschungsmethoden und Evaluation in den Sozial- und Humanwissenschaften. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-41089-5.\n\n\nGerlitz, and Rieder. 2018. “Tweets are not created equal: Investigating Twitter’s client ecosystem.” International Journal of Communication Systems, no. 12: 528–47. https://pure.uva.nl/ws/files/23266519/5974_30096_2_PB.pdf.\n\n\nGibbs, Martin, James Meese, Michael Arnold, Bjorn Nansen, and Marcus Carter. 2015. “#Funeral and Instagram: death, social media, and platform vernacular.” Information, Communication and Society 18 (3): 255–68. https://doi.org/10.1080/1369118X.2014.987152.\n\n\nHaim, Mario. 2023. Computational Communication Science: Eine Einführung. Springer Fachmedien Wiesbaden.\n\n\nHochman, Nadav, and Lev Manovich. 2013. “Zooming into an Instagram City: Reading the local through social media.” First Monday, June. https://doi.org/10.5210/fm.v18i7.4711.\n\n\nJockers, Matthew L. 2013. Macroanalysis: Digital Methods and Literary History. University of Illinois Press.\n\n\nKanthawala, Shaheen, Kelley Cotter, Kali Foyle, and J R Decook. 2022. Proceedings of the 55th Hawaii international conference on system sciences. Proceedings of the ... Annual Hawaii International Conference on System Sciences. Annual Hawaii International Conference on System Sciences. Hawaii International Conference on System Sciences. https://doi.org/10.24251/hicss.2022.000.\n\n\nLazer, David, Alex Pentland, Lada Adamic, Sinan Aral, Albert-Laszlo Barabasi, Devon Brewer, Nicholas Christakis, et al. 2009. “Social science. Computational social science.” Science 323 (5915): 721–23. https://doi.org/10.1126/science.1167742.\n\n\nLeaver, Tama, Tim Highfield, and Crystal Abidin. 2020. Instagram: Visual Social Media Cultures. John Wiley & Sons.\n\n\nManovich, Lev. 2020. Cultural Analytics. MIT Press.\n\n\nMcCrow-Young, Ally. 2021. “Approaching Instagram data: reflections on accessing, archiving and anonymising visual social media.” Communication Research and Practice 7 (1): 21–34. https://doi.org/10.1080/22041451.2020.1847820.\n\n\nMoretti, Franco. 2000. “Conjectures on World Literature.” New Left Review II (1): 54–68. https://newleftreview.org/issues/ii1/articles/franco-moretti-conjectures-on-world-literature.\n\n\nOmena, Janna Joceli, Elaine Teixeira Rabello, and André Goes Mintz. 2020. “Digital Methods for Hashtag Engagement Research.” Social Media + Society 6 (3): 2056305120940697. https://doi.org/10.1177/2056305120940697.\n\n\nQuan-Haase, Anabel, and Luke Sloan. 2022a. “Chapter 1: Introduction.” In The SAGE handbook of social media research methods, edited by Anabel Quan-Haase and Luke Sloan, 2nd ed., 1–9. London, England: SAGE Publications. https://doi.org/10.4135/9781529782943.\n\n\n———. 2022b. The SAGE handbook of social media research methods. Edited by Anabel Quan-Haase and Luke Sloan. 2nd ed. London, England: SAGE Publications. https://doi.org/10.4135/9781529782943.\n\n\nRat für Sozial- und Wirtschaftsdaten (RatSWD). 2019. “Big Data in den Sozial-, Verhaltens- und Wirtschaftswissenschaften: Datenzugang und Forschungsdatenmanagement - Mit Gutachten \"Web Scraping in der unabhängigen wissenschaftlichen Forschung\".” RatSWD Output. German Data Forum ( RatSWD). https://doi.org/10.17620/02671.39.\n\n\nRejeb, Abderahman, Karim Rejeb, Alireza Abdollahi, and Horst Treiblmaier. 2022. “The big picture on Instagram research: Insights from a bibliometric analysis.” Telematics and Informatics 73 (September): 101876. https://doi.org/10.1016/j.tele.2022.101876.\n\n\nRogers, Richard. 2013. Digital Methods. MIT Press.\n\n\nRose, Gillian. 2016. Visual Methodologies: An Introduction to Researching with Visual Materials. SAGE Publications.\n\n\nVenturini, Tommaso, and Richard Rogers. 2019. “‘API-Based Research’ or How can Digital Sociology and Journalism Studies Learn from the Facebook and Cambridge Analytica Data Breach.” Digital Journalism 7 (4): 532–40. https://doi.org/10.1080/21670811.2019.1591927.\n\n\nZimmer, Michael. 2010. “\"But the Data is Already Public\": On the Ethics of Research in Facebook.” Ethics and Information Technology 12 (4): 313–25. https://doi.org/10.1007/s10676-010-9227-5."
  },
  {
    "objectID": "getting-started/related-work.html",
    "href": "getting-started/related-work.html",
    "title": "Related Work",
    "section": "",
    "text": "While the two visual platforms Instagram and TikTok, are relatively new, plenty of research has already been published about both platforms. A naive search on google scholar for the term instagram analysis results in 4.180.000 results, for tiktok analysis in 54.800 results. We are going to take a look at current literature review studies, concentrating on Instagram. The goal for this chapter is to identify major research areas in (visual) social media research. Beyond themes, trends, and topics, review studies also offer methodological overviews on how to study social media platforms.\nAdditionally, we will explore tools like Publish or Perish that help in creating one’s own literature review. The Related Work section forms a pivotal foundation for high-quality scientific research, and a successful project report."
  },
  {
    "objectID": "getting-started/related-work.html#literature-reviews",
    "href": "getting-started/related-work.html#literature-reviews",
    "title": "Related Work",
    "section": "Literature Reviews",
    "text": "Literature Reviews\nRejeb et al. (2022) compiled a bibliometric analysis of 2,242 publications collected from the Web of Science1 database. They cover publications dated from 2013–2021 and outline 22 prior review studies, most of them concentrating on a smaller scope. Topics of these reviews include: Health, Psychology, Journalism, Mental Health, Body Image, and Marketing. Overall, their bibliographic study found similar themes in the current research: Some articles analyse the use of Instagram in the context of business, marketing, and travel. Others take a psychological angle and look into personality traits or health issues. They also found scholarly articles on privacy concerns and Instagram. Here’s some research interests they encountered in their review:\n\n\nHow does Instagram affect social and health issues, such as social comparison, eating disorders, addiction, and suicidal ideation?\nHow does Instagram facilitate and transform healthcare?\nWhat are the security and privacy concerns that result from the use of Instagram?\nHow does Instagram inter-relate with other social media platforms, such as Facebook and Twitter?\nWhat are the emerging research trends and frontiers in Instagram research?\n\n\nThey found researchers to use a multitude of methods, including surveys and questionnaires; content analysis to examine user-generated content; experimental designs to test the effects of Instagram use on users’ psychological states and behaviors; and qualitative methods, such as interviews and focus groups, to gain in-depth insights into users’ experiences with Instagram.\nInterestingly the bibliometric study seems to overlook a larger portion of research covering political communication on Instagram. Bast (2021) concentrates on this exact topic, she reviewed 37 studies on Instagram usage by politicians, parties, and governments. 30 studies were concerned with the Instagram use of political actors. They explored different aspects, like the self-presentation of politicians, mobilization and campaign information or whether they used Instagram to talk about political issues or interact with voters. Some of the studies use a comparative approach, e.g. comparing the Instagram activity of multiple actors, others compared the Instagram usage of political actors across different countries, political systems, or election/non-election periods (Bast 2021).\nFrom a methodological point of view the review of visual content analysis for image-based social media by Milanesi and Guercini (2020) is quite interesting: They included 29 articles in their study and explored the platforms, that have been invastigated as well as the approach, whether the analysis was manual or automated. Outstanding at first is the large share of projects that have been classified as using automated approaches. Upon closer inspection, they have also classified the use of qualitative data analysis software like NVivo, as automation. Few projects, however, have already been using deep learning and computer vision based approaches for image analysis. Finally, the paper suggests that a mixed methodology that combines a netnographic approach, a research methodology that adapts ethnographic research techniques to the study of online communities, for textual and visual data collection in online communities and textual and visual content analysis may provide new insights for branding or destination management research. Overall, they argue for a combined analysis of textual and visual data. It should be noted, that their review focuses on literature from marketing research.\nOverall, each of the outlined reviews has a different focus. Taken together, they display a large variety of different fields and questions, which Instagram content helps to answer. We can use these literature reviews in two way: We can identify patterns of how to approach social media content, how to operationalize, what questions to ask, what methods to use, and – looking at the future work sections of the reviews and the reviewed papers, where to pick up! Secondly, the literature review helps us to identify interesting literature for our own related work section and reading."
  },
  {
    "objectID": "getting-started/related-work.html#selected-articles",
    "href": "getting-started/related-work.html#selected-articles",
    "title": "Related Work",
    "section": "Selected Articles",
    "text": "Selected Articles\nThrought the next passages I’d like to introduce few interesting pieces. First, I’ll outline some of the first papers concerned with Instagram content. Thereafter we proceed to take a look at Instagram stories and ephemeral content in social media.\nOne of the first analyses of Instagram content was published in 2013: The article explores how the interfaces of social media platforms like Instagram shape user interactions and the creation and sharing of media. Through computational analysis and visualizations of Instagram content, the authors study social and cultural patterns. They compare visual data from 13 global cities and provide a detailed analysis of photos from Tel Aviv, Israel, showing how such visualizations can offer insights into social, cultural, and political activities in specific locales over time​ (Hochman and Manovich 2013).\n\n\n\nScreenshot of the phototrails website visualizing 50.000 images per city.\n\n\nShortly afterwards, in 2014, one of the most cited studies about Instagram was published. It provides a comprehensive analysis of Instagram photo content and user types, using computer vision techniques and clustering. The authors collected Instagram data using the Instagram API and developed a coding scheme for categorizing the photos. They identified eight popular photo categories and five distinct types of Instagram users in terms of their posted photos. They also found that a user’s audience (number of followers) is independent of their shared photos on Instagram. This study was the first in-depth analysis of content and users on Instagram (Hu, Manikonda, and Kambhampati 2014).\n\nInstagram Stories\nStories, as a special format due to their ephemeral nature, and have often been evaded academic research. The freature has been introducted of Instagram Stories in 2016 Leaver, Highfield, and Abidin (2020). An early analysis of stories is part of a master thesis on Snapchat and Instagram: Through qualitative content analysis, observation and in-depth interviews Amancio (2017) found four narrative elements used by Snapchat and Instagram storytellers to tell their stories and construct a narrative. Looking at Instagram specifically, Bainotti, Caliandro, and Gandini (2020) investigated 292 Stories by private users using an ethnographic coding approach. They claim to have identified specific grammars by matching the content and context-of-use, the two main ones are: “a grammar for documentation and a grammar for interaction”. Other areas of interest for stories were ephemeral journalism (Vázquez-Herrero, Direito-Rebollal, and López-Garcı́a 2019) and Female Atheletes’ self-presentation (Li et al. 2021). Finally, just recently Towner and Muñoz (2022) published a first analysis of political communication in Instagram Stories, studying the stories published by the two U.S. presidential candidates in the 2020 campaign. The authors took a marketing perspective, and identified several flaws of the campaign: missed opportunities to share user-generated content, and inconsistencies to communication norms of the ephemeral format.\nOverall, stories have been explored by researchers from different domains. The ephemeral character sticks out in a world where the effort for deleting photos may be more expensive than keeping them (Mayer-Schönberger 2011). Thus, I see potential for many different cultural and societal questions to be answered by looking at this type of content, and great potential for using stories in our semester projects. In contrast to most other social media content, stories need to be collected in real time. This is a challenge for research and limits our questions to material that we may collect throughout the seminar.\n\n\nTikTok\n\n\n\n\n\n\nWork-In-Progress\n\n\n\nI have yet concentrated on literature about Instagram. An update for this section will be the outcome of our seminar!\n\n\n\n\nRecommended Reading\n\n\n\n\n\n\n\n\nReference\nTitle\nNote\n\n\n\n\nBainotti, Caliandro, and Gandini (2020)\nFrom archive cultures to ephemeral content, and back: Studying Instagram Stories with digital methods\nThis paper explores Instagram stories and their collection. The authors conduct a content analysis and derive different grammars for private Instagram stories.\n\n\nHaßler, Kümpel, and Keller (2021)\nInstagram and political campaigning in the 2017 German federal election. A quantitative content analysis of German top politicians’ and parliamentary parties’ posts\nA detailed analysis of the 2017 election campaign showcasing theory-driven operationalization and (manual) content analysis.\n\n\nOmena, Rabello, and Mintz (2020)\nDigital Methods for Hashtag Engagement Research\nIntroduction of a multilayer hashtags engagement research framework paired with the concept of grammars of action. Demonstrates an interesting concept of grouping users.\n\n\nRettberg (2018)\nSnapchat: Phatic Communication and Ephemeral Social Media\nOne of the first scholarly articles on ephemeral stories, originally introduced by Snapchat.\n\n\nSánchez-Querubı́n et al. (2023)\nPolitical TikTok: Playful performance, ambivalent critique and event-commentary\nAn interesting blueprint for doing research of political communication on TikTok; take a special look at the coding variables!\n\n\n…\nto be continued!\n…"
  },
  {
    "objectID": "getting-started/related-work.html#writing-the-related-work-section",
    "href": "getting-started/related-work.html#writing-the-related-work-section",
    "title": "Related Work",
    "section": "Writing the Related Work section",
    "text": "Writing the Related Work section\nThe aim of our project paper diverges somewhat from a comprehensive literature review, such as those that commonly serve as the start for dissertations. Nevertheless, the “Related Work” section of your paper is as an important element of your research project. The goal here is to showcase a thorough understanding of the existing literature in your field of study. This enables you to position your research within the broader academic context, highlighting its relevance and identifying gaps that your project seeks to address. It is important to discuss your findings in this section, offering insights into the methodologies, findings, and limitations of the studies you review. Here are some steps to follow:\n\nAsk yourself: What is your research interest?\n\nWrite down key-words for your research interest.\nUsing the key-words, start your initial search with e.g. the Quick and Dirty strategy. Using the first results, start an in-depth search based on other strategies.\nWrite notes to retain search terms and selected results. Tools like Obsidian or Notion are excelent tools for notes, Excel or Google Sheets are simple, yet efficient, tools to structure your searches and selected literature (and we can export the data as csv files to process them using Python). Publish or Perish is a great tool to help in this stage, as it retains a protocol of your searches and offers the data export of search results.\nConcentrate on reading the abstract in your initial searches. We have to work efficiently, the abstract should contain the most relevant information about a given article for a first evaluation of its importance for your project.\nUse literature management software like Paperpile, Zotero, or Citavi to organize your reading. You might start using the software already at the skimming and abstract reading stage, once the reading starts, however, I would absolutely recommend to add the read articles to the managment software: Keep the PDFs organized using the software, keep your annotations in there, keep your notes in there!\nAt the end of your literature search process, you should be able to write the related work section of your project report and methods section. The related work section is part of your introduction and should include a summary and analysis of the relevant studies and research that has been conducted on your topic. Furthermore, your method section should ideally contain references to previous studies that have used similar methods or approaches.\n\n\n\n\n\n\n\nWork-In-Progress\n\n\n\nAs a warm-up with ChatGPT / GPT we will extract information from abstracts in out tools session. The notebook will be added shortly, you will be able to use this approach with Publish or Perish lists.\n\n\n\nPublish or Perish\nPublish or Perish is a neat piece of software, that helps documenting your literature review process. It provides a unified interface to a majority of databases. Each search can be saved, multiple searches can be organized into folders. Additionally, the results can be exported to different formats. Thus, Publish or Perish is also a good starting point for AI-Assisted literature reviews.\n\n\n\n\n\n\n\nConnected Papers\nConnected Papers is one of my favorite tools for literature reseraches. Paste any DOI into the search field and the tool will create a graph of the article, linking the cited literature as well as incorporating newer literature that cites the work that you’ve been looking for. Using colors and node sizes all data is visualised neatly.\n\n\n\nAI Tools\nOver the past months, several AI Literature Review tools have been released:\n\nPerplexity incorporates GPT-3 (GPT-4 and Claude-2 in the pro version) and offers a chat interface. You can ask any question, it starts answering your question based on sources which are provided in the interface.\nElicit works somewhat differently, it expects you to ask a research question and tries to answer you question based on papers and has the ability to extract different type of information from papers automatically. In my experience the system does not work that well for social science questions.\nChatPDF is one of many tools that allow to upload PDF files, process them, and allow to chat with their content. In my experience it works rather well. However, as with all AI tools, we should be careful to manually verify the responses. The tool returns a link to the text anchor it refers to for answers. Overall, I recommend this tool for refinding information in papers that you’ve already read, or as a companion for skimming papers – although you might miss out important information!\nLangChain and LlamaIndex are python package that help building applications like ChatPDF yourself.\n\n\n\n\nReferences\n\n\nAmancio, Marina. 2017. “‘Put it in your Story’: Digital Storytelling in Instagram and Snapchat Stories.” PhD thesis. https://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1111663&dswid=-5700.\n\n\nBainotti, Lucia, Alessandro Caliandro, and Alessandro Gandini. 2020. “From archive cultures to ephemeral content, and back: Studying Instagram Stories with digital methods.” New Media & Society, September, 1461444820960071. https://doi.org/10.1177/1461444820960071.\n\n\nBast, Jennifer. 2021. “Politicians, Parties, and Government Representatives on Instagram: A Review of Research Approaches, Usage Patterns, and Effects.” Review of Communication Research 9 (July). https://www.rcommunicationr.org/index.php/rcr/article/view/108.\n\n\nHaßler, Jörg, Anna Sophie Kümpel, and Jessica Keller. 2021. “Instagram and political campaigning in the 2017 German federal election. A quantitative content analysis of German top politicians’ and parliamentary parties’ posts.” Information, Communication and Society, July, 1–21. https://doi.org/10.1080/1369118X.2021.1954974.\n\n\nHochman, Nadav, and Lev Manovich. 2013. “Zooming into an Instagram City: Reading the local through social media.” First Monday, June. https://doi.org/10.5210/fm.v18i7.4711.\n\n\nHu, Yuheng, Lydia Manikonda, and Subbarao Kambhampati. 2014. “What We Instagram: A First Analysis of Instagram Photo Content and User Types.” Proceedings of the International AAAI Conference on Web and Social Media 8 (1): 595–98. https://doi.org/10.1609/icwsm.v8i1.14578.\n\n\nLeaver, Tama, Tim Highfield, and Crystal Abidin. 2020. Instagram: Visual Social Media Cultures. John Wiley & Sons.\n\n\nLi, Bo, Olan K M Scott, Michael L Naraine, and Brody J Ruihley. 2021. “Tell Me a Story: Exploring Elite Female Athletes’ Self-Presentation via an Analysis of Instagram Stories.” Journal of Interactive Advertising 21 (2): 108–20. https://doi.org/10.1080/15252019.2020.1837038.\n\n\nMayer-Schönberger, Viktor. 2011. Delete: The Virtue of Forgetting in the Digital Age. Princeton University Press.\n\n\nMilanesi, Matilde, and Simone Guercini. 2020. “Image-based Social Media and Visual Content Analysis: Insights from a Literature Review.” Micro & Macro Marketing, no. 3: 537–58. https://ideas.repec.org/a/mul/jyf1hn/doi10.1431-97640y2020i3p537-558.html.\n\n\nOmena, Janna Joceli, Elaine Teixeira Rabello, and André Goes Mintz. 2020. “Digital Methods for Hashtag Engagement Research.” Social Media + Society 6 (3): 2056305120940697. https://doi.org/10.1177/2056305120940697.\n\n\nRejeb, Abderahman, Karim Rejeb, Alireza Abdollahi, and Horst Treiblmaier. 2022. “The big picture on Instagram research: Insights from a bibliometric analysis.” Telematics and Informatics 73 (September): 101876. https://doi.org/10.1016/j.tele.2022.101876.\n\n\nRettberg, Jill Walker. 2018. “Snapchat: Phatic Communication and Ephemeral Social Media.” In Appified: Culture in the Age of Apps, edited by Jeremy Wade Morris and Sarah Murray, 188–95. “University of Michigan Press.”\n\n\nSánchez-Querubı́n, Natalia, Shuaishuai Wang, Briar Dickey, and Andrea Benedetti. 2023. “Political TikTok: Playful performance, ambivalent critique and event-commentary.” In The Propagation of Misinformation in Social Media, edited by Richard Rogers, 187–206. A Cross-Platform Analysis. Amsterdam University Press. https://doi.org/10.2307/jj.1231864.12.\n\n\nTowner, Terri L, and Caroline Lego Muñoz. 2022. “A Long Story Short: An Analysis of Instagram Stories during the 2020 Campaigns.” Journal of Political Marketing, July, 1–14. https://doi.org/10.1080/15377857.2022.2099579.\n\n\nVázquez-Herrero, Jorge, Sabela Direito-Rebollal, and Xosé López-Garcı́a. 2019. “Ephemeral Journalism: News Distribution Through Instagram Stories.” Social Media + Society 5 (4): 2056305119888657. https://doi.org/10.1177/2056305119888657."
  },
  {
    "objectID": "getting-started/related-work.html#footnotes",
    "href": "getting-started/related-work.html#footnotes",
    "title": "Related Work",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAccessible via VPN / on campus (when connected to eduroam).↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on Computational Social Media Research",
    "section": "",
    "text": "Welcome to this collection of notes on social media analysis with a special focus on computational methods. It is a work-in-progress website, created as part of my PhD project and teaching at the Media Informatics Group at the University of Regensburg, Germany. My name is Michael Achmann-Denkler and I’m currently experimenting with computational approaches for multimodal analysis of social media content, like Instagram posts and stories. My aim for this website is to develop a collection of notes exploring various methodologies, techniques, and tools for social media research. As a first milestone, the website will accompany my research seminar Computational Analysis of Visual Social Media in the 2023/24 winter semester."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Notes on Computational Social Media Research",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\nDate\nContent\n\n\n\n\n16.10.23\nCourse Organization\n\n\n23.10.23\nIntroduction to Social Media Analysis\n\n\n30.10.23\nProjects & Groups,  Getting Started: Tools\n\n\n06.11.23\nData Collection: IG Posts & Stories\n\n\n13.11.23\nData Collection: TikTok\n\n\n20.11.23\n- entfallen -\n\n\n27.11.23\nData Preprocessing: OCR & Whisper\n\n\n04.12.23\nExploration of Textual Data using GPT\n\n\n11.12.23\nOperationalization I & Computational Text Classification using GPT\n\n\n18.12.23\nData Annotation: LabelStudio & Annotation Guides\n\n\n08.01.24\nEvaluation I: Optimizing Text Classification\n\n\n15.01.24\nExploration of Visual Data\n\n\n22.01.24\nOperationalization II & Computational Image Classification using CLIP\n\n\n29.01.24\nEvaluation II: Optimizing Image Classification\n\n\n05.02.24\nData Analysis as a Conversation: Exploring trends using ChatGPT Visual Presentation of your Data & Results: RAWGraphs and more."
  },
  {
    "objectID": "index.html#citation-and-licences",
    "href": "index.html#citation-and-licences",
    "title": "Notes on Computational Social Media Research",
    "section": "Citation and Licences",
    "text": "Citation and Licences\nThe website repository is available on GitHub and registered with Zenodo . Please use the citation data provided by Zenodo when quoting parts of this website in academic work. Code examples and computational notebooks are published on the supplement repository, which is also registered with Zenodo . All text content on this website is published under the creative commons attribution (CC-BY) license. All code is released under the GNU GPLv3."
  },
  {
    "objectID": "processing/index.html",
    "href": "processing/index.html",
    "title": "Text as Data",
    "section": "",
    "text": "The analysis of texutal data has a long tradition under the term Natural Language Processing (NLP). As noted by Bengfort, Bilbro, and Ojeda (2018), “Language is unstructured data that has been produced by people to be understood by other people”. This characterization of language as unstructured data highlights its contrast with structured or semi-structured data. Unlike structured data, which is organized in a way that computers can easily parse and analyze, unstructured data like language requires more complex methods to be processed and understood. In the context of e.g. Instagram, CrowdTangle exports contain structured data columns such as ‘User Name’, ‘Like Count’, or ‘Comment Count’. These pieces of data are quantifiable and can be easily sorted, filtered, or counted, e.g. using tools like Excel or Python’s pandas library. For instance, we can quickly determine the most active users by counting the number of rows associated with each username. In contrast, unstructured data is not organized in a predefined manner and is typically more challenging to process and analyze. The ‘Description’ column in our dataset, which contains the captions of Instagram posts, is a prime example of unstructured data. These captions, composed of paragraphs or sentences, require different analytical approaches to extract meaningful insights. Unlike structured data, we cannot simply count or sort these texts in a straightforward manner. In our context, we often refer to the collection of texts we analyze as a “Corpus”. Each individual piece of text is called a “Document”. Each document can be broken down into smaller units known as “features”. Features can be words, phrases, or even patterns of words, which we then use to quantify and analyze the text (compare p. 230 Haim 2023). For the goal of our research seminar, we can follow the three technical perspectives inspired by Haim (2023): 1. Frequency Analysis, 2. Contextual Analysis, and 3. Content Analysis."
  },
  {
    "objectID": "processing/index.html#schedule",
    "href": "processing/index.html#schedule",
    "title": "Text as Data",
    "section": "Schedule",
    "text": "Schedule\n\nIn our first session, we begin with frequency analyses of our corpus, which involves counting words or phrases to identify the most common elements. This method provides a foundational understanding of the prominent themes or topics. Additionally, we learn to convert embedded text in images and videos into machine-readable format, using OCR, and automated audio transcription.\nNext, we will engage in explorative text analysis. This step enhances our understanding of the corpus and lays the groundwork for quantitative content analysis. We plan to utilize tools like GPT (and possibly BERTopic for an in-depth exploration of our documents.\nFinally, we move towards more complex methods like classification or coding. These techniques allow us to categorize text into predefined groups or themes, enabling a more nuanced and quantitative understanding of the content. By applying these methods, we can, for example, classify Instagram captions into categories such as ‘promotional’, ‘personal’, ‘informative’, etc., based on their content and context."
  },
  {
    "objectID": "processing/index.html#hands-on",
    "href": "processing/index.html#hands-on",
    "title": "Text as Data",
    "section": "Hands-On",
    "text": "Hands-On\nWe are working with Python and pandas, our data is structured in tables, also known as DataFrames. Each DataFrame (df) consists of rows and columns. We can store and structure data differently using these two dimensions, one concept for storing research data using tables is Tidy Data (Wickham 2014). According to this standard\n\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\n\n\nVisualization of the tidy data components, source: R for Data Science.\n\n\nWhat, in context of social media data, is an obersavtion? Is it a post? I suggest to start by seeing posts as observations, i.e. rows. Thus, we have one table for our corpus, consisting of one row per post with multiple columns for different variables, including an ID, possibly a link, a referrence to the image / video, and one or more text variables for each post. When dealing with Instagram or TikTok posts, we might have three text columns: caption / description, OCR, and transcription. When dealing with stories two: OCR and transcription.\n\n\n\n\n\n\nNote\n\n\n\nWhen dealing with more complex data, e.g. Instagram albums that may contain multiple images per post, we will have to reconsider this choice. In this case we might consider each observation to be one image / video, which has variables like OCR and transcription. Keeping the ID column for images and videos, we have a fixed reference to the original post, thus we may re-merge the data later on with the post metadata or combine variables across media for one post.\n\n\nAll data exported from CrowdTangle, 4CAT, and Zeeschuimer-F are saved as CSV files. Throughout the semester, we keep using this file format to save our progress. We work with multiple Jupyter notebooks, generally one notebook per task. This helps to keep a good structure of our projects. Each time we modified the df, we save the CSV file to our Google Drive / Harddrive. In the two examples below we add an OCR and a Transcription column to our DataFrame, for each task we use one notebook. After completing each task, we store the results in a file. While Google Drive provides file versioning to mitigate data loss in certain scenarios, I recommend to save your results to a new file during the experimental phase. This practice ensures data safety until you have fully verified the functionality of your code. Additionally, I recommend naming your files in a YYYY-MM-DD-descriptive-name.csv fashion. When working with colab notebooks I recommend to keep track of notebooks using notes / lists, e.g. using the Dataloom plugin for Obsidian.\n\n\n\nKeeping track of Colab notebooks with Obsidian and the Dataloom plugin.\n\n\nThe CSV files contain only metadata, the actual media files (images / videos) are saved to different locations. The OCR and Transcription notebooks below contain code to import media files from 4CAT and Zeeschuimer-F. I suggest to save the files to media/videos or media/images. Both notebooks introduce a column image_file or video_file where the relative location of the media files is written to. Creating a new ZIP file using the new folder structure and saving the file to Google Drive allows us to use the media files in future notebooks (e.g. for image classification) without modifying the image_file or video_file columns again.\n\n\n\n\n\n\nNote\n\n\n\nThis page and all referenced notebooks deal with 4CAT and Zeeschuimer-F metadata and media files. Generally all information applies to instaloader as well. Its advisable to use the --filename-pattern command line parameter to control the filename of the media files. Mapping JSON metadata to actual media objects becomes easier this way. Once all posts / stories have been loaded using instaloader, I recommend to read all JSON files in a loop and create a DataFrame (see Data Collection / Posts / Instaloader for more information and code examples).\n\n\nKey Take-Aways\n\nWe organize our data inspired by TidyData\n\nOne row per post\nOne column per variable\n\nWe use one notebook per task\nWe save our progress to CSV files, either on our harddrive or Google Drive\nWe keep a reference to media files as a relative reference in our DataFrame\nWe keep our media files in the structure media/videos, and media/images, which we compress to ZIP and keep on our Google Drive (or central HDD location)\nWhen working with experimental code, keep backups of your data file, do not overwrite the original file!"
  },
  {
    "objectID": "processing/index.html#from-images-videos-to-text",
    "href": "processing/index.html#from-images-videos-to-text",
    "title": "Text as Data",
    "section": "From Images / Videos to Text",
    "text": "From Images / Videos to Text\nComputational approaches for text analyses are established as part of computational sociales science research (Baden et al. 2022), which we may utilize when dealing with visual and multimodal social media. Instagram posts often contain embedded text, TikTok posts often contain an audio layer, both of which we can transform to computer readable text. For the first, we are going to use OCR, for the second we apply Whisper. The following subchapters demonstrate the application of these technique in order to extract textual content from images and videos. In the thirs subchapter, I demonstrate a simple application of corpus analytics for a first analysis of the social media content based on word frequencies.\n\nOCR\n\nWe’re using easyocr. See the documentation for more complex configurations. Using CPU only this process takes from minutes to hours (depends on the amount of images). OCR may also be outsourced (e.g. using Google Vision API), see future sessions (and Memespector) for this.\n\n!pip -q install easyocr\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 29.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 908.3/908.3 kB 57.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.2/307.2 kB 29.6 MB/s eta 0:00:00\n\n\n\n# Imports for OCR\nimport easyocr\nreader = easyocr.Reader(['de','en'])\n\nProgress: |██████████████████████████████████████████████████| 100.0% CompleteProgress: |██████████████████████████████████████████████████| 100.0% Complete\n\n\nWe define a very simple method to receive one string for all text recognized: The readtextmethod returns a list of text areas, in this example we concatenate the string, therefore the order of words is sometimes not correct.\nAlso, we save the file to Google Drive to save our results.\n\ndef run_ocr(image_path):\n    ocr_result = reader.readtext(image_path, detail = 0)\n    ocr_text = \" \".join(ocr_result)\n    return ocr_text\n\ndf['ocr_text'] = df['image_file'].apply(run_ocr)\n\n# Saving Results to Drive\ndf.to_csv('/content/drive/MyDrive/2022-11-09-Stories-Exported.csv')\n\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0.1\nUnnamed: 0\nID\nTime of Posting\nType of Content\nvideo_url\nimage_url\nUsername\nVideo Length (s)\nExpiration\n...\nIs Verified\nStickers\nAccessibility Caption\nAttribution URL\nvideo_file\naudio_file\nduration\nsampling_rate\nimage_file\nocr_text\n\n\n\n\n0\n0\n0\n3234500408402516260_1383567706\n2023-11-12 15:21:53\nImage\nNaN\nNaN\nnews24\nNaN\n2023-11-13 15:21:53\n...\nTrue\n[]\nPhoto by News24 on November 12, 2023. May be a...\nhttps://www.threads.net/t/CzjB80Zqme0\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234500408402516260_1383567706.jpg\nKeee WEEKEND NEWS24 PLUS: TESTING FORDS RANGER...\n\n\n1\n1\n1\n3234502795095897337_8537434\n2023-11-12 15:26:39\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:26:39\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234502795095897337_8537434.jpg\nDieses Auto ist einfach der Horror Du glaubst ...\n\n\n2\n2\n2\n3234503046678453705_8537434\n2023-11-12 15:27:10\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:27:10\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234503046678453705_8537434.jpg\nTouchdown bei Taylor Swift und Travis Kelce De...\n\n\n3\n3\n3\n3234503930728728807_8537434\n2023-11-12 15:28:55\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:28:55\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234503930728728807_8537434.jpg\nHorror-Diagnose für Barton Cowperthwaite Netfl...\n\n\n4\n4\n4\n3234504185910204562_8537434\n2023-11-12 15:29:25\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:29:25\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234504185910204562_8537434.jpg\n3v Bilde GG JJ Besorgniserregende Ufo-Aktivitä...\n\n\n\n\n\n5 rows × 21 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nSource: OCR using easyocr\n\n\nAutomated Audio Transcription (Whisper)\n\nExtract Audio from Video File\nAfter loading the metadta and media files from the Google Drive, we extract the audio from each video file to prepare the automated transcription.\n\n!pip install -q moviepy\n\n\nimport os\n\n# Set audio directory path\naudio_path = \"media/audio/\"\n\n# Check if the directory exists\nif not os.path.exists(audio_path):\n    # Create the directory if it does not exist\n    os.makedirs(audio_path)\n\n\nfrom moviepy.editor import *\n\nfor index, row in df.iterrows():\n    if row['video_file'] != \"\":\n        # Load the video file\n        video = VideoFileClip(row['video_file'])\n        filename = row['video_file'].split('/')[-1]\n\n        # Extract the audio from the video file\n        audio = video.audio\n\n        if audio is not None:\n            sampling_rate = audio.fps\n            current_suffix = filename.split(\".\")[-1]\n            new_filename = filename.replace(current_suffix, \"mp3\")\n\n            # Save the audio to a file\n            audio.write_audiofile(\"{}{}\".format(audio_path, new_filename))\n        else:\n            new_filename = \"No Audio\"\n            sampling_rate = -1\n\n        # Update DataFrame inplace\n        df.at[index, 'audio_file'] = new_filename\n        df.at[index, 'duration'] = video.duration\n        df.at[index, 'sampling_rate'] = sampling_rate\n\n        df.at[index, 'video_file'] = row['video_file'].split('/')[-1]\n\n        # Close the video file\n        video.close()\n\nMoviePy - Writing audio in media/audio/CzD93SEIi-E.mp3\nMoviePy - Done.\n\n\nWe’ve extracted the audio content of each video file to a mp3 file in the media/audio folder. The files keep the name of the video file. We added new columns to the metadata for audio duration and sampling_rate. In case the video did not include an audio file, smapling_rateis set to -1, which we use to filter the df when transcribing the files.\n\ndf[df['video_file'] != \"\"].head()\n\n\n  \n    \n\n\n\n\n\n\nid\nthread_id\nparent_id\nbody\nauthor\nauthor_fullname\nauthor_avatar_url\ntimestamp\ntype\nurl\n...\nnum_comments\nnum_media\nlocation_name\nlocation_latlong\nlocation_city\nunix_timestamp\nvideo_file\naudio_file\nduration\nsampling_rate\n\n\n\n\n4\nCzD93SEIi-E\nCzD93SEIi-E\nCzD93SEIi-E\nMitzuarbeiten für unser Land, Bayern zu entwic...\nmarkus.soeder\nMarkus Söder\nhttps://scontent-fra3-1.cdninstagram.com/v/t51...\n2023-10-31 12:06:23\nvideo\nhttps://www.instagram.com/p/CzD93SEIi-E\n...\n227\n1\nNaN\nNaN\nNaN\n1698753983\nCzD93SEIi-E.mp4\nCzD93SEIi-E.mp3\n67.89\n44100.0\n\n\n\n\n\n1 rows × 24 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\nLet’s update the ZIPed folder to include the audio files.\n\n!zip -r /content/drive/MyDrive/2023-11-24-4CAT-Images-Clean.zip media\n\nupdating: media/ (stored 0%)\nupdating: media/videos/ (stored 0%)\nupdating: media/videos/CzD93SEIi-E.mp4 (deflated 0%)\n  adding: media/audio/ (stored 0%)\n  adding: media/audio/CzD93SEIi-E.mp3 (deflated 1%)\n\n\nAnd save the updated metadata file. Change filename when importing stories here!\n\ndf.to_csv(four_cat_file_path)\n\nTranscriptions using Whisper\n\nThe Whisper model was proposed in Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.\n\n\nThe abstract from the paper is the following:\n\n\n\nWe study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.\n\n\n– https://huggingface.co/docs/transformers/model_doc/whisper\n\n!pip install -q transformers\n\nThe next code snippet initializes the Whisper model. The transcribe_aduio method is applied to each row of the dataframe where sampling_rate &gt; 0, thus only to those lines with referencees to audio files. Each audio file is transcribed using Whisper, the result, one text string, is saved to the transcript column.\nAdjust the language variable according to your needs! The model is also capable of automated translation, e.g. setting language to english when processing German content results in an English translation of the speech. (Additionally, the task variable accepts translate).\n\nimport torch\nfrom transformers import pipeline, WhisperProcessor, WhisperForConditionalGeneration\nimport librosa\n\n# Set device to GPU if available, else use CPU\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize the Whisper model pipeline for automatic speech recognition\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=\"openai/whisper-large\",\n    chunk_length_s=30,\n    device=device,\n)\n\n# Load model and processor for multilingual support\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n\n# Function to read, transcribe, and handle longer audio files in different languages\ndef transcribe_audio(filename, language='german'):\n    try:\n        # Load and resample audio file\n        audio_path = f\"{audio_folder}/{filename}\"\n        waveform, original_sample_rate = librosa.load(audio_path, sr=None, mono=True)\n        waveform_resampled = librosa.resample(waveform, orig_sr=original_sample_rate, target_sr=16000)\n\n        # Get forced decoder IDs for the specified language\n        forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=\"transcribe\")\n\n        # Process the audio file in chunks and transcribe\n        transcription = \"\"\n        for i in range(0, len(waveform_resampled), 16000 * 30):  # 30 seconds chunks\n            chunk = waveform_resampled[i:i + 16000 * 30]\n            input_features = processor(chunk, sampling_rate=16000, return_tensors=\"pt\").input_features\n            predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n            chunk_transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n            transcription += \" \" + chunk_transcription\n\n        return transcription.strip()\n    except Exception as e:\n        print(f\"Error processing file {filename}: {e}\")\n        return \"\"\n\n\n# Filter the DataFrame (sampling_rates &lt; 0 identify items without audio)\nfiltered_index = df['sampling_rate'] &gt; 0\n\n# Apply the transcription function to each row in the filtered DataFrame\ndf.loc[filtered_index, 'transcript'] = df.loc[filtered_index, 'audio_file'].apply(transcribe_audio)\n\n\ndf[df['video_file'] != \"\"].head()\n\n\n  \n    \n\n\n\n\n\n\nid\nthread_id\nparent_id\nbody\nauthor\nauthor_fullname\nauthor_avatar_url\ntimestamp\ntype\nurl\n...\nnum_media\nlocation_name\nlocation_latlong\nlocation_city\nunix_timestamp\nvideo_file\naudio_file\nduration\nsampling_rate\ntranscript\n\n\n\n\n4\nCzD93SEIi-E\nCzD93SEIi-E\nCzD93SEIi-E\nMitzuarbeiten für unser Land, Bayern zu entwic...\nmarkus.soeder\nMarkus Söder\nhttps://scontent-fra3-1.cdninstagram.com/v/t51...\n2023-10-31 12:06:23\nvideo\nhttps://www.instagram.com/p/CzD93SEIi-E\n...\n1\nNaN\nNaN\nNaN\n1698753983\nCzD93SEIi-E.mp4\nCzD93SEIi-E.mp3\n67.89\n44100.0\nIch bitte auf den abgelagerten Vortrag der Maa...\n\n\n\n\n\n1 rows × 25 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\n\ndf.loc[4, 'transcript']\n\n'Ich bitte auf den abgelagerten Vortrag der Maaßen-Söder-Entfühlen ein.  Erfüllung meiner Amtspflichten, so wahr mir Gott helfe. Ich schwöre Treue der Verfassung des Freistaates Bayern, Gehorsam den Gesetzen und gewissenhafte Erfüllung meiner Amtspflichten, so wahr mir Gott helfe. Herr Ministerpräsident, ich darf Ihnen im Namen des ganzen Hauses ganz persönlich die herzlichsten Glückwünsche aussprechen und wünsche Ihnen viel Erfolg und gute Nerven auch bei Ihrer Aufgabe. Herzlichen Dank.  Applaus'\n\n\nOverall, the transcriptions work well. The first sentence above, however, shows that we still can expect misinterpretations.\nSource: Transcription using Whisper\n\n\nAnalyzing Corpus and Word Frequencies\n\nAmong a variety of possibilities, we can, for example, look at the frequencies of the words contained in the corpus or examine the corpus for recurring themes it contains.\nFirst we need to import all the required libraries once again. The Natural Language Toolkit (NLTK) gives us access to a variety of natural language processing functions (e.g. tokenisation, stop word removal, part-of-speech tagging, …).\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport requests\nimport pandas as pd\n\nWhen analysing word frequencies, we can use stop word lists to ignore words that occur frequently but are not relevant to us. We can easily download such a list. However, this can also be individually adapted to the purpose.\n\n# Retrieve Stopwords from Github\nsw_json = requests.get('https://github.com/stopwords-iso/stopwords-de/raw/master/stopwords-de.json')\n\nNow we can tokenise the existing text, remove the stop words or punctuation marks they contain, convert the words to lower case, or use bi-grams in addition to single-word tokens.\nWe then sum up the occurrences of the individual words and make the results available in a DataFrame.\n\ndef word_freq(text, punctuation=False, stop_words = False, lowercasing = False, bigrams = False):\n\n    if punctuation:\n        # Tokenizing, removing punctuation\n        tokens = RegexpTokenizer(r'\\w+').tokenize(text) # https://regexr.com/\n    else:\n        # Tokenizing, w/o removing punctuation\n        # tokens = text.split()\n        tokens = word_tokenize(text)\n\n    if stop_words:\n        # Removing Stopwords\n        tokens = [w for w in tokens if not w.lower() in stop_words]\n\n    if lowercasing:\n        # Lower-Casing\n        tokens = [w.lower() for w in tokens]\n\n    if bigrams:\n        # Converting text tokens into bigrams\n        tokens = nltk.bigrams(tokens)\n\n    # Creating Data Frame\n    freq = nltk.FreqDist(tokens) # display(freq)\n    df = pd.DataFrame.from_dict(freq, orient='index')\n    df.columns = ['Frequency']\n    df.index.name = 'Term'\n\n    # Here we calculate the total number of tokens in our Frequency List\n    total_tokens = sum(freq.values()) # sum([2,3,4,5,6])\n\n    # Here we add a new column `Relative` (*100 for percentage)\n    df['Relative'] = (df['Frequency'] / total_tokens) * 100\n\n    return df\n\n\nfrom pathlib import Path\nimport os\n\n#@markdown Do you want bigrams included?\nbigrams = True #@param {type:\"boolean\"}\n\n#@markdown Should all words get lower cased before counting the occurances?\nlowercasing = True #@param {type:\"boolean\"}\n\n#@markdown Do you want to exclude stopwords in your result list?\nstopwords = True #@param {type:\"boolean\"}\n\n#@markdown Do you want to remove punctuation before counting the occurances?\npunctuation = True #@param {type:\"boolean\"}\n\n\n# Load stopwords file if necessary\nif stopwords:\n    stopwords = sw_json.json()\n\n# Read source file and concat all texts\ntext = ' '.join(list(df[text_column]))\n\n# Call word_freq() with specified parameters\ndf_freq = word_freq(text, punctuation = punctuation, stop_words = stopwords, lowercasing = lowercasing, bigrams = bigrams)\n\n# Sort results for descending values\ndf_freq = df_freq.sort_values(\"Relative\", ascending = False)\n\ndisplay(df_freq[0:10])\n\n\n  \n    \n\n\n\n\n\n\nFrequency\nRelative\n\n\nTerm\n\n\n\n\n\n\n(jüdisches, leben)\n5\n1.259446\n\n\n(allerheiligen, allerseelen)\n4\n1.007557\n\n\n(ilse, aigner)\n3\n0.755668\n\n\n(bayerischer, landtag)\n3\n0.755668\n\n\n(klare, haltung)\n2\n0.503778\n\n\n(wünschen, einfach)\n2\n0.503778\n\n\n(vaters, freundschaftliche)\n2\n0.503778\n\n\n(tod, vaters)\n2\n0.503778\n\n\n(günter, tod)\n2\n0.503778\n\n\n(schwiegervater, günter)\n2\n0.503778\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWordcloud\nOne way to visualise word frequencies and recurring themes of texts are word clouds. These basically show the most frequently occurring words in the text (similar to the table created earlier), but more frequently occurring words are depicted larger than less frequently occurring words.\nFirst, we have to install the necessary library wordcloud.\n\n!pip install -q wordcloud\n\nThe actual implementation of this approach is relatively simple. We need to combine all the texts into a single text, as we did in the previous step with the frequency analysis, and pass it to the imported library.\n\nfrom wordcloud import WordCloud, STOPWORDS\n\ndef generate_wordcloud(text, path):\n\n    text = ' '.join(list(text))\n\n    # Generate a word cloud image\n    wordcloud = WordCloud(background_color=\"white\",width=1920, height=1080).generate(text)\n\n    # Dazugehörige Grafik erstellen\n    plt.imshow(wordcloud, interpolation=\"bilinear\") # Auflösung/Interpolation der Grafik\n    plt.axis(\"off\")\n    plt.figtext(0.5, 0.1, wordcloud_subcaption, wrap=True, horizontalalignment='center', fontsize=12)\n    plt.savefig(path, dpi=300)\n    plt.show()\n\nOnce again, we have the option of adjusting various parameters. Remember to specify the right file path, file name and column of your text data!\n\n#@markdown Input for additional stopwords; whitespace separated\nstopwords_extension_wc = '' #@param {type: \"string\"}\n\n#@markdown Subcaption for the wordcloud, leave blank to ignore\nwordcloud_subcaption = 'Markus S\\xF6der' #@param {type: \"string\"}\n\nNow all we have to do is load the stop word file, add our own additions and then trigger the creation of the word cloud using the function we created at the beginning.\nThe result image is saved in the defined data_path.\n\nimport matplotlib.pyplot as plt\nimport requests\n\n# Retrieve Stopwords from Github\nr = requests.get('https://github.com/stopwords-iso/stopwords-de/raw/master/stopwords-de.json')\nstop_words = r.json()\n\n# Convert input into list\nstopwords_extension_wc_list = stopwords_extension_wc.split(' ')\nstop_words.extend(stopwords_extension_wc_list)\n\n# Stopwörter in die WordCloud laden\nSTOPWORDS.update(stop_words)\n\n\ngenerate_wordcloud(df[text_column], 'wordcloud.png')\n\n\n\n\nSource: Introduction to Corpus Analysis"
  },
  {
    "objectID": "processing/index.html#conclusion",
    "href": "processing/index.html#conclusion",
    "title": "Text as Data",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, this session provides us with the practical skills to use Python, pandas, and Jupyter notebooks for the computational analysis of multimodal social media data. Our adherence to Tidy Data principles and the integration of technologies like OCR and Whisper are integral to extract and analyze textual content from multimedia sources. In the next session we will keep exploring the content through a textual lens. Further, we will use prompting as a technique to classify texts as part of a computational content analysis."
  },
  {
    "objectID": "processing/index.html#more-resources",
    "href": "processing/index.html#more-resources",
    "title": "Text as Data",
    "section": "More Resources",
    "text": "More Resources\nPython & Computational Social Sciences\n\nPython for Computational Social Science and Digital Humanities (YouTube)\nIntroduction to Computational Social Science methods with Python (Online)\nIntroduction to Data Science: A Python Approach to Concepts, Techniques and Applications (E-Book)\nR for Data Science (2nd edition) – not Python, but the principles can easily be migrated to pandas.\n\nPython & NLP\n\nNatural Language Processing (Notebook, GESIS CSS)\nWord Frequencies (Online)\nIntroduction Jupyter Notebooks (Online)\nKonchady (2016): Text Mining Application Programming (Somewhat older, still an interesting reading for the basics of computational corpus analysis)"
  },
  {
    "objectID": "processing/operationalization.html",
    "href": "processing/operationalization.html",
    "title": "Operationalization",
    "section": "",
    "text": "Work-In-Progress\n\n\n\nWill be available for the 12/04 session.\n\n\n\n\n\nReuseLicenced under GNU Affero General Public License v3.0"
  },
  {
    "objectID": "data-collection/ig-stories.html",
    "href": "data-collection/ig-stories.html",
    "title": "Project Creation",
    "section": "",
    "text": "Instagram stories, characterized by their ephemeral nature, expire after 24 hours. Therefore, it’s crucial to collect them in a timely manner, as retrospective data collection is not an option with this format. There are two feasible methods: Instaloader and Zeeschuimer-F. Additionally, commercial tools such as 4k Stogram are also available.\nOverall, the ephemeral nature of stories necessitates our continuous monitoring and data collection from our targeted profiles. To ensure that we capture every story item, I recommend collecting stories twice daily, approximately 12 hours apart. This method accounts for potential inaccuracies in timing, as the intervals overlap. Data can be gathered manually or through computational means. The manual approach, especially in conjunction with Zeeschuimer-F, is preferable as it does not violate the Terms of Service (TOS). For this method, we would install the plugin and view all stories in our browser twice daily. Alternatively, using Instaloader involves simply initiating the command and waiting for the software to gather all the data. Optimally, we could utilize tools like Cron to automate this process."
  },
  {
    "objectID": "data-collection/ig-stories.html#instaloader",
    "href": "data-collection/ig-stories.html#instaloader",
    "title": "Project Creation",
    "section": "Instaloader",
    "text": "Instaloader\nInstaloader for Stories operates in a manner akin to collecting Posts. Initially, the package must be installed:\n!pip -q install instaloader\nUnlike the method outlined in the previous tutorial, I advise employing the command-line interface of Instaloader for collecting stories. To do this, open a terminal and execute the command below:\ninstaloader --login your.profile.name --dirname-pattern ig_stories/{profile} :stories --no-compress-json\nExecuting this command generates a dedicated subfolder within ig_stories for each user followed by your profile. It downloads the metadata, images, and videos of each story. The metadata is saved in a JSON file. While these files are typically xz-compressed by default, using the --no-compress-json option prevents this compression. Subsequently, the JSON files can be imported into a pandas DataFrame in Python.\nThis process can be automated, for example, by utilizing a bash script in conjunction with cron:\n#!/bin/bash\n\n# Generate a random number of seconds between 0 and 3600 (1 hours)\nsleep_duration=$(( RANDOM % 3600 ))\n\n# Print the sleep duration\necho \"Sleeping for $sleep_duration seconds...\"\n\n# Sleep for the random duration\nsleep $sleep_duration\n\n# Run Instaloader command to download the latest Instagram stories\ninstaloader --login your.profile.name --dirname-pattern ~/ig_stories/{profile} :stories  --no-compress-json\n\n# Add more script to check for success and send alerts in case of error\nStart cron by entering crontab -e on your terminal and add a line pointing to the bash script, e.g.:\n* 8,20 * * * /path/to/your/script.sh &gt;/dev/null 2&gt;&1\n\n\nPros:\n\n\nVery easy to automate\n\n\nCollects all data: metadata, images, videos\n\n\n\n\nCons:\n\n\nPossibly against the TOS\n\n\nRate Limits\n\n\nBlocked Accounts (very quickly)"
  },
  {
    "objectID": "data-collection/ig-stories.html#zeeschuimer-f",
    "href": "data-collection/ig-stories.html#zeeschuimer-f",
    "title": "Project Creation",
    "section": "Zeeschuimer-F",
    "text": "Zeeschuimer-F\n\nThis method is based on the Zeeschuimer Firefox Plugin. I have adapted the original plugin to create Zeeschuimer-F, which is specifically tailored for collecting Instagram stories and interfacing with the Zeeschuimer-Firebase-Backend for real-time data collection. You can find Zeeschuimer-F on GitHub. To use it, download the latest version via Firefox and install the plugin. For our seminar, I’ll provide a backend instance; refer to the README.md on GitHub for guidance on setting up your own instance on Firebase. Credentials for our seminar will be distributed through GRIPS. Follow these steps to download stories using Zeeschuimer-F:\n\nDownload and install the plugin.\nCreate a project on the backend (via Notebook).\nConfigure the plugin.\nRegularly view stories in Firefox to collect them.\nDownload the collected data (via Notebook).\n\n\nPlugin Installation\nTo install the plugin, download the latest release .xpi file from GitHub using Firefox. After downloading, click on the file in Firefox and confirm the installation of the extension.\n\n\n\nScreenshot of Firefox with the open extensions menu\n\n\nVerify the extension’s installation by checking the right-hand menu in Firefox. We will return to the browser shortly.\n\n\n\nThe Firebase Interface Notebook\n\nThe following lines of code assume that the firebase Credential File has been downloaded from GRIPS and uploaded to Colab / your Jupyter project path. First of all install the necessary packages:\n\n!pip -q install firebase-admin\n\nNext, we connect to our firebase project. Please update the credentials_path variable with the path to your credentials file (see above).\n\nimport firebase_admin\nfrom firebase_admin import credentials, firestore\n\ncredentials_path = '/content/XXXX-adminsdk-YYYYYY.json' \n\ncred = credentials.Certificate(credentials_path)\nfirebase_admin.initialize_app(cred)\ndb = firestore.client()\n\n\nProject Creation\nPlease provide an alert_email and project_name to create a new project on the backend. The backend checks hourly when the last stories have been uploaded to a project. If no story has been uploaded for more than 12 hours, an email alert will be triggered.\nRun the cell to create the new project on the backend. When successfull, the project id and api key will be displayed.\n\nfrom IPython.display import display, Markdown\nimport pandas as pd\n\nalert_email = 'michael@achmann.me'\nproject_name = 'Forschungsseminar23 Test'\n\n# Create Project\nimport uuid\n\n# Generate a UUID for the document\nproject_id = str(uuid.uuid4())\napi_key = str(uuid.uuid4())\n\n# Your data\ndata = {\n    \"api_key\": api_key,\n    \"email\": alert_email,\n    \"name\": project_name\n}\n\n# Add a new document with a UUID as the document name (ID)\ndoc_ref = db.collection('projects').document(project_id)\ndoc_ref.set(data)\n\ndisplay(Markdown(\"### Project Created:\"))\ndisplay(Markdown(f\"**Project Name:** {project_name}\"))\ndisplay(Markdown(f\"**Alert Email:** {alert_email}\"))\ndisplay(Markdown(f\"**Project ID:** {project_id}\"))\ndisplay(Markdown(f\"**API-Key:** {api_key}\"))\n\n\nProject Created:\nProject Name: Forschungsseminar23 Test\nAlert Email: michael@achmann.me\nProject ID: 959466fe-4088-4099-a6b2-3cbe058889d3\nAPI-Key: 554fbce8-fb15-44f1-bb4d-54cdc57554f2\n\n\n\nConfigure the Plugin\nConfigure Zeeschuimer-F using the above information after creating a project. In order to access the settings of Firefox plugins click on the puzzle tile on the top right of the browser. Click on Zeeschuimer F and the settings open.\n\n\n\nScreenshot of Firefox with open extensions menu\n\n\nFill in the Firebase Project field with the project id and aFirebase API Key with the api key provided after running the Project Creation. The Firebase Endopint URL will be provided via GRIPS (unless you’ve installed your own instance).\n\n\n\nScreenshot of the Settings for Zeeschuimer-F\n\n\n1) Turn the IG Stories Switch on, 2) restart your browser for the values to be loaded correctly. Once the browser has started again, you’re ready to collect you first stories! Open the Instagram website and open any story.\n\n\n\nScreenshot of the switch\n\n\nCheck the extension settings page to see whether it is collecting stories while browsing. The counter should increase with each story visit. The remote collection process can currently only be checked through the Firebase Interface notebook. Follow the next steps to download the collected data.\n\n\nProject Export\nThe following code downloads all stories in JSON format and saves it locally (i.e. on your colab instance). Provide the PROJECT_ID variable and an export_path to download all stories.\n\nfrom tqdm.auto import tqdm\nimport os\nimport json\n\nPROJECT_ID = '959466fe-4088-4099-a6b2-3cbe058889d3'\nexport_path = '/content/export' \n\n\ndef fetch_stories(project_id):\n    stories_ref = db.collection('projects').document(project_id).collection('stories')\n    docs = stories_ref.stream()\n\n    stories = []\n    for doc in docs:\n        stories.append(doc.to_dict())\n\n    return stories\n\ndb = fetch_stories(PROJECT_ID)\n\nif not os.path.exists('export'):\n    os.makedirs('export')\n\n# Iterate over each element in the database\nfor element in tqdm(db, desc='Exporting elements'):\n    # Serialize the element to JSON\n    element_json = json.dumps(element, indent=4)\n\n    # Write to a file named {id}.json\n    with open(os.path.join('export', f\"{element['id']}.json\"), 'w') as f:\n        f.write(element_json)\n\n\n\n\n\n\nConvert to DataFrame\nNext, we convert the exported JSON files to a pandas DataFrame and save the table as CSV. Provide the df_export_path variable for the location where to save the exported CSV file.\n\n\n\n\n\n\nWork-In-Progress\n\n\n\nThe DataFrame in the current version has a different structure than the one we created when downloading Instagram Posts.. In order to compare stories with posts we will might want to use the same data structure.\n\n\n\n\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n\ndf_export_path = '/content/2022-11-09-Stories-Exported.csv' \n\ndef process_instagram_story(data):\n\n    # Extract relevant information\n    story_info = {\n        'ID': data.get(\"id\"),\n        'Time of Posting': datetime.utcfromtimestamp(data['taken_at']).strftime('%Y-%m-%d %H:%M:%S'),\n        'Type of Content': 'Video' if 'video_duration' in data else 'Image',\n        'video_url': None,\n        'image_url': None,\n        'Username': data['user']['username'],\n        'Video Length (s)': data.get('video_duration', None),\n        'Expiration': (datetime.utcfromtimestamp(data['taken_at']) + timedelta(hours=24)).strftime('%Y-%m-%d %H:%M:%S'),\n        'Caption': data.get('caption', None),\n        'Is Verified': data['user']['is_verified'],\n        'Stickers': data.get('story_bloks_stickers', []),\n        'Accessibility Caption': data.get('accessibility_caption', ''),\n        'Attribution URL': data.get('attribution_content_url', '')\n    }\n\n    return story_info\n\nrows = []\nfor element in db:\n  rows.append(process_instagram_story(element))\n\ndf = pd.DataFrame(rows)\ndf.to_csv(df_export_path)\nprint(f\"Successfully exported {len(df)} rows as CSV.\")\n\nSuccessfully exported 22 rows as CSV.\n\n\nNow let’s take a look at the structure of the exported data:\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nID\nTime of Posting\nType of Content\nvideo_url\nimage_url\nUsername\nVideo Length (s)\nExpiration\nCaption\nIs Verified\nStickers\nAccessibility Caption\nAttribution URL\n\n\n\n\n0\n3231585718932790545_1483455177\n2023-11-08 14:50:59\nImage\n&lt;NA&gt;\nhttps://storage.googleapis.com/zeeschuimer-fb-...\nrmf24.pl\nNaN\n2023-11-09 14:50:59\nNone\nFalse\n[]\nPhoto by Fakty RMF FM | Rozmowy | Podcasty on ...\n\n\n\n1\n3231585778860997221_1483455177\n2023-11-08 14:51:06\nImage\n&lt;NA&gt;\nhttps://storage.googleapis.com/zeeschuimer-fb-...\nrmf24.pl\nNaN\n2023-11-09 14:51:06\nNone\nFalse\n[]\nPhoto by Fakty RMF FM | Rozmowy | Podcasty on ...\n\n\n\n2\n3231750838597692854_1349651722\n2023-11-08 20:19:00\nVideo\nhttps://storage.googleapis.com/zeeschuimer-fb-...\n&lt;NA&gt;\ntagesschau\n13.300\n2023-11-09 20:19:00\nNone\nTrue\n[]\n\n\n\n\n3\n3231750989408058657_1349651722\n2023-11-08 20:19:18\nVideo\nhttps://storage.googleapis.com/zeeschuimer-fb-...\n&lt;NA&gt;\ntagesschau\n15.267\n2023-11-09 20:19:18\nNone\nTrue\n[]\n\n\n\n\n4\n3231751135118088390_1349651722\n2023-11-08 20:19:35\nVideo\nhttps://storage.googleapis.com/zeeschuimer-fb-...\n&lt;NA&gt;\ntagesschau\n17.000\n2023-11-09 20:19:35\nNone\nTrue\n[]\n\n\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nDownload Images and Videos\nAll videos and images for our Instagram stories have been downloaded by our firebase backend. They are saved in a Cloud Bucket. The following cell helps with these two steps:\n\nCreate a signed link to each video and image\nDownload each file and saves it in the following structure: {media_export_path}/{image|video}/{username}/{ID.jpg|mp4}. It is important to keep a unique identifier (here ID) to map metadata and images for future data analysis.\n\nPlease provide a storage_bucket and media_export_path.\n\nstorage_bucket = \"XXXX.appspot.com\"  \nmedia_export_path =  '/content/media/'\n\nfrom firebase_admin import storage\nimport os\nimport requests\n\nbucket = storage.bucket(storage_bucket)\n\ndef generate_signed_url(username, content_id, file_type):\n    if file_type not in ['images', 'videos']:\n        raise ValueError(\"Invalid file type specified\")\n\n    ext = 'jpeg' if file_type == 'images' else 'mp4'\n    blob_path = f\"projects/{PROJECT_ID}/stories/{file_type}/{username}/{content_id}.{ext}\"\n    blob = bucket.blob(blob_path)\n    # Set the expiration of the link. Here, it's set to 24 hours.\n    return blob.generate_signed_url(expiration=timedelta(hours=24), method='GET')\n\n# Create a function to be applied across DataFrame rows\ndef apply_generate_signed_url(row):\n    image_url = generate_signed_url(row['Username'], row['ID'], 'images')\n    video_url = generate_signed_url(row['Username'], row['ID'], 'videos') if row['Type of Content'] == 'Video' else pd.NA\n    return pd.Series({'image_url': image_url, 'video_url': video_url})\n\n# Apply the function along the axis=1 (row-wise)\ndf[['image_url', 'video_url']] = df.apply(apply_generate_signed_url, axis=1)\n\n# Now, creating the lists for images and videos can be done more efficiently\ndata_images = df.loc[df['image_url'].notna(), ['ID', 'image_url', 'Username', 'Time of Posting']] \\\n               .rename(columns={'image_url': 'url', 'Time of Posting': 'datetime'}) \\\n               .to_dict('records')\n\ndata_videos = df.loc[df['video_url'].notna(), ['ID', 'video_url', 'Username', 'Time of Posting']] \\\n               .rename(columns={'video_url': 'url', 'Time of Posting': 'datetime'}) \\\n               .to_dict('records')\n\n\ndef create_directories(base_path, entries, subdir):\n    usernames = set(entry['Username'] for entry in entries)\n    for username in usernames:\n        os.makedirs(os.path.join(base_path, subdir, username), exist_ok=True)\n\ndef download_file(entry, media_type, media_export_path, session):\n    directory = os.path.join(media_export_path, media_type, entry['Username'])\n    ext = 'jpg' if media_type == 'images' else 'mp4'\n    filename = os.path.join(directory, f\"{entry['ID']}.{ext}\")\n\n    with session.get(entry['url'], stream=True) as response:\n        if response.status_code == 200:\n            with open(filename, 'wb') as file:\n                for chunk in response.iter_content(8192):\n                    file.write(chunk)\n        else:\n            print(f\"Failed to download {entry['url']}. Status code: {response.status_code}\")\n\nsession = requests.Session()\n# Pre-create directories\ncreate_directories(media_export_path, data_images, 'images')\ncreate_directories(media_export_path, data_videos, 'videos')\n\n# Download images\nfor entry in tqdm(data_images, desc=\"Downloading Images\", unit=\"file\"):\n    download_file(entry, 'images', media_export_path, session)\n\n# Download videos\nfor entry in tqdm(data_videos, desc=\"Downloading Videos\", unit=\"file\"):\n    download_file(entry, 'videos', media_export_path, session)\n\nprint(\"Download complete!\")\n\n\n\n\n\n\n\nDownload complete!\n\n\n\n\nPrepare Downloadable ZIP\nRun the following to ZIP all files. Optionally copy them to Google Drive.\n\n!zip -r 2023-11-09-Story-Media-Export.zip media/*\n\n\n!cp 2023-11-09-Story-Media-Export.zip /content/drive/MyDrive/\n\n\nSource: Firebase Interface Notebook\n\n\n\nPros:\n\n\nWe do not infringe the TOS\n\n\nCollects all data: metadata, images, videos\n\n\nThe firebase backend handles alert emails\n\n\n\n\nCons:\n\n\nCurrent solution relies on the firebase backend\n\n\nWe need to manually browse the stories twice a day (can be automated using Selenium)\n\n\nData is collected on firebase storage, we need to export to use it"
  },
  {
    "objectID": "data-collection/ig-stories.html#conclusion",
    "href": "data-collection/ig-stories.html#conclusion",
    "title": "Project Creation",
    "section": "Conclusion",
    "text": "Conclusion\nThis page offers an overview of two methods for collecting ephemeral Instagram stories, which are crucial to capture in real time due to their 24-hour expiration period. The first method, instaloader, is theoretically effective. However, similar to the case with posts, Instagram accounts utilizing Instaloader face a high risk of being banned swiftly.\nThe second approach adopts a less invasive strategy. It involves capturing the data transmitted to the browser while viewing stories on Instagram, and then transferring the metadata to our Firebase project. Upon the addition of a new story to the database, the backend initiates the download of videos and images for that story.\nTo facilitate this process, I have provided a notebook for project creation, a manual for configuring the plugin, and additional code to export the captured stories via a Jupyter notebook."
  },
  {
    "objectID": "data-collection/ig-posts.html",
    "href": "data-collection/ig-posts.html",
    "title": "Instagram Posts",
    "section": "",
    "text": "Instagram offers two ways of image sharing: permanent posts and ephemeral stories. In this chapter I will offer three approaches for collecting posts: Instaloader, CrowdTangle, and Zeeschuimer.\nPosts are shaped by several affordances and contain different type of media: least one image or video, often paired with text (captions). Posts may also contain an album consisting of more than one image or video. Captions may contain hashtags and / or mentions. Hashtags are used to self-organize posts on the platform, users can subscribe to hashtags and search for them. Mentions are used to link a post to another profile. Moreover, users can like, share and comment posts. Some data-collection approaches, like CrowdTangle, offer access to one image and post metrics, like the comment and like count. Instaloader, offer access to all images / videos, while being the legally most questionable approach. And then there’s the middle ground: Zeeschuimer (optionally in connection with 4CAT).\nThrough the following subchapters I will try to illuminate the advantages of each collection methods. For each method I will provide a manual to follow in order to collect metadata and the actual media for Instagram posts."
  },
  {
    "objectID": "data-collection/ig-posts.html#instaloader",
    "href": "data-collection/ig-posts.html#instaloader",
    "title": "Instagram Posts",
    "section": "Instaloader",
    "text": "Instaloader\nInstaloader is a python package for downloading instagram pictures and videos along with their metadata. I have written a getting started tutorial on Medium. It is, together with the provided notebook, the basis for this chapter.\n\n\n\n\n\n\nNote\n\n\n\nInstaloader is a stand-alone piece of software: It offers options to download most Instagram content, like posts and stories, through different strategies, e.g. lists of profiles or by hashtag. For complex tasks I recommend to call instaloader from terminal, see the documentation for more information.\n\n\n\nIn order to download posts and stories from Instagram, we use the package instaloader. You can install package for python using pip install &lt;package&gt;, the command -q minimizes the output.\n\n!pip -q install instaloader\n\n     |████████████████████████████████| 60 kB 3.0 MB/s eta 0:00:011\n  Building wheel for instaloader (setup.py) ... done\n\n\nOnce you install instaloader we log in using your username and password. Session information (not your credentials!) is stored in Google Drive to minimize the need for signing in.\nIn order to minimize the risk for your account to be disabled we suggest creating a new account on your phone before proceeding!\n\nusername = 'your.username'\n\n# We save the sessionfile to the following directory. Default is the new folder `.instaloader` in your google drive. (This is optional)\nsession_directory = '/content/drive/MyDrive/.instaloader/'\n\nimport instaloader\nfrom os.path import exists\nfrom pathlib import Path\n\n# Creating session directory, if it does not exists yet\nPath(session_directory).mkdir(parents=True, exist_ok=True)\n\nfilename = \"{}session-{}\".format(session_directory, username)\nsessionfile = Path(filename)\n\n\n# Get instance\nL = instaloader.Instaloader(compress_json=False)\n\n# Check if sessionfile exists. If so load session,\n# else login interactively\nif exists(sessionfile):\n  L.load_session_from_file(username, sessionfile)\n\nelse:\n  L.interactive_login(username)\n  L.save_session_to_file(sessionfile)\n\nLoaded session from /content/drive/MyDrive/.instaloader/session-mi_sm_lab05.\n\n\n\nDownloading first Posts\nNext, we try to download all posts of a profile. Provide a username and folder:\n\ndest_username = 'some.profile' \ndest_dir = '/content/drive/MyDrive/insta-posts/' # Once more we save the files to Google Drive. Replace this with a local directory if necessary.\n\nt = Path(\"{}{}\".format(dest_dir, dest_username))\nt.mkdir(parents=True, exist_ok=True)\n\nprofile = instaloader.Profile.from_username(L.context, dest_username)\nfor post in profile.get_posts():\n    L.download_post(post, target=t)\n\nWell, you just downloaded your first posts! Open Google Drive and check the folder insta-posts/ (or whatever folder you chose above)! There should be three files for each post, the image, a .json file and a .txt file. The .txt includes the image caption, the .json lots of metadata about the post.\n\nDiving into the metadata\nThe next cell reads all .json files of the downloaded posts. Then we browse through some interesting data.\n\n# Reading the paths of all JSON files from dest_dir\nimport os\n\njson_files = []\n\nfor subdir, dirs, files in os.walk(t):\n    for file in files:\n        fullpath = os.path.join(subdir, file)\n        filename, file_extension = os.path.splitext(fullpath)\n        if file_extension == \".json\":\n          json_files.append(fullpath)\n\n\n# Reading all JSON files\nfrom tqdm.notebook import tqdm\nimport json\n\njson_data = []\n\nfor file in tqdm(json_files):\n  with open(file, 'r') as f:\n    data = json.load(f)\n    json_data.append(data)\n\n\n\n\nOk, now all metadata for all posts is saved to the variable json_data. Run the next line and copy its output to http://jsonviewer.stack.hu/. Your output should look similar, go ahead and play around to explore your data! What information can you extract?\n\nprint(json.dumps(json_data[0]))\n\n\n\nMetadata Preprocessing\nPosts contain plenty of data, like time and location of the post, the authoring user, a caption, tagged users and more. The following cells demonstrate how to normalize the data into a table format, which is useful when working with pandas. Nevertheless, this is optional!\n\n# Use booleans (True / False) values to select what type of data you'd like to analyse. \nusername = True #@param {type:\"boolean\"}\ntimestamp = True #@param {type:\"boolean\"}\ncaption = True #@param {type:\"boolean\"}\nlocation = True #@param {type:\"boolean\"}\nshortcode = True #@param {type:\"boolean\"}\nid = True #@param {type:\"boolean\"}\ntagged_users = True #@param {type:\"boolean\"}\n\nNext we loop through the data and create a new pandas DataFrame. The DataFrame will have one column for each variable selected above and one row for each downloaded posts.\nIf you are not yet familiar with the concept of dataframes have a look at YouTube, there’s plenty of introductory videos available.\n\nimport pandas as pd\n\nposts = []  # Initializing an empty list for all posts\nfor post in tqdm(json_data):\n  row = {} # Initializing an empty row for the post\n\n  node = post.get(\"node\")\n\n  if username:\n    owner = node.get(\"owner\")\n    row['username'] = owner.get(\"username\")\n\n  if timestamp:\n    row['timestamp'] = node.get(\"taken_at_timestamp\")\n\n  if location:\n    l = node.get(\"location\", None)\n    if l:\n      row['location'] = l.get(\"name\")\n\n  if shortcode:\n    row['shortcode'] = node.get(\"shortcode\")\n\n  if id:\n    row['id'] = node.get(\"id\")\n  \n  if tagged_users:\n    pass\n\n  if caption:\n    c = \"\"\n    emtc = node.get(\"edge_media_to_caption\")\n    edges = emtc.get(\"edges\")\n    for element in edges:\n      caption_node = element.get(\"node\")\n      c = c + caption_node.get(\"text\")\n    row['caption'] = c\n\n  # Finally add row to posts\n  posts.append(row)\n\n# After looping through all posts create data frame from list\nposts_df = pd.DataFrame.from_dict(posts)\n\nNow all information selected above is saved to the dataframe posts_df. Run the next cell and it will return a nicely formatted table. If your data is quite long, output will be cropped. Click the wand and after a few seconds you are able to browse through the data or filter by columns\n\nposts_df\n\nIn order to get a first impression of dataframes, the head() method is also useful. Run the next cell to see the result\n\nposts_df.head()\n\nThe dataframe is only saved in memory, thus when disconnecting and deleting the runtime, the dataframe is lost. Running the next cell saves the table to a CSV-file on your drive.\nNow the processed data may be recovered or used in another notebook.\n\nposts_df.to_csv('{}{}.csv'.format(dest_dir, username))\n\n\n\nSource: Collecting Posts with Instaloader\n\n\nPros:\n\n\nMaximum Flexibility\n\n\nCan collect everything out of the box\n\n\nWe can collect content computationally\n\n\n\n\nCons:\n\n\nPossibly against the TOS\n\n\nRate Limits\n\n\nBlocked Accounts"
  },
  {
    "objectID": "data-collection/ig-posts.html#downloading-first-posts",
    "href": "data-collection/ig-posts.html#downloading-first-posts",
    "title": "Instagram Posts",
    "section": "Downloading first Posts",
    "text": "Downloading first Posts\nNext, we try to download all posts of a profile. Provide a username and folder:\n\ndest_username = 'some.profile' \ndest_dir = '/content/drive/MyDrive/insta-posts/' # Once more we save the files to Google Drive. Replace this with a local directory if necessary.\n\nt = Path(\"{}{}\".format(dest_dir, dest_username))\nt.mkdir(parents=True, exist_ok=True)\n\nprofile = instaloader.Profile.from_username(L.context, dest_username)\nfor post in profile.get_posts():\n    L.download_post(post, target=t)\n\nWell, you just downloaded your first posts! Open Google Drive and check the folder insta-posts/ (or whatever folder you chose above)! There should be three files for each post, the image, a .json file and a .txt file. The .txt includes the image caption, the .json lots of metadata about the post.\n\nDiving into the metadata\nThe next cell reads all .json files of the downloaded posts. Then we browse through some interesting data.\n\n# Reading the paths of all JSON files from dest_dir\nimport os\n\njson_files = []\n\nfor subdir, dirs, files in os.walk(t):\n    for file in files:\n        fullpath = os.path.join(subdir, file)\n        filename, file_extension = os.path.splitext(fullpath)\n        if file_extension == \".json\":\n          json_files.append(fullpath)\n\n\n# Reading all JSON files\nfrom tqdm.notebook import tqdm\nimport json\n\njson_data = []\n\nfor file in tqdm(json_files):\n  with open(file, 'r') as f:\n    data = json.load(f)\n    json_data.append(data)\n\n\n\n\nOk, now all metadata for all posts is saved to the variable json_data. Run the next line and copy its output to http://jsonviewer.stack.hu/. Your output should look similar, go ahead and play around to explore your data! What information can you extract?\n\nprint(json.dumps(json_data[0]))\n\n\n\nMetadata Preprocessing\nPosts contain plenty of data, like time and location of the post, the authoring user, a caption, tagged users and more. The following cells demonstrate how to normalize the data into a table format, which is useful when working with pandas. Nevertheless, this is optional!\n\n# Use booleans (True / False) values to select what type of data you'd like to analyse. \nusername = True #@param {type:\"boolean\"}\ntimestamp = True #@param {type:\"boolean\"}\ncaption = True #@param {type:\"boolean\"}\nlocation = True #@param {type:\"boolean\"}\nshortcode = True #@param {type:\"boolean\"}\nid = True #@param {type:\"boolean\"}\ntagged_users = True #@param {type:\"boolean\"}\n\nNext we loop through the data and create a new pandas DataFrame. The DataFrame will have one column for each variable selected above and one row for each downloaded posts.\nIf you are not yet familiar with the concept of dataframes have a look at YouTube, there’s plenty of introductory videos available.\n\nimport pandas as pd\n\nposts = []  # Initializing an empty list for all posts\nfor post in tqdm(json_data):\n  row = {} # Initializing an empty row for the post\n\n  node = post.get(\"node\")\n\n  if username:\n    owner = node.get(\"owner\")\n    row['username'] = owner.get(\"username\")\n\n  if timestamp:\n    row['timestamp'] = node.get(\"taken_at_timestamp\")\n\n  if location:\n    l = node.get(\"location\", None)\n    if l:\n      row['location'] = l.get(\"name\")\n\n  if shortcode:\n    row['shortcode'] = node.get(\"shortcode\")\n\n  if id:\n    row['id'] = node.get(\"id\")\n  \n  if tagged_users:\n    pass\n\n  if caption:\n    c = \"\"\n    emtc = node.get(\"edge_media_to_caption\")\n    edges = emtc.get(\"edges\")\n    for element in edges:\n      caption_node = element.get(\"node\")\n      c = c + caption_node.get(\"text\")\n    row['caption'] = c\n\n  # Finally add row to posts\n  posts.append(row)\n\n# After looping through all posts create data frame from list\nposts_df = pd.DataFrame.from_dict(posts)\n\nNow all information selected above is saved to the dataframe posts_df. Run the next cell and it will return a nicely formatted table. If your data is quite long, output will be cropped. Click the wand and after a few seconds you are able to browse through the data or filter by columns\n\nposts_df\n\nIn order to get a first impression of dataframes, the head() method is also useful. Run the next cell to see the result\n\nposts_df.head()\n\nThe dataframe is only saved in memory, thus when disconnecting and deleting the runtime, the dataframe is lost. Running the next cell saves the table to a CSV-file on your drive.\nNow the processed data may be recovered or used in another notebook.\n\nposts_df.to_csv('{}{}.csv'.format(dest_dir, username))"
  },
  {
    "objectID": "data-collection/ig-posts.html#crowdtangle",
    "href": "data-collection/ig-posts.html#crowdtangle",
    "title": "Instagram Posts",
    "section": "CrowdTangle",
    "text": "CrowdTangle\n\n\n\nScreenshot of the CrowdTangle interface.\n\n\nCrowdTangle is the best option to collect IG posts – in theory. It provides legal access to Instagram data and offers several tools to export large amount of data. For a current project we’ve exported more than 500.000 public posts through a hashtag query. Unfortunately there are several restrictions: CrowdTangle is the best tool to export metadata of public posts, and captions. The abilty to collect images through the platform is limited: Image links expire after a certain amount of time, thus we need to use some makeshift approach to download the images. When we can download the images, it’s always just one per post, no matter if it’s a gallery or a single image. And let’s not talk about videos. I have written another Medium story with a step-by-step guide to CrowdTangle.\n\n\nPros:\n\n\nLegal Access\n\n\nWe can select the time frame for export\n\n\nExport in CSV format\n\n\n\n\nCons:\n\n\nOnly access to one image for album posts\n\n\nLimited access to historical images, the browsing to the bottom strategy is limited\n\n\nNo videos for newer posts"
  },
  {
    "objectID": "data-collection/ig-posts.html#zeeschuimer-4cat",
    "href": "data-collection/ig-posts.html#zeeschuimer-4cat",
    "title": "Instagram Posts",
    "section": "Zeeschuimer & 4CAT",
    "text": "Zeeschuimer & 4CAT\n\n\n\nScreenshot of Zeeschuimer\n\n\nZeeschuimer (Peeters, n.d.) and 4CAT (Peeters, Hagen, and Wahl, n.d.) are two tools developed for the https://wiki.digitalmethods.net/. The first is a firefox plugin that captures traffic when browsing websites likes Instagram or TikTok. The second, 4CAT, is an analysis platform incorporating several steps of preprocessing and further analyses. For post collection we can use the original Zeeschuimer Firefox Plugin, download the latest release from GitHub and install it in Firefox. To download Instagram posts using Zeeschuimer follow these steps (* steps are only necessary when working with 4CAT):\n\nDownload and install Firefox\nDownload and install the Plugin\n*Register a 4CAT Account\nActivate the Instagram (Posts) Switch.\n*Fill out the 4CAT server URL field (https://4cat.digitalhumanities.io/).\nOpen Instagram in a new tab. Browse the profiles you’re interested in. Keep scrolling to the bottom of the profile until you reach posts at the end of your period of investigation.\nDownload the data from the plugin or export the data to 4CAT.\n\n\n\nPros:\n\n\nWe do not infringe the TOS\n\n\nCan collect data from private profiles\n\n\nWe can collect all media, also albums and videos\n\n\n\n\nCons:\n\n\nWe need to browse through the profiles\n\n\nPractical limitations (e.g. volume, timeframe, # of profiles …)\n\n\n\n\n\nWorking with 4CAT\n\n\n\nScreenshot of 4CAT\n\n\n4CAT is a tool developed by the Digital Methods Initiative. The collected data can be exported to 4CAT with only the click of a button. After successfully importing the post data, the tools offers several modules. At first, download the images associated with each post with the Download images module at the bottom. Select image_url in the options tab and hit Run.\n\n\n\nAvailable modules for visual analysis using 4CAT\n\n\nOnce the images have been downloaded more analysis options are available when clicking the More button on the right. Further, you may download images as a ZIP file and can export the posts from 4CAT in CSV format. Repeat the process with the Download Video function to access posted videos. We will be able to use the collected data using the CSV export and the media files provided in the ZIP packages. Additionally, each ZIP file contains a .metadata.json file which we may use to map filenames to media files.\nThe authors of Zeeschuimer and 4CAT have published a manual here.\n\n\nWorking with Python\nData collected using Zeeschuimer can also be exported as ndjson files. The Zeeschuimer Import notebook provides a code example for reading the files and converting them to either 4CAT format, or a table format compatible with the above notebooks for CrowdTangle and instaloader.\n\n\n\n\n\n\nWork-In-Progress\n\n\n\nWe could download multiple images / videos for albums with little refactoring. We will work on an update if necessary."
  },
  {
    "objectID": "data-collection/ig-posts.html#references",
    "href": "data-collection/ig-posts.html#references",
    "title": "Instagram Posts",
    "section": "References",
    "text": "References\n\n\nPeeters, Stijn. n.d. “Zeeschuimer.” https://doi.org/10.5281/zenodo.8399900.\n\n\nPeeters, Stijn, Sal Hagen, and Dale Wahl. n.d. “4CAT Capture and Analysis Toolkit.” https://doi.org/10.5281/zenodo.8139174."
  },
  {
    "objectID": "processing/exploration.html",
    "href": "processing/exploration.html",
    "title": "Text Exploration",
    "section": "",
    "text": "Work-In-Progress\n\n\n\nWill be available for the 12/04 session.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Starting in 2022, several student projects and bachelor and master theses have been exploring Social Media Analysis under my supervision. Each project approached the field with a unique research interest and questions, influenced by interdisciplinary questions and perspectives. On this page I keep track of past and current projects."
  },
  {
    "objectID": "projects/index.html#semester-projects",
    "href": "projects/index.html#semester-projects",
    "title": "Projects",
    "section": "Semester Projects",
    "text": "Semester Projects\n\nDigital Humanities – Winter 2022/2023\n\n“Trends in Visual Features of NFT Art from the Most Economically Successful Artists on OpenSea” (Ferah Noor, Mari McCarville)\n“Body Positivity auf Instagram – Eine qualitative Inhaltsanalyse” (Anna Ignjatovic, Adela Myslikova, Ronny Retschmeier)\n“Aktivismus oder Klimaterror? Die Kommentierung der Klimabewegung auf Twitter in der Analyse” (Marie Ederer, Sebastian Daniel)\n“Die Relevanz des Ukraine-Kriegs im nationalen Kontext BILD, SZ und Tagesschau im Vergleich” (Milena Bach, Tobias Ederer, Sebastian Mißler)\n“Die Verwendung naturverbundener Farben anhand ausgewählter Food-Influencerinnen” (Anna Zagel, Mona Meier-to-Krax, Chiara Rahe)\n“Instagram Beiträge zum Thema Ukraine von Nachrichtenkanälen aus verschiedenen Ländern (vor und während des Krieges im Vergleich)” (Philipp Pielmeier, Katharina Kampa, Johanna Grünler)\n“Die Vermarktung von ESN-Fitnessprodukten in den Stories auf der Plattform Instagram” (Kessler Julia, Nett Ellena, Ousseni Océane, Traßl-Wilterius Annika, Umbreit Janosch)\n“Politisches Posten im Rahmen des Krieges in der Ukraine” (Jakob Berg)\n\n\n\nComputational Analysis of Visual Social Media – Winter 2023/2024\nWork in Progress: We will form groups with a final set of topics on October, 30th. We organize our groups and topics on mural."
  },
  {
    "objectID": "projects/index.html#theses",
    "href": "projects/index.html#theses",
    "title": "Projects",
    "section": "Theses",
    "text": "Theses\n\nBachelor\n\n\n\n\n\n\n\n\n\nTitle\nAuthor\nStatus\nYear\n\n\n\n\nSocial Media Analyse von Instagram Stories am Beispiel politischer Akteure während der Bundestagswahl 2021\nLisa Hampel\nCompleted\n2021\n\n\nSammlung und Auswertung eines Social-Media-Korpus durch Entwicklung eines Browser-Plugins zur Annotation von Instagram Stories\nRuslan Asabidi\nCompleted\n2021\n\n\nClassification of Multimodal Social Media Crisis Data – Evaluation and Comparison of two Multimodal Machine Learning Models\nMarkus Weinberger\nCompleted\n2023\n\n\nReaktionen politischer Akteure auf den russischen Angriffskrieg in Instagram Stories & Videos\nFranka Heinlein\nCompleted\n2023\n\n\nPolitical stories – improving face recognition performance for political Instagram story analysis\nPhilip Pirkl\nCompleted\n2023\n\n\nData Donations and Ephemeral Content: Obtaining Instagram-Stories\nTobias Lanzl\nWork in Progress\n2023\n\n\nXu Hướng: An Analysis of Trending TikTok Videos in Vietnam\nThuy-Linh Nguyen\nWork in Progress\n2023\n\n\nEntwicklung eines interaktiven Dashboards zur Echtzeitauswertung der politischen Kommunikation auf Instagram im Landtagswahlkampf 2023\nJonas Ernst\nWork in Progress\n2023\n\n\n\n\n\nMaster\n\n\n\nTitle\nAuthor\nStatus\nYear\n\n\n\n\nUntersuchung der Telegram-Kanäle der “Querdenker”-Bewegung\nTheresa Strohmeier\nCompleted\n2022\n\n\nInvestigating African American Writing and Thought by comparison of two corpora via Distant Reading\nAenne Knierim\nCompleted\n2022\n\n\nSelbstoptimierung vs. Selbstliebe? Eine vergleichende Inhaltsanalyse von Fitspiration- und Bodypositivity-Bildern auf Instagram mit Methoden der automatischen Bildklassifikation\nJulia Glas\nCompleted\n2022\n\n\nAnalyse visueller und textueller Kommunikationsaspekte von deutschen Lifestyle-Influencern auf Instagram und deren Einfluss auf das User Engagement\nNina Dillinger\nCompleted\n2023"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Michael Achmann  Universitätsstr. 31  93053 Regensburg  Email: michael.achmann@informatik.uni-regensburg.de  Phone: 0941 / 943 5098"
  },
  {
    "objectID": "about.html#website-owner",
    "href": "about.html#website-owner",
    "title": "About",
    "section": "",
    "text": "Michael Achmann  Universitätsstr. 31  93053 Regensburg  Email: michael.achmann@informatik.uni-regensburg.de  Phone: 0941 / 943 5098"
  },
  {
    "objectID": "about.html#copyright-information",
    "href": "about.html#copyright-information",
    "title": "About",
    "section": "Copyright Information",
    "text": "Copyright Information\nCopyright © Michael Achmann 2023. The text-content and supplement materials are licensed unter GNU GPL 3.0.\n\nCitation\nCiting information for the website will follow soon."
  },
  {
    "objectID": "about.html#disclaimer",
    "href": "about.html#disclaimer",
    "title": "About",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe information contained on this website is for general informational purposes only. While we make every effort to keep the information up to date and accurate, we make no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability, or availability concerning the website or the information, products, services, or related graphics contained on the website for any purpose. Any reliance you place on such information is strictly at your own risk."
  },
  {
    "objectID": "about.html#links-to-third-party-websites",
    "href": "about.html#links-to-third-party-websites",
    "title": "About",
    "section": "Links to Third-Party Websites",
    "text": "Links to Third-Party Websites\nThis website may contain links to third-party websites. These links are provided solely for your convenience and do not imply any endorsement, sponsorship, or recommendation by us. We have no control over the content of these websites and assume no responsibility for their accuracy, legality, or content."
  },
  {
    "objectID": "about.html#data-protection",
    "href": "about.html#data-protection",
    "title": "About",
    "section": "Data Protection",
    "text": "Data Protection\nThis Quarto website is hosted on GitHub. While this website does not use any analytics software, GitHub may store cookies on your device. You can change the cookie preferences at any time."
  },
  {
    "objectID": "about.html#trademark-notice",
    "href": "about.html#trademark-notice",
    "title": "About",
    "section": "Trademark Notice",
    "text": "Trademark Notice\nAll trademarks and logos used on this website are the property of their respective owners."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\nIf you have any questions or concerns regarding this impressum or our website, please contact us using the contact information provided above."
  },
  {
    "objectID": "getting-started/tools.html",
    "href": "getting-started/tools.html",
    "title": "Tools and Software",
    "section": "",
    "text": "This page provides an overview of useful tools – not just for the visual social media analysis. In our tools session (October 30th) we will take a look at Colab and git / GitHub. As the semester progresses we will learn more tools, which gradually will be added to this page. A first list of software for the future sessions is linked at the bottom of the page."
  },
  {
    "objectID": "getting-started/tools.html#colab",
    "href": "getting-started/tools.html#colab",
    "title": "Tools and Software",
    "section": "Colab",
    "text": "Colab\nColab is a platform created by Google for collaborative work and research. It offers a preconfigured Python development environment with access to popular libraries and tools. It is based on Jupyter notebooks, which allows users to create and share documents that contain live code, equations, visualizations, and narrative text. Throughout the semester I am going to provide code for different applications as Jupyter notebooks, which can easily be accessed and run on Colab.\n\n\n\nA screenshot of Colab\n\n\nColab can be used for free, but it also offers a paid subscription plan called Colab Pro. The pro version offers, among other features, access to GPUs, which are often used for machine learning. We are probably going to use APIs and GPT throughout the semester, if we need to access GPUs we may use schlaubox."
  },
  {
    "objectID": "getting-started/tools.html#obsidian",
    "href": "getting-started/tools.html#obsidian",
    "title": "Tools and Software",
    "section": "Obsidian",
    "text": "Obsidian\nObsidian and Notion are excellent tools for note-taking. My personal recommendation is Obsidian, as it is free for personal use and notes are saved in markdown format on your harddrive. Thus, the software does not require any subscriptions. Use Dropbox, iCloud or Nextcloud to backup your files! Obsidian is a note-taking app based on the concept of interconnected notes.\n\n\n\nA screenshot of Obsidian\n\n\nThe app allows to easily link between notes, it is a flexible and powerful tool with a wide range of plugins available. Thanks to this large amount of plugins, it is also possible to extend its use. I recommend the dataloom plugin to organize excel-like lists, the textgenerator plugin to use GPT within Obsidian, and the Kanban plugin to organize your tasks. Additionally, use Day Planner to create daily todo lists, the citations plugin to organize your literature notes, and the admonition plugin to add visually outstanding text blocks. Use obsidian-git to collaborate using GitHub."
  },
  {
    "objectID": "getting-started/tools.html#open-research-quarto-for-project-documentation",
    "href": "getting-started/tools.html#open-research-quarto-for-project-documentation",
    "title": "Tools and Software",
    "section": "Open Research: Quarto for Project Documentation",
    "text": "Open Research: Quarto for Project Documentation\nQuarto is an innovative open-source scientific and technical publishing system. We can draft our research and projects using Jupyter notebooks or with plain text markdown in our chosen editors. What’s more, we’re able to craft dynamic content using Python, and other languages. When it comes to publishing our findings, we can produce reproducible, top-quality articles, presentations, websites, blogs, and books in various formats, including HTML, PDF, MS Word, and ePub. Writing is made easy with Pandoc markdown, letting us include equations, citations, cross-references, figure panels, callouts, and advanced layouts. The source for this website is available on GitHub. When working on your projects you will be able to share milestones using Quarto, telling a story with your data.\n\n\n\nA screenshot of this website opened in Visual Studio Code with quarto running in the terminal\n\n\nYou may clone the repository with git clone git@github.com:michaelachmann/social-media-lab-quarto.git. Add a folder for your projects in the projects folder and create a cover page called index.qmd. See the README for more information on how to commit and push your changes for publication. Everything will be reviewed before publication!"
  },
  {
    "objectID": "getting-started/tools.html#git-github",
    "href": "getting-started/tools.html#git-github",
    "title": "Tools and Software",
    "section": "Git & GitHub",
    "text": "Git & GitHub\nGit is a distributed version control system that enables us to track changes in our codebase, allowing multiple team members to work simultaneously without overwriting each other’s contributions. By creating and switching between different branches, we can experiment with new features or bug fixes without disturbing the main code. When we’re ready, merging these changes back into the main branch is straightforward. Moreover, Git’s history tracking feature ensures that we can always trace back our steps, understand the evolution of our code, and even revert to previous versions if necessary.\n\n\n\nA screenshot of commiting and pushing changes to the repository of this website.\n\n\n\nGitHub, Inc. (/ˈɡɪthʌb/[a]) is a platform and cloud-based service for software development and version control using Git, allowing developers to store and manage their code. It provides the distributed version control of Git plus access control, bug tracking, software feature requests, task management, continuous integration, and wikis for every project.[6] Headquartered in California, it has been a subsidiary of Microsoft since 2018. Wikipedia\n\nUsing GitHub, we can manage our projects, collaborate on coding tasks, and track changes seamlessly using Git. The platform can also be used to host research data and can connected to OSF, to provide code and data anonymously to reviewers. Using Zenodo we can create DOIs and provide citable software packages."
  },
  {
    "objectID": "getting-started/tools.html#future-sessions-outlook",
    "href": "getting-started/tools.html#future-sessions-outlook",
    "title": "Tools and Software",
    "section": "Future Sessions – Outlook",
    "text": "Future Sessions – Outlook\nVisual Exploration\n\nImageJ\nPixPlot\n\nImage Feature Extraction using APIs\n\nMemespector\n\nData Visualization\n\nFigma\nRawgraphs"
  },
  {
    "objectID": "getting-started/index.html",
    "href": "getting-started/index.html",
    "title": "About the Seminar",
    "section": "",
    "text": "The research seminar Computational Analysis of Visual Social Media consists of project-centred work in groups, lectures on theory and practical sessions. Each group will follow their own research interests and datasets. Groups will be formed in the third session, together with preliminary topics. We have participants from different fields, the topics will mirror this interdisciplinarity, roughly drawn from the interesctions of media studies, political science, and communication science. The seminar aims at master students with first knowledge of at least one programming language.\n\n\nBy the end of the semester you should know more about:\n\nThe state of Social Media Research,\ninteresting questions to answer with social media data,\nethical and legal restrictions,\ndevelop operationalizations for visual and textual data,\n\n\n\n\nBy the end of the semester you will be able to:\n\nCollect Instagram stories, posts and TikTok videos,\napply OCR and automatically transcribe videos,\ncomputationally classify text and images using GPT and CLIP,\nevaluate and optimize your classifications using human annotations,\npresent your results.\n\n\n\n\nThe following expectations and criteria must be met to pass the course:\n\nIndependent familiarization with your own scientific topic: Literature research, formulation of research questions, and operationalization.\nWillingness to master new tools (supported by practical units and some provided Jupyter Notebooks).\nActive and regular team collaboration on the individual project.\nContinuous documentation of current progress through a project wiki.\nWriting a project report at the end of the semester.\n\n\n\n\n\nWe will openly document our project progress, incorporating a strong Open Science stance.\nThe project report template is a good starting point for your report and continuous documentation.\nThrough the semester we will come back to the draft and extend it towards the final report.\nThe goal is to publish the report on social-media-lab.net.\n\n\n\n\n\nThe project report will be handed in collaboratively,\nconsists of app. 20 pages,\nfollows the IMRAD1 principle,\nuses APA citation style,\nneeds to be handed in no later than 31.03.2024.\n\n\n\n\n\nSuggest your own project\nPossible Projects:\n\nLandtagswahl BY 2023\n\nIG Stories & Posts\nTikTok\nJugendorganisationen\n\nPolitische Influencer auf TikTok\nWar in Sozialen Medien:\n\nUkraine Invasion\nHamas Angriff auf Israel\n\nFalschinformationen & KI-Generierte Inhalte\n\n\n\n\n\n\nMost course material will be available on social-media-lab.net.\nAdditional material will be provided via GRIPS.\nWe will work collaboratively on the website through the semester.\nThe content is edited using Quarto and Markdown, you will need a GitHub Account.\nPlease provide your GitHub username to get access to the repository.\nAll content will be published under GPL-3."
  },
  {
    "objectID": "getting-started/index.html#what-to-expect-theoretical-skills",
    "href": "getting-started/index.html#what-to-expect-theoretical-skills",
    "title": "About the Seminar",
    "section": "",
    "text": "By the end of the semester you should know more about:\n\nThe state of Social Media Research,\ninteresting questions to answer with social media data,\nethical and legal restrictions,\ndevelop operationalizations for visual and textual data,"
  },
  {
    "objectID": "getting-started/index.html#what-to-expect-practical-skills",
    "href": "getting-started/index.html#what-to-expect-practical-skills",
    "title": "About the Seminar",
    "section": "",
    "text": "By the end of the semester you will be able to:\n\nCollect Instagram stories, posts and TikTok videos,\napply OCR and automatically transcribe videos,\ncomputationally classify text and images using GPT and CLIP,\nevaluate and optimize your classifications using human annotations,\npresent your results."
  },
  {
    "objectID": "getting-started/index.html#class-requirements",
    "href": "getting-started/index.html#class-requirements",
    "title": "About the Seminar",
    "section": "",
    "text": "The following expectations and criteria must be met to pass the course:\n\nIndependent familiarization with your own scientific topic: Literature research, formulation of research questions, and operationalization.\nWillingness to master new tools (supported by practical units and some provided Jupyter Notebooks).\nActive and regular team collaboration on the individual project.\nContinuous documentation of current progress through a project wiki.\nWriting a project report at the end of the semester."
  },
  {
    "objectID": "getting-started/index.html#project-documentation",
    "href": "getting-started/index.html#project-documentation",
    "title": "About the Seminar",
    "section": "",
    "text": "We will openly document our project progress, incorporating a strong Open Science stance.\nThe project report template is a good starting point for your report and continuous documentation.\nThrough the semester we will come back to the draft and extend it towards the final report.\nThe goal is to publish the report on social-media-lab.net."
  },
  {
    "objectID": "getting-started/index.html#project-report",
    "href": "getting-started/index.html#project-report",
    "title": "About the Seminar",
    "section": "",
    "text": "The project report will be handed in collaboratively,\nconsists of app. 20 pages,\nfollows the IMRAD1 principle,\nuses APA citation style,\nneeds to be handed in no later than 31.03.2024."
  },
  {
    "objectID": "getting-started/index.html#project-ideas",
    "href": "getting-started/index.html#project-ideas",
    "title": "About the Seminar",
    "section": "",
    "text": "Suggest your own project\nPossible Projects:\n\nLandtagswahl BY 2023\n\nIG Stories & Posts\nTikTok\nJugendorganisationen\n\nPolitische Influencer auf TikTok\nWar in Sozialen Medien:\n\nUkraine Invasion\nHamas Angriff auf Israel\n\nFalschinformationen & KI-Generierte Inhalte"
  },
  {
    "objectID": "getting-started/index.html#social-media-lab",
    "href": "getting-started/index.html#social-media-lab",
    "title": "About the Seminar",
    "section": "",
    "text": "Most course material will be available on social-media-lab.net.\nAdditional material will be provided via GRIPS.\nWe will work collaboratively on the website through the semester.\nThe content is edited using Quarto and Markdown, you will need a GitHub Account.\nPlease provide your GitHub username to get access to the repository.\nAll content will be published under GPL-3."
  },
  {
    "objectID": "getting-started/index.html#introduction-to-social-media-analysis",
    "href": "getting-started/index.html#introduction-to-social-media-analysis",
    "title": "About the Seminar",
    "section": "Introduction to Social Media Analysis",
    "text": "Introduction to Social Media Analysis\n\nOverview of social media studies\n\nWhich academic disciplines are interested in plattforms like Instagram?\nWhat is their interest, how do they study the user generated content?\nSpecial focus: Political Communication on Instagram\n\nHow to conduct your own literature review\nTheory: Digital Methods & Cultural Analytics\nA short word about ethics & laws"
  },
  {
    "objectID": "getting-started/index.html#getting-started-tools",
    "href": "getting-started/index.html#getting-started-tools",
    "title": "About the Seminar",
    "section": "Getting Started: Tools",
    "text": "Getting Started: Tools\n\nInstallation & Configuration of different tools.\n\nGoogle Colab / Jupyter Notebooks\nQuarto & Markdown for project documentation\nGit & GitHub\nFirefox Plugins\nFigma\nand more"
  },
  {
    "objectID": "getting-started/index.html#data-collection-ig-posts-stories",
    "href": "getting-started/index.html#data-collection-ig-posts-stories",
    "title": "About the Seminar",
    "section": "Data Collection: IG Posts & Stories",
    "text": "Data Collection: IG Posts & Stories\n\nPost types and platform affordances of Instagram\nHow to use Instaloader\nHow to use CrowdTangle2\nCollecting Stories using Zeeschuimer-F and the firebase backend.\nCollecting Posts using Zeeschuimer and 4CAT\n\n\n\n\nScreenshot of the CrowdTangle interface."
  },
  {
    "objectID": "getting-started/index.html#data-collection-tiktok",
    "href": "getting-started/index.html#data-collection-tiktok",
    "title": "About the Seminar",
    "section": "Data Collection: TikTok",
    "text": "Data Collection: TikTok\n\nPost types and platform affordances of TikTok\nCollecting TikToks using Zeeschuimer and 4CAT\n\n\n\n\nScreenshot of the Firebase Backend and Zeeschuimer-F."
  },
  {
    "objectID": "getting-started/index.html#data-preprocessing",
    "href": "getting-started/index.html#data-preprocessing",
    "title": "About the Seminar",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nOCR\nWe are going to use easyocr to detect and recognize text embedded in images, such as posts and stories.\nWe will export the first frame of videos for OCR and further analyses.\nAutomated Transcription\n\nWe will extract any audio from collected videos.\nWe will use whisper to transcribe the audio content of videos"
  },
  {
    "objectID": "getting-started/index.html#textual-exploration",
    "href": "getting-started/index.html#textual-exploration",
    "title": "About the Seminar",
    "section": "Textual Exploration",
    "text": "Textual Exploration\n\nWe will first take a look at the textual data using simple frequency analyses and wordclouds.\nWe will use the GPT-API to explore the textual content of our data.\nOptional we might use BERTopic to explore our textual data."
  },
  {
    "objectID": "getting-started/index.html#operationalization-i",
    "href": "getting-started/index.html#operationalization-i",
    "title": "About the Seminar",
    "section": "Operationalization I",
    "text": "Operationalization I\n\nThis session depends on your own research: By december you should have developed an initial research request and explored related work in order to develop the first operationalization for content analysis.\nWe will learn more about content analysis in this session.\nBased on your research, and the explorations of the previous sesssion, we will develop the first annotation guide.\nThrough the session we will explore how to (efficiently) use GPT for text data annotation."
  },
  {
    "objectID": "getting-started/index.html#data-annotation",
    "href": "getting-started/index.html#data-annotation",
    "title": "About the Seminar",
    "section": "Data Annotation",
    "text": "Data Annotation\n\nIn this session we will import our data into LabelStudio and develop a final annotation manual.\nUsing the manuals and LabelStudio projects we will annotate the data.\nWe will shuffle annotators: Everyone will annotate for another group."
  },
  {
    "objectID": "getting-started/index.html#evaluation-i",
    "href": "getting-started/index.html#evaluation-i",
    "title": "About the Seminar",
    "section": "Evaluation I",
    "text": "Evaluation I\n\nUsing the human annotations we will evaluate the performance of our computational text annotations / information extractions.\nWe can fine-tune our prompts using the annotation data to improve the annotation quality.\nWe will learn how to present and visualize the quality of the model."
  },
  {
    "objectID": "getting-started/index.html#exploration-of-visual-data",
    "href": "getting-started/index.html#exploration-of-visual-data",
    "title": "About the Seminar",
    "section": "Exploration of Visual Data",
    "text": "Exploration of Visual Data\n\nWe will explore different tools to visualize images:\nImageJ\nPixPlot\nMemespector and Gephi3\nThe visualization forms the basis for image classification: In this stage we want to find similarities and differences.\n\n\n\n\nExample of image exploration using PixPlot."
  },
  {
    "objectID": "getting-started/index.html#operationalization-ii",
    "href": "getting-started/index.html#operationalization-ii",
    "title": "About the Seminar",
    "section": "Operationalization II",
    "text": "Operationalization II\n\nOnce more a dive in the literature: This time on visual content analysis.\nCombining the results of our exploration, reserach interest and related work with content analysis, we will develop an annotation manual for the images.\nWe will learn how to use CLIP for image classification\nBased on your previous experience you will create human annotations.\nWe will shuffle annotators: Everyone will annotate for another group.\n\n\n\n\nDecomposition of different layers in a Story by @gruenebayern during the 2023 Bavarian state elections."
  },
  {
    "objectID": "getting-started/index.html#evaluation-ii",
    "href": "getting-started/index.html#evaluation-ii",
    "title": "About the Seminar",
    "section": "Evaluation II",
    "text": "Evaluation II\n\nOnce more we will evaluate the quality of our model,\nand fine-tune our prompts.\nWork in Progress: We might organize this session differently on short notice, depending on the outcomes of my current research project.\nWaiting in Progress: In case of visual GPT being published we might have to adapt.\n\n\n\n\nExample of a visual inspection of classification results: Intermediary results of image types classifications using CLIP for the 2021 federal election. Two out of five stories posted by differnt parties have been misclassified."
  },
  {
    "objectID": "getting-started/index.html#data-wrangling-as-a-conversation",
    "href": "getting-started/index.html#data-wrangling-as-a-conversation",
    "title": "About the Seminar",
    "section": "Data Wrangling as a Conversation",
    "text": "Data Wrangling as a Conversation\n\nThe Advanced Data Analysis mode of ChatGPT is a powerful tool to (quickly) analyze metadata (and more) of social media data.\nWe will give it a shot with some simple analyses, like trends over time.\nExperimental in case we have enough time left, we might try to create a workflow with LangChain and LlamaIndex to chat with our data."
  },
  {
    "objectID": "getting-started/index.html#visual-presentation-of-your-data",
    "href": "getting-started/index.html#visual-presentation-of-your-data",
    "title": "About the Seminar",
    "section": "Visual Presentation of your Data",
    "text": "Visual Presentation of your Data\n\nOur last session of the semester will be all about telling a story with your data.\nWe will use Python (Jupyter Notebooks) to transform our data in CSV files.\nWe will import the data into RAWGraphs to create convincing plots\nWe will use Figma to collaboratively sketch the layout of your project report website."
  },
  {
    "objectID": "getting-started/index.html#footnotes",
    "href": "getting-started/index.html#footnotes",
    "title": "About the Seminar",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIntroduction, Method, Result, Analaysis, Discussion↩︎\nI will not be able to provide access to the tool. We can, however, export data for our projects from the platform and you will learn how to use the exported data.↩︎\nfollowing Omena’s concept for cloud vision labels, see related work.↩︎"
  },
  {
    "objectID": "notebooks/ig-instaloader.html",
    "href": "notebooks/ig-instaloader.html",
    "title": "Instagram Posts",
    "section": "",
    "text": "In order to download posts and stories from Instagram, we use the package instaloader. You can install package for python using pip install &lt;package&gt;, the command -q minimizes the output.\n!pip -q install instaloader\n\n     |████████████████████████████████| 60 kB 3.0 MB/s eta 0:00:011\n  Building wheel for instaloader (setup.py) ... done\nOnce you install instaloader we log in using your username and password. Session information (not your credentials!) is stored in Google Drive to minimize the need for signing in.\nIn order to minimize the risk for your account to be disabled we suggest creating a new account on your phone before proceeding!\nusername = 'your.username'\n\n# We save the sessionfile to the following directory. Default is the new folder `.instaloader` in your google drive. (This is optional)\nsession_directory = '/content/drive/MyDrive/.instaloader/'\n\nimport instaloader\nfrom os.path import exists\nfrom pathlib import Path\n\n# Creating session directory, if it does not exists yet\nPath(session_directory).mkdir(parents=True, exist_ok=True)\n\nfilename = \"{}session-{}\".format(session_directory, username)\nsessionfile = Path(filename)\n\n\n# Get instance\nL = instaloader.Instaloader(compress_json=False)\n\n# Check if sessionfile exists. If so load session,\n# else login interactively\nif exists(sessionfile):\n  L.load_session_from_file(username, sessionfile)\n\nelse:\n  L.interactive_login(username)\n  L.save_session_to_file(sessionfile)\n\nLoaded session from /content/drive/MyDrive/.instaloader/session-mi_sm_lab05."
  },
  {
    "objectID": "notebooks/ig-instaloader.html#downloading-first-posts",
    "href": "notebooks/ig-instaloader.html#downloading-first-posts",
    "title": "Instagram Posts",
    "section": "Downloading first Posts",
    "text": "Downloading first Posts\nNext, we try to download all posts of a profile. Provide a username and folder:\n\ndest_username = 'some.profile' \ndest_dir = '/content/drive/MyDrive/insta-posts/' # Once more we save the files to Google Drive. Replace this with a local directory if necessary.\n\nt = Path(\"{}{}\".format(dest_dir, dest_username))\nt.mkdir(parents=True, exist_ok=True)\n\nprofile = instaloader.Profile.from_username(L.context, dest_username)\nfor post in profile.get_posts():\n    L.download_post(post, target=t)\n\nWell, you just downloaded your first posts! Open Google Drive and check the folder insta-posts/ (or whatever folder you chose above)! There should be three files for each post, the image, a .json file and a .txt file. The .txt includes the image caption, the .json lots of metadata about the post.\n\nDiving into the metadata\nThe next cell reads all .json files of the downloaded posts. Then we browse through some interesting data.\n\n# Reading the paths of all JSON files from dest_dir\nimport os\n\njson_files = []\n\nfor subdir, dirs, files in os.walk(t):\n    for file in files:\n        fullpath = os.path.join(subdir, file)\n        filename, file_extension = os.path.splitext(fullpath)\n        if file_extension == \".json\":\n          json_files.append(fullpath)\n\n\n# Reading all JSON files\nfrom tqdm.notebook import tqdm\nimport json\n\njson_data = []\n\nfor file in tqdm(json_files):\n  with open(file, 'r') as f:\n    data = json.load(f)\n    json_data.append(data)\n\n\n\n\nOk, now all metadata for all posts is saved to the variable json_data. Run the next line and copy its output to http://jsonviewer.stack.hu/. Your output should look similar, go ahead and play around to explore your data! What information can you extract?\n\nprint(json.dumps(json_data[0]))\n\n\n\nMetadata Preprocessing\nPosts contain plenty of data, like time and location of the post, the authoring user, a caption, tagged users and more. The following cells demonstrate how to normalize the data into a table format, which is useful when working with pandas. Nevertheless, this is optional!\n\n# Use booleans (True / False) values to select what type of data you'd like to analyse. \nusername = True #@param {type:\"boolean\"}\ntimestamp = True #@param {type:\"boolean\"}\ncaption = True #@param {type:\"boolean\"}\nlocation = True #@param {type:\"boolean\"}\nshortcode = True #@param {type:\"boolean\"}\nid = True #@param {type:\"boolean\"}\ntagged_users = True #@param {type:\"boolean\"}\n\nNext we loop through the data and create a new pandas DataFrame. The DataFrame will have one column for each variable selected above and one row for each downloaded posts.\nIf you are not yet familiar with the concept of dataframes have a look at YouTube, there’s plenty of introductory videos available.\n\nimport pandas as pd\n\nposts = []  # Initializing an empty list for all posts\nfor post in tqdm(json_data):\n  row = {} # Initializing an empty row for the post\n\n  node = post.get(\"node\")\n\n  if username:\n    owner = node.get(\"owner\")\n    row['username'] = owner.get(\"username\")\n\n  if timestamp:\n    row['timestamp'] = node.get(\"taken_at_timestamp\")\n\n  if location:\n    l = node.get(\"location\", None)\n    if l:\n      row['location'] = l.get(\"name\")\n\n  if shortcode:\n    row['shortcode'] = node.get(\"shortcode\")\n\n  if id:\n    row['id'] = node.get(\"id\")\n  \n  if tagged_users:\n    pass\n\n  if caption:\n    c = \"\"\n    emtc = node.get(\"edge_media_to_caption\")\n    edges = emtc.get(\"edges\")\n    for element in edges:\n      caption_node = element.get(\"node\")\n      c = c + caption_node.get(\"text\")\n    row['caption'] = c\n\n  # Finally add row to posts\n  posts.append(row)\n\n# After looping through all posts create data frame from list\nposts_df = pd.DataFrame.from_dict(posts)\n\nNow all information selected above is saved to the dataframe posts_df. Run the next cell and it will return a nicely formatted table. If your data is quite long, output will be cropped. Click the wand and after a few seconds you are able to browse through the data or filter by columns\n\nposts_df\n\nIn order to get a first impression of dataframes, the head() method is also useful. Run the next cell to see the result\n\nposts_df.head()\n\nThe dataframe is only saved in memory, thus when disconnecting and deleting the runtime, the dataframe is lost. Running the next cell saves the table to a CSV-file on your drive.\nNow the processed data may be recovered or used in another notebook.\n\nposts_df.to_csv('{}{}.csv'.format(dest_dir, username))"
  },
  {
    "objectID": "notebooks/ocr-notebook.html",
    "href": "notebooks/ocr-notebook.html",
    "title": "Social Media Lab",
    "section": "",
    "text": "We’re using easyocr. See the documentation for more complex configurations. Using CPU only this process takes from minutes to hours (depends on the amount of images). OCR may also be outsourced (e.g. using Google Vision API), see future sessions (and Memespector) for this.\n\n!pip -q install easyocr\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 29.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 908.3/908.3 kB 57.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.2/307.2 kB 29.6 MB/s eta 0:00:00\n\n\n\n# Imports for OCR\nimport easyocr\nreader = easyocr.Reader(['de','en'])\n\nWARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\nWARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\nWARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n\n\nProgress: |██████████████████████████████████████████████████| 100.0% CompleteProgress: |██████████████████████████████████████████████████| 100.0% Complete\n\n\nWe define a very simple method to receive one string for all text recognized: The readtextmethod returns a list of text areas, in this example we concatenate the string, therefore the order of words is sometimes not correct.\nAlso, we save the file to Google Drive to save our results.\n\ndef run_ocr(image_path):\n    ocr_result = reader.readtext(image_path, detail = 0)\n    ocr_text = \" \".join(ocr_result)\n    return ocr_text\n\ndf['ocr_text'] = df['image_file'].apply(run_ocr)\n\n# Saving Results to Drive\ndf.to_csv('/content/drive/MyDrive/2022-11-09-Stories-Exported.csv')\n\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0.1\nUnnamed: 0\nID\nTime of Posting\nType of Content\nvideo_url\nimage_url\nUsername\nVideo Length (s)\nExpiration\n...\nIs Verified\nStickers\nAccessibility Caption\nAttribution URL\nvideo_file\naudio_file\nduration\nsampling_rate\nimage_file\nocr_text\n\n\n\n\n0\n0\n0\n3234500408402516260_1383567706\n2023-11-12 15:21:53\nImage\nNaN\nNaN\nnews24\nNaN\n2023-11-13 15:21:53\n...\nTrue\n[]\nPhoto by News24 on November 12, 2023. May be a...\nhttps://www.threads.net/t/CzjB80Zqme0\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234500408402516260_1383567706.jpg\nKeee WEEKEND NEWS24 PLUS: TESTING FORDS RANGER...\n\n\n1\n1\n1\n3234502795095897337_8537434\n2023-11-12 15:26:39\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:26:39\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234502795095897337_8537434.jpg\nDieses Auto ist einfach der Horror Du glaubst ...\n\n\n2\n2\n2\n3234503046678453705_8537434\n2023-11-12 15:27:10\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:27:10\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234503046678453705_8537434.jpg\nTouchdown bei Taylor Swift und Travis Kelce De...\n\n\n3\n3\n3\n3234503930728728807_8537434\n2023-11-12 15:28:55\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:28:55\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234503930728728807_8537434.jpg\nHorror-Diagnose für Barton Cowperthwaite Netfl...\n\n\n4\n4\n4\n3234504185910204562_8537434\n2023-11-12 15:29:25\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:29:25\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234504185910204562_8537434.jpg\n3v Bilde GG JJ Besorgniserregende Ufo-Aktivitä...\n\n\n\n\n\n5 rows × 21 columns"
  },
  {
    "objectID": "notebooks/literature-review.html",
    "href": "notebooks/literature-review.html",
    "title": "GPT Literature Review Assistant",
    "section": "",
    "text": "This notebook demonstrates how to work with the GPT-API based on a simple use case. First, we are going to import search results of our literature review with Publish or Perish. Next, we are going to explore how we could use python in combination with Publish or Perish to speed up our review process: We will manually code the relevance, using the Jupyter notebook as our labelling interface. Afterwards we are going to add a GPT-API call to extract features from the abstract, the first step towards our assistant guiding our literature review process.\nOverall, this notebooks is a simple implementation demonstrating how prompts work and how easy it is to use GPT in Jupyter notebooks. The notebook is available in the supplement repository, you can clone the notebook to your Colab account with one click."
  },
  {
    "objectID": "notebooks/literature-review.html#setup",
    "href": "notebooks/literature-review.html#setup",
    "title": "GPT Literature Review Assistant",
    "section": "Setup",
    "text": "Setup\nAt first we need to install necessary packages. Hit run and wait.\n\nprint(\"Install Packages\")\n!pip install -q openai crossref-commons"
  },
  {
    "objectID": "notebooks/literature-review.html#import-publish-or-perish-data.",
    "href": "notebooks/literature-review.html#import-publish-or-perish-data.",
    "title": "GPT Literature Review Assistant",
    "section": "Import Publish or Perish Data.",
    "text": "Import Publish or Perish Data.\nIf this is the start of your review process, upload the csv file exported from Publish or Perish in the left-hand Files pane. Enter the filename in publish_or_perish_file_name. Define the output name in file_name. If you want to save the imported file in the google drive add /content/drive/MyDrive/ to the path.  Skip this cell if you want to work with a file that has been imported in the past.\n\n\n\n\n\n\nWarning\n\n\n\nWe delete rows with missing DOIs. Without a DOI our code cannot retrieve abstracts. When importing the Publish or Perish file, the following code will display the number of rows that have been deleted due to missing DOIs. When using this notebook for real-world projects, you should be aware of the missing rows and manually review them!\n\n\n\n#@title Import from Publish or Perish Data.\n#@markdown If this is the start of your review process, upload the `csv` file exported from [Publish or Perish](https://harzing.com/resources/publish-or-perish) in the left-hand *Files* pane. Enter the filename in `publish_or_perish_file_name`. Define the output name in `file_name`. If you want to save the imported file in the google drive add `/content/drive/MyDrive/` to the path. &lt;br/&gt; **Skip this cell if you want to work with a file that has been imported in the past.**\n\nimport pandas as pd\nimport numpy as np\nimport io\n\npublish_or_perish_file_name = \"scholar.csv\" # @param {type: \"string\"}\nfile_name = \"2023-10-31-Literature-Review.csv\" # @param {type: \"string\"}\n\n# Initialize empty DataFrame\nall_data = pd.DataFrame()\n\n\ntry:\n    all_data = pd.read_csv(publish_or_perish_file_name)\n\n    # Remove Duplicates\n    initial_len = len(all_data)\n    all_data = all_data.drop_duplicates(subset='DOI', keep='first')\n    removed_len = initial_len - len(all_data)\n    print(f'Removed {removed_len} duplicates based on DOI.')\n\n    # Remove missing DOIs\n    initial_len = len(all_data)\n    all_data = all_data[~pd.isna(all_data['DOI'])]\n    removed_len = initial_len - len(all_data)\n    print(f'Removed {removed_len} rows without DOI.')\n\n    all_data = all_data.sort_values(by='Cites', ascending=False).reset_index(drop=True)\n\n    print('Sorted Table by Cites.')\n\n    # Create empty columns for Literature Review\n    all_data[\"Relevant\"] = \"\"\n    all_data[\"Notes\"] = \"\"\n    all_data[\"Checked\"] = False\n\n    print('Initialized Columns')\n\n    all_data.to_csv(file_name)\n    print(f\"Success: Saved data to {file_name}\")\n\n    print(f'Success: Data loaded from File \"{file_name}\".')\nexcept Exception as e:\n    print(f\"Error: Failed to load data from File. {str(e)}\")\n\nRemoved 172 duplicates based on DOI.\nRemoved 1 rows without DOI.\nSorted Table by Cites.\nInitializes Columns\nSuccess: Saved data to 2023-10-31-Literature-Review.csv\nSuccess: Data loaded from File \"2023-10-31-Literature-Review.csv\"."
  },
  {
    "objectID": "notebooks/literature-review.html#read-previously-imported-file",
    "href": "notebooks/literature-review.html#read-previously-imported-file",
    "title": "GPT Literature Review Assistant",
    "section": "Read previously imported File",
    "text": "Read previously imported File\nIf you want to keep going with a former review process, we can read an uploaded file / a file from google drive. Only run one cell, this one or the above.\n\n#@title Read previously imported File\n#@markdown If you want to keep going with a former review process, we can read an uploaded file / a file from google drive. **Only run one cell, this one or the above.**\nimport pandas as pd\nimport numpy as np\nimport io\n\nfile_name = \"2023-10-31-Literature-Review.csv\" # @param {type: \"string\"}\n\ntry:\n    all_data = pd.read_csv(file_name)\n\n    print(f'Success: Data loaded from File \"{file_name}\".')\nexcept Exception as e:\n    print(f\"Error: Failed to load data from File. {str(e)}\")\n\nSuccess: Data loaded from File \"2023-10-31-Literature-Review.csv\".\n\n\n\nIn this example we’ve saved the file locally. When working with Colab, the file will be deleted when we disconnect. For colab you should link your google drive (open the files pane on the left, click the Google Drive button). Once connected, save the file in the folder /content/drive/MyDrive/YOUR-FILENAME.csv. It will be accessible through Drive, and Colab is from now on going to connect automatically to drive.\nCheck the imported data. We’re using pandas, the imported data is saved in the all_datavariable. head(2)displays the two top rows of the table. Additionally, we have added three columns: Relevant, Notes, and Checked. We are going to make use of them to keep track of our progress.\n\n# Check the structure (and content) of the file\nall_data.head(2)\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0.2\nUnnamed: 0.1\nUnnamed: 0\nCites\nAuthors\nTitle\nYear\nSource\nPublisher\nArticleURL\n...\nAge\nAbstract\nFullTextURL\nRelatedURL\nbabbage_similarity\nbabbage_search\nsimilarities\nRelevant\nNotes\nChecked\n\n\n\n\n0\n0\n746\n844\n21\nFlorian Arendt\nSuicide on Instagram – Content Analysis of a G...\n2019.0\nCrisis\nHogrefe Publishing Group\nhttp://dx.doi.org/10.1027/0227-5910/a000529\n...\n3.0\nAbstract. Background: Suicide is the second le...\nhttps://econtent.hogrefe.com/doi/pdf/10.1027/0...\nNaN\n[-0.0018475924152880907, 0.022463073953986168,...\n[-0.014954154379665852, 0.026176564395427704, ...\n-1\nNaN\nNaN\nFalse\n\n\n1\n1\n770\n868\n4\nPaloma de H. Sánchez-Cobarro, Francisco-Jose M...\nThe Brand-Generated Content Interaction of Ins...\n2020.0\nJournal of Theoretical and Applied Electronic ...\nMDPI AG\nhttp://dx.doi.org/10.3390/jtaer16030031\n...\n2.0\nThe last decade has seen a considerable increa...\nhttps://www.mdpi.com/0718-1876/16/3/31/pdf\nNaN\n[-0.0029447057750076056, 0.01190990675240755, ...\n[-0.01012819167226553, 0.02539714053273201, -0...\n-1\nNaN\nNaN\nFalse\n\n\n\n\n\n2 rows × 35 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nIn the next step we are going to start our literature review:\n\nWe filter for the first unchecked row, ordered by the cite count.\nWe retrieve the abstract from CrossRef API using the DOI.\nWe display all information\nWe answer whether the paper appear to be relevant by entering y or n for yes or no.\n\nFor our session, the cell only runs through one row and finishes afterwards. For a real world application you’d probably like to add some kind of loop.\n\nfrom crossref_commons.retrieval import get_publication_as_json\nimport json\nimport openai\nimport textwrap\nimport IPython\nimport re\n\n# Get one row: Not checked, highest Citation count.\nhighest_cites_unchecked = all_data[all_data['Checked'] == False].sort_values(by=\"Cites\", ascending=False).iloc[0]\nindex = highest_cites_unchecked.name\n\n# Retrieve Abstract from Crossref\nresponse = get_publication_as_json(highest_cites_unchecked['DOI'])\nabstract = response.get(\"abstract\", \"\")\n\n# Remove XML\nabstract = re.sub(r'&lt;[^&gt;]+&gt;', '', abstract)\nall_data.loc[index, 'Abstract'] = abstract\n\n\n# Display all information\nIPython.display.clear_output(wait=True)\ntitle_disp = IPython.display.HTML(\"&lt;h2&gt;{}&lt;/h2&gt;\".format(highest_cites_unchecked['Title']))\nauthors_disp = IPython.display.HTML(\"&lt;p&gt;{}&lt;/p&gt;\".format(highest_cites_unchecked['Authors']))\ndoi_disp = IPython.display.HTML(\"&lt;p&gt;&lt;a target='_blank' href='https://doi.org/{}'&gt;{}&lt;/a&gt;&lt;/p&gt;\".format(highest_cites_unchecked['DOI'],highest_cites_unchecked['DOI']))\ndisplay(title_disp, authors_disp, doi_disp)\nprint(textwrap.fill(abstract, 80))\nrelevant_input = input('Relevant? (y/n): ').lower().strip() == 'y'\n\n# Save user input\nall_data.loc[index, 'Checked'] = True\nall_data.loc[index, 'Relevant'] = relevant_input\n\nSuicide on Instagram – Content Analysis of a German Suicide-Related Hashtag\n\n\nFlorian Arendt\n\n\n10.1027/0227-5910/a000529\n\n\nAbstract. Background: Suicide is the second leading cause of death among\n15–29-year-olds globally. Unfortunately, the suicide-related content on\nInstagram, a popular social media platform for youth, has not received the\nscholarly attention it deserves. Method: The present study provides a content\nanalysis of posts tagged as #selbstmord, a German suicide-related hashtag. These\nposts were created between July 5 and July 11, 2017. Results: Approximately half\nof all posts included words or visuals related to suicide. Cutting was by far\nthe most prominent method. Although sadness was the dominant emotion, self-hate\nand loneliness also appeared regularly. Importantly, inconsistency – a gap\nbetween one's inner mental state (e.g., sadness) and one's overtly expressed\nbehavior (e.g., smiling) – was also a recurring theme. Conversely, help-seeking,\ndeath wishes, and professional awareness–intervention material were very rare.\nAn explorative analysis revealed that some videos relied on very fast cutting\ntechniques. We provide tentative evidence that users may be exposed to\npurposefully inserted suicide-related subliminal messages (i.e., exposure to\ncontent without the user's conscious awareness). Limitations: We only\ninvestigated the content of posts on one German hashtag, and the sample size was\nrather small. Conclusion: Suicide prevention organizations may consider posting\nmore awareness–intervention materials. Future research should investigate\nsuicide-related subliminal messages in social media video posts. Although\ntentative, this finding should raise a warning flag for suicide prevention\nscholars.\nRelevant? (y/n): y\n\n\nNext, we check whether our input has been saved:\n\n# Check the result\nall_data.iloc[index]\n\nUnnamed: 0.2                                                          0\nUnnamed: 0.1                                                        746\nUnnamed: 0                                                          844\nCites                                                                21\nAuthors                                                  Florian Arendt\nTitle                 Suicide on Instagram – Content Analysis of a G...\nYear                                                             2019.0\nSource                                                           Crisis\nPublisher                                      Hogrefe Publishing Group\nArticleURL                  http://dx.doi.org/10.1027/0227-5910/a000529\nCitesURL                                                            NaN\nGSRank                                                               26\nQueryDate                                           2022-09-08 10:44:44\nType                                                    journal-article\nDOI                                           10.1027/0227-5910/a000529\nISSN                                                          0227-5910\nCitationURL                                                         NaN\nVolume                                                             40.0\nIssue                                                               1.0\nStartPage                                                          36.0\nEndPage                                                            41.0\nECC                                                                  21\nCitesPerYear                                                        7.0\nCitesPerAuthor                                                       21\nAuthorCount                                                           1\nAge                                                                 3.0\nAbstract              Abstract. Background: Suicide is the second le...\nFullTextURL           https://econtent.hogrefe.com/doi/pdf/10.1027/0...\nRelatedURL                                                          NaN\nbabbage_similarity    [-0.0018475924152880907, 0.022463073953986168,...\nbabbage_search        [-0.014954154379665852, 0.026176564395427704, ...\nsimilarities                                                         -1\nRelevant                                                           True\nNotes                                                               NaN\nChecked                                                            True\nName: 0, dtype: object"
  },
  {
    "objectID": "notebooks/literature-review.html#using-gpt-to-extract-information-from-abstracts",
    "href": "notebooks/literature-review.html#using-gpt-to-extract-information-from-abstracts",
    "title": "GPT Literature Review Assistant",
    "section": "Using GPT to extract information from abstracts",
    "text": "Using GPT to extract information from abstracts\nNow for the fun part: Is it possible to use GPT to help us during the review process? We are going to try and extract text features automatically. For the moment we are going to use gpt3.5-turbo.\nNote: Please feel free to test different prompts and questions. The Promptingguide is a good resource to learn more about different prompting techniques. Use the ChatGPT interface to cheaply test prompts prior to using them with the API. Use the OpenAI Playground to optimize your prompts with a visual user interface for different settings and a prompting history (trust me, this can save your life!).\nPrompts: We’re going to use the system prompt for our instructions, and the user prompt to send our content.\n\n\n\n\n\n\nCaution\n\n\n\nA word of warning: You should not trust the quality of the GPT output at this stage. The prompt has not been evaluated, overall LLMs produce output that appears meaningful most of the times. Sometimes, however, it is Hallucinations. Thus, before using prompts and LLMs for production, we have to make sure we can trust their outputs. We will dive deeper into this topic in the classification sessions.\n\n\n\nsystem_prompt = \"\"\"\nYou're an advanced AI research assistant. Your task is to extract **research questions**, **operationalization**, **data sources**, **population**, and **scientific disciplines** from user input. Return \"None\" if you can't find the information in user input.\n\n**Formatting**\nReturn a markdown table, one row for each extracted feature: **research questions**, **operationalization**, **data sources**, **population**, and **scientific disciplines**.\n\"\"\"\n\nPlease enter your API-Code in the next code cell for the openai.api_key variable. We have changed the cell to include the gpt_prompt variable, which sends the title and abstract as a user prompt. We’re using the openai.ChatCompletion.create() method to send our request to the API. We expect the response in api_response['choices'][0]['message']['content'] to be markdown (see prompt above), as such we display the markdown in our notebook.\n\nfrom crossref_commons.retrieval import get_publication_as_json\nimport json\nimport openai\nimport textwrap\nimport IPython\nimport re\n\n# Enter OpenAI API-Code\nopenai.api_key = \"sk-XXXXXXXXX\"\n\n# Get one row: Not checked, highest Citation count.\nhighest_cites_unchecked = all_data[all_data['Checked'] == False].sort_values(by=\"Cites\", ascending=False).iloc[0]\nindex = highest_cites_unchecked.name\n\n# Retrieve Abstract from Crossref\nresponse = get_publication_as_json(highest_cites_unchecked['DOI'])\nabstract = response.get(\"abstract\", \"\")\n\n# Remove XML\nabstract = re.sub(r'&lt;[^&gt;]+&gt;', '', abstract)\n\nall_data.loc[index, 'Abstract'] = abstract\n\n# Display all information (before we send the request to OpenAI)\nIPython.display.clear_output(wait=True)\ntitle_disp = IPython.display.HTML(\"&lt;h2&gt;{}&lt;/h2&gt;\".format(highest_cites_unchecked['Title']))\nauthors_disp = IPython.display.HTML(\"&lt;p&gt;{}&lt;/p&gt;\".format(highest_cites_unchecked['Authors']))\ndoi_disp = IPython.display.HTML(\"&lt;p&gt;&lt;a target='_blank' href='https://doi.org/{}'&gt;{}&lt;/a&gt;&lt;/p&gt;\".format(highest_cites_unchecked['DOI'],highest_cites_unchecked['DOI']))\ndisplay(title_disp, authors_disp, doi_disp)\nprint(textwrap.fill(abstract, 80))\n\ngpt_prompt = f\"\"\"\n**Title**: {highest_cites_unchecked['Title']}\n**Abstract**: {abstract}\n\"\"\"\n\n# Sending request, takes a moment. In the meantime you may read the abstract.\nmessages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": abstract}\n]\n\ntry:\n  api_response = openai.ChatCompletion.create(\n      model=\"gpt-3.5-turbo\",\n      messages=messages,\n      temperature=0,\n      timeout=30\n    )\n\n  gpt_result = api_response['choices'][0]['message']['content']\n\n  # Display the GPT result\n  display(IPython.display.HTML(f\"&lt;h3&gt;GPT Extracted Data&lt;/h3&gt;\"))\n  display(IPython.display.Markdown(gpt_result))\nexcept:\n  print(\"GPT API Error\")\n\nrelevant_input = input('Relevant? (y/n): ').lower().strip() == 'y'\n\n# Save user input\nall_data.loc[index, 'Checked'] = True\nall_data.loc[index, 'Relevant'] = relevant_input\n\nThe Brand-Generated Content Interaction of Instagram Stories and Publications: A Comparison between Retailers and Manufacturers\n\n\nPaloma de H. Sánchez-Cobarro, Francisco-Jose Molina-Castillo, Cristina Alcazar-Caceres\n\n\n10.3390/jtaer16030031\n\n\nThe last decade has seen a considerable increase in entertainment-oriented\ncommunication techniques. Likewise, the rise of social networks has evolved,\noffering different formats such as publication and stories. Hence, there has\nbeen a growing interest in knowing which strategies have the greatest social\nimpact to help position organizations in the mind of the consumer. This research\naims to analyze the different impact that stories and publications can have on\nthe Instagram social network as a tool for generating branded content. To this\nend, it analyses the impact of the different Instagram stories and publications\nin various sectors using a methodology of structural equations with composite\nconstructs. The results obtained, based on 800 stories and publications in four\ndifferent companies (retailers and manufacturers), show that the reach of the\nstory generally explains the interaction with Instagram stories. In contrast, in\nthe case of publications, impressions are of greater importance in explaining\nthe interaction with the publication. Among the main contributions of the work,\nwe find that traditional pull communication techniques have been losing\neffectiveness in front of new formats of brand content generation that have been\noccupying the time in the relationship between users and brands.\nRelevant? (y/n): y\n\n\nGPT Extracted Data\n\n\n\n\n\n\n\n\n\nFeature\nValue\n\n\n\n\nResearch questions\n- What strategies have the greatest social impact on Instagram?- How do stories and publications on Instagram impact the consumer’s perception of brands?- What is the relationship between reach and interaction with Instagram stories?- What is the relationship between impressions and interaction with Instagram publications?\n\n\nOperationalization\n- Analyzing the impact of Instagram stories and publications in various sectors- Using a methodology of structural equations with composite constructs\n\n\nData sources\n- 800 stories and publications on Instagram\n\n\nPopulation\n- Four different companies (retailers and manufacturers)\n\n\nScientific disciplines\n- Marketing- Communication\n\n\n\n\n\n\nThe above output shows a formatted table listing all extracted features. In this short warm-up session on GPT we have seen one use case of the LLM: The extraction of text feautures. In future sessions we are going to dive deeper into this topic.\n\n\n\n\n\n\nNote\n\n\n\nDid you create an excellent prompt? Share it with us! Enter your prompt into this Excel Sheet"
  },
  {
    "objectID": "notebooks/literature-review.html#save-your-progress",
    "href": "notebooks/literature-review.html#save-your-progress",
    "title": "GPT Literature Review Assistant",
    "section": "Save your Progress",
    "text": "Save your Progress\nThe following line saves all progress to file_name. If file_name is a path to Google Drive you will be able to pick up your work later on.\n\nall_data.to_csv(file_name)"
  }
]