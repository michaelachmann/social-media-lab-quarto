[
  {
    "objectID": "notebooks/2023_12_11_GPT_Text_Classification3.html",
    "href": "notebooks/2023_12_11_GPT_Text_Classification3.html",
    "title": "From Documents to Markdown Tables",
    "section": "",
    "text": "system_prompt = \"\"\"\nYou are an advanced classifying AI. Your task is to classify the sentiment of a text. Sentiment can be either ‚Äòpositive‚Äô, ‚Äònegative‚Äô, or ‚Äòneutral‚Äô.\n**Instructions**\n  1. Examine each row in the table under the 'Text' column.\n  2. For each row consisting of social media comments, classify the content into either ‚Äònegative‚Äô, ‚Äòneutral‚Äô, or ‚Äòpositive‚Äô.\n  3. Fill the 'Classification' column for the corresponding 'Text' row with your answer. Your answer MUST be one of [‚Äònegative‚Äô, ‚Äòneutral‚Äô, ‚Äòpositive‚Äô], and it should be presented in lowercase.\n**Formatting**\nReturn a markdown table with the columns \"shortcode\" and \"Classification\"\n\"\"\"\n\nWe use the tabulate python package to create markdown tables for as many tables as we manage to send within the model‚Äôs context window. Currently, the result_table token length (the mockup response) is calculated using the length of False. Replace the value if you expect longer classifications in this line:\ncurrent_result_table = tabulate(batched_data + [(row[meta], False)], headers=[meta, \"Classification\"], tablefmt=\"pipe\")\n\nfrom tabulate import tabulate\nfrom datetime import datetime\nfrom gpt_cost_estimator import num_tokens_from_messages\n\ndef batch_rows_for_tables(df, system_prompt, column, meta, model=\"gpt-3.5-turbo-0613\", **kwargs):\n    max_rows = kwargs.get(\"max_rows\", 999)\n    if model == \"gpt-4-0613\":\n      max_tokens = 8192\n\n    if model == \"gpt-4-1106-preview\":\n      max_tokens = 128000 # This model has not been tested with the multidocument approach. It is only capable of 4096 tokens output, therefore we might run into trouble\n\n    if model == \"gpt-3.5-turbo-0613\":\n      max_tokens = 4096\n\n    \"\"\"Batch rows from the dataframe to fit within token limits and return as a list of markdown tables.\"\"\"\n    tables = []\n\n    df[column] = df[column].astype(str)\n\n    pbar = tqdm(total=len(df))\n\n\n    while not df.empty:\n        current_tokens = 0\n        batched_data = []\n        batched_results = []\n\n        i = 0\n        for index, row in df.iterrows():\n            # Remove newline characters from the specific column\n            cleaned_data = row[column].replace('\\n', ' ')\n\n            # Construct the table for the current batch\n            current_table = tabulate(batched_data + [(row[meta], cleaned_data)], headers=[meta, \"Text\"], tablefmt=\"pipe\")\n            current_result_table = tabulate(batched_data + [(row[meta], False)], headers=[meta, \"Classification\"], tablefmt=\"pipe\")\n\n            message = [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": current_table},\n                {\"role\": \"assistant\", \"content\": current_result_table}\n                ]\n\n            tokens_needed = num_tokens_from_messages(message, model=model)\n\n            if tokens_needed &lt;= max_tokens and i &lt; max_rows:\n                current_tokens = tokens_needed\n                batched_data.append((row[meta], cleaned_data))\n                batched_results.append((row[meta], False))\n                df.drop(index, inplace=True)\n                i += 1\n            else:\n                # Stop when you've reached close to the max token count\n                pbar.update(len(batched_data))\n                break\n\n        # Convert batched rows to a markdown table and store in tables list\n        markdown_table = tabulate(batched_data, headers=[meta, \"Text\"], tablefmt=\"pipe\")\n        tables.append(markdown_table)\n\n    pbar.close()\n\n    return tables\n\nThe next command uses the above function to generate all necessary markdown tables. The column parameter of batch_rows_for_tables expects the name of the text column, the meta parameter expects the name of the identifier column. Additionally, we pass the dataframe, system_prompt, and MODEL to the function. Fill in the TEXT_COLUMN, IDENTIFIER, MODEL, and MAX_ROWS variables as needed. See the comments above each variable for more information.\n\n#@markdown What's the column name of the text column?\nTEXT_COLUMN = 'Text' # @param {type: \"string\"}\n#@markdown What's the column name of the text column?\nIDENTIFIER = 'shortcode' # @param {type: \"string\"}\n#@markdown Which model do you want to use?\nMODEL = \"gpt-4-0613\" # @param [\"gpt-3.5-turbo-0613\", \"gpt-4-1106-preview\", \"gpt-4-0613\"] {allow-input: true}\n#@markdown Is there a maximum length of rows? (**Set a very high number, like 999, to disable this feature**)\nMAX_ROWS = 999 # @param {type: \"number\", min:0}\n\n# Create a copy of your df. This is important! The batching process removes processed rows from the df.\ndf_batch_copy = df.copy()\n\n# Batching the tables, takes a few seconds (~1 Minute)\ntables = batch_rows_for_tables(df_batch_copy, system_prompt, TEXT_COLUMN, IDENTIFIER, MODEL, max_rows=MAX_ROWS)\n\n\n\n\nLet‚Äôs inspect the table. This is one of many tables that will be sent to the model. (I set the MAX_ROWS to 5 to keep the example short. When working with this approach I usually use MAX_ROWS=999.)\n\nprint(tables[0])\n\n| shortcode   | Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n|:------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| CyMAe_tufcR | #Landtagswahl23 ü§©üß°üôè #FREIEW√ÑHLER #Aiwanger #Danke #Landtagswahl                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| CyL975vouHU | Die Landtagswahl war f√ºr uns als Liberale hart. Wir haben alles gegeben, um die FDP wieder in den Landtag zu bringen, aber leider hat es nicht gereicht. Danke f√ºr euren Einsatz, egal ob beim Plakatieren, Flyern oder am Infostand. üíõ  Wir Julis stehen f√ºr unsere √úberzeugungen ein, auch wenn es gerade nicht gut l√§uft. Das macht uns aus! Das haben wir in diesem Wahlkampf gezeigt und das werden wir auch in der au√üerparlamentarischen Opposition zeigen. üí™  Du bist auch davon √ºberzeugt, dass Freiheit und Eigenverantwortung eine Stimme in der Politik brauchen? Dann steh auch du jetzt f√ºr diese √úberzeugung ein. Unter www.julis.de/mitglied-werden/ kannst du noch heute Mitglied der besten Jugendorganisation der Welt werden. üöÄ  #freistart23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| CyL8GWWJmci | Nach einem starken Wahlkampf ein verdientes Ergebnis! üí™ Herzlichen Gl√ºckwunsch an die CSU und unsere bayrischen JUler, die in der n√§chsten Legislaturperiode f√ºr ein sicheres und stabiles Bayern arbeiten werden. Wir w√ºnschen euch viel Erfolg und alles Gute f√ºr das Landtagsmandat (v.l.n.r.): Manuel Knoll, Konrad Baur, Daniel Artmann, Kristan von Waldenfels.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| CyL7wyJtTV5 | So viele Menschen am Odeonsplatz heute mit einer klaren Botschaft: Wir stehen an der Seite Israels.   Die massiven und brutalen Angriffe der Terrororganisation Hamas sind abscheuliche Verbrechen an unschuldigen M√§nnern, Frauen und Kindern. Die Bilder und Videos der barbarischen Morde zerrei√üen einem das Herz.   Der Terror der Hamas ist durch nichts zu rechtfertigen und muss sofort gestoppt werden. Israel hat ein v√∂lkerrechtlich verbrieftes Recht auf Selbstverteidigung.  Wir Gedenken den Toten. Wir trauern mit den Familien und Angeh√∂rigen. Und wir bangen und hoffen mit den verschleppten Israelis.   Es ist gut, dass die Bundesregierung die Entwicklungshilfe f√ºr die palestinensischen Gebiete eingefroren hat. Das ist richtig.   Nicht richtig ist, dass Menschen in Deutschland die Angriffe der Hamas auf J√ºdinnen und Juden feiern. Das ist mit nichts zu rechtfertigen und wir verurteilen es aufs sch√§rfste.   Wir hier in Deutschland und Bayern haben noch viel zu tun: Antisemitismus und auch israelbezogener Antisemitismus ist in der Mitte unserer Gesellschaft vorhanden. Es ist die Aufgabe des frisch gew√§hlten Bayerischen Landtags noch mehr gegen Judenhass zu tun.   üì∏ @andreasgregor   #standwithisrael #israel #m√ºnchen #bayern |\n| CyLxwHuvR4Y | Herzlichen Gl√ºckwunsch zu diesem grandiosen Wahlsieg!  Mit allen 12 JU-Direktkandidaten seid ihr in den hessischen Landtag gezogen üéâ Wir gratulieren euch und w√ºnschen euch viel Erfolg f√ºr den Start und die n√§chsten f√ºnf Jahre im Parlament (v.l.n.r.): Kim-Sarah Speer, Frederik Bouffier, Sebastian Sommer, Lucas Schmitz, Sebastian M√ºller, Christin Ziegler, Marie-Sophie K√ºnkel, Maximilian Schimmel, Christoph Mikuschek, Patrick Appel, Maximilian Bathon und Dominik Leyh!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n\n\nWe can also inspect them using Markdown formatting in the notebooks:\n\nfrom IPython.display import Markdown, display\n\ndisplay(Markdown(tables[0]))\n\n\n\n\n\n\n\n\nshortcode\nText\n\n\n\n\nCyMAe_tufcR\n#Landtagswahl23 ü§©üß°üôè #FREIEW√ÑHLER #Aiwanger #Danke #Landtagswahl\n\n\nCyL975vouHU\nDie Landtagswahl war f√ºr uns als Liberale hart. Wir haben alles gegeben, um die FDP wieder in den Landtag zu bringen, aber leider hat es nicht gereicht. Danke f√ºr euren Einsatz, egal ob beim Plakatieren, Flyern oder am Infostand. üíõ Wir Julis stehen f√ºr unsere √úberzeugungen ein, auch wenn es gerade nicht gut l√§uft. Das macht uns aus! Das haben wir in diesem Wahlkampf gezeigt und das werden wir auch in der au√üerparlamentarischen Opposition zeigen. üí™ Du bist auch davon √ºberzeugt, dass Freiheit und Eigenverantwortung eine Stimme in der Politik brauchen? Dann steh auch du jetzt f√ºr diese √úberzeugung ein. Unter www.julis.de/mitglied-werden/ kannst du noch heute Mitglied der besten Jugendorganisation der Welt werden. üöÄ #freistart23\n\n\nCyL8GWWJmci\nNach einem starken Wahlkampf ein verdientes Ergebnis! üí™ Herzlichen Gl√ºckwunsch an die CSU und unsere bayrischen JUler, die in der n√§chsten Legislaturperiode f√ºr ein sicheres und stabiles Bayern arbeiten werden. Wir w√ºnschen euch viel Erfolg und alles Gute f√ºr das Landtagsmandat (v.l.n.r.): Manuel Knoll, Konrad Baur, Daniel Artmann, Kristan von Waldenfels.\n\n\nCyL7wyJtTV5\nSo viele Menschen am Odeonsplatz heute mit einer klaren Botschaft: Wir stehen an der Seite Israels. Die massiven und brutalen Angriffe der Terrororganisation Hamas sind abscheuliche Verbrechen an unschuldigen M√§nnern, Frauen und Kindern. Die Bilder und Videos der barbarischen Morde zerrei√üen einem das Herz. Der Terror der Hamas ist durch nichts zu rechtfertigen und muss sofort gestoppt werden. Israel hat ein v√∂lkerrechtlich verbrieftes Recht auf Selbstverteidigung. Wir Gedenken den Toten. Wir trauern mit den Familien und Angeh√∂rigen. Und wir bangen und hoffen mit den verschleppten Israelis. Es ist gut, dass die Bundesregierung die Entwicklungshilfe f√ºr die palestinensischen Gebiete eingefroren hat. Das ist richtig. Nicht richtig ist, dass Menschen in Deutschland die Angriffe der Hamas auf J√ºdinnen und Juden feiern. Das ist mit nichts zu rechtfertigen und wir verurteilen es aufs sch√§rfste. Wir hier in Deutschland und Bayern haben noch viel zu tun: Antisemitismus und auch israelbezogener Antisemitismus ist in der Mitte unserer Gesellschaft vorhanden. Es ist die Aufgabe des frisch gew√§hlten Bayerischen Landtags noch mehr gegen Judenhass zu tun. üì∏ @andreasgregor #standwithisrael #israel #m√ºnchen #bayern\n\n\nCyLxwHuvR4Y\nHerzlichen Gl√ºckwunsch zu diesem grandiosen Wahlsieg! Mit allen 12 JU-Direktkandidaten seid ihr in den hessischen Landtag gezogen üéâ Wir gratulieren euch und w√ºnschen euch viel Erfolg f√ºr den Start und die n√§chsten f√ºnf Jahre im Parlament (v.l.n.r.): Kim-Sarah Speer, Frederik Bouffier, Sebastian Sommer, Lucas Schmitz, Sebastian M√ºller, Christin Ziegler, Marie-Sophie K√ºnkel, Maximilian Schimmel, Christoph Mikuschek, Patrick Appel, Maximilian Bathon und Dominik Leyh!\n\n\n\n\n\n\nRun the Multidocument Request\nhe following code snippet uses my gpt-cost-estimator package to simulate API requests and calculate a cost estimate. Please run the estimation whne possible to asses the price-tag before sending requests to OpenAI!\nFill in the MOCK, RESET_COST, SAMPLE_SIZE, CLASS_NAME, and FILE_NAME variables as needed (see comments above each variable.)\n\nfrom tqdm.auto import tqdm\nimport json\nimport ast\nfrom datetime import datetime\nfrom io import StringIO\n\n#@title Run the Multidocument Request\n#@markdown T\n#@markdown Do you want to mock the OpenAI request (dry run) to calculate the estimated price?\nMOCK = False # @param {type: \"boolean\"}\n#@markdown Do you want to reset the cost estimation when running the query?\nRESET_COST = True # @param {type: \"boolean\"}\n\n#@markdown How many **tables** do you want to send? Enter $0$ for all.\nSAMPLE_SIZE = 1 # @param {type: \"number\", min: 0}\n\n#@markdown Filename for the **new** table that only contains sentiments.\nFILE_NAME = '/content/drive/MyDrive/2023-12-08-Posts-LTW-Sentiment' # @param {type: \"string\"}\n\n#@markdown Name for the classification column\nCLASS_NAME = 'Sentiment' # @param {type: \"string\"}\n\n\ndef safe_literal_eval(value):\n    if isinstance(value, (str, bytes)):\n        try:\n            return ast.literal_eval(value)\n        except ValueError:\n            return value  # or handle the error in another way if you want\n    return value\n\ndef parse_response(response):\n    # Determine if the response is a list or markdown table\n    if ':' in response.split('\\n')[0]:\n        # List\n        lines = [line.strip() for line in response.strip().split('\\n')]\n        data = [(int(line.split(': ')[0]), line.split(': ')[1]) for line in lines]\n        # Convert the parsed data into a DataFrame\n        result_df = pd.DataFrame(data, columns=['uuid', 'Positioning'])\n    else:\n        # Markdown Table\n        csv_data = '\\n'.join([','.join(line.split('|')[1:-1]) for line in response.split('\\n') if line.strip() and not line.startswith('|:')])\n        result_df = pd.read_csv(StringIO(csv_data.strip()), sep=\",\", skipinitialspace=True)\n\n\n    # Striping Whitespaces\n    result_df.columns = [col.strip() for col in result_df.columns]\n    if 'Classification' in result_df.columns:\n        # Renaming the column to fit the rest of the project.\n        result_df = result_df.rename(columns={\"Classification\": CLASS_NAME})\n\n    result_df = result_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n\n    return result_df\n\n\ntry:\n    # Attempt to read the CSV file into a DataFrame\n    new_df = pd.read_csv(FILE_NAME)\nexcept FileNotFoundError:\n    # If the file is not found, create an empty DataFrame with the specified columns\n    new_df = pd.DataFrame(columns=[IDENTIFIER, CLASS_NAME])\n\n# Reset Estimates\nCostEstimator.reset()\nprint(\"Reset Cost Estimation\")\n\nif 0 &lt; SAMPLE_SIZE &lt;= len(tables):\n    filtered_tables = tables[:SAMPLE_SIZE]\nelse:\n    filtered_tables = tables\n\nfor table in tqdm(filtered_tables):\n    result = run_request(system_prompt, table, MODEL, MOCK)\n    if result and not MOCK:\n      # Parsing the data\n      result_df = parse_response(result.choices[0].message.content)\n\n      # Append it to master_df\n      new_df = pd.concat([new_df, result_df], ignore_index=True)\n\n      # Save Progress\n      new_df.to_csv(FILE_NAME, index=False)\n\nprint()\n\nif not MOCK:\n  print(f\"Saved {FILE_NAME}.\")\n\n  new_df = new_df.dropna(subset=[IDENTIFIER])\n  new_df[CLASS_NAME] = new_df[CLASS_NAME].apply(safe_literal_eval)\n  uuid_to_classification = new_df.set_index(IDENTIFIER)[CLASS_NAME].to_dict()\n  mask = df[IDENTIFIER].isin(uuid_to_classification.keys())\n  df.loc[mask, CLASS_NAME] = df.loc[mask, IDENTIFIER].replace(uuid_to_classification)\n\nprint()\n\nReset Cost Estimation\nCost: $0.1408 | Total: $0.1408\nSaved /content/drive/MyDrive/2023-12-08-Posts-LTW-Sentiment.\n\n\n\n\n\n\n\nnew_df.head()\n\n\n  \n    \n\n\n\n\n\n\nshortcode\nSentiment\n\n\n\n\n0\nCyMAe_tufcR\npositive\n\n\n1\nCyL975vouHU\nneutral\n\n\n2\nCyL8GWWJmci\npositive\n\n\n3\nCyL7wyJtTV5\nnegative\n\n\n4\nCyLxwHuvR4Y\npositive\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nThe code above expects the GPT-API to return results in a markdown formatted table (see above). We keep appending the API responses to a new_df where we temporarily store the classifications. For each loop (i.e.¬†each time received a classification), we store the results on Google Drive as a backup, since each result has a price tag. In case of error we can resume the operation later without the need to start all over again. The code above does not provide the necessary logic for that, but you should be able to quickly add it.\nOnce the loop finished, we use the shortcode column from the API response and join the classification data with df:\n\nAnd finally our df looks as follows. As outlined at the start of the text exploration chapter, we want to fill one dataframe piece by piece with more and more classifications.\n\ndf[mask][['shortcode', 'Text', 'Text Type', 'Sentiment']].head()\n\n\n  \n    \n\n\n\n\n\n\nshortcode\nText\nText Type\nSentiment\n\n\n\n\n0\nCyMAe_tufcR\n#Landtagswahl23 ü§©üß°üôè #FREIEW√ÑHLER #Aiwanger #Da...\nCaption\npositive\n\n\n1\nCyL975vouHU\nDie Landtagswahl war f√ºr uns als Liberale hart...\nCaption\nneutral\n\n\n2\nCyL8GWWJmci\nNach einem starken Wahlkampf ein verdientes Er...\nCaption\npositive\n\n\n3\nCyL7wyJtTV5\nSo viele Menschen am Odeonsplatz heute mit ein...\nCaption\nnegative\n\n\n4\nCyLxwHuvR4Y\nHerzlichen Gl√ºckwunsch zu diesem grandiosen Wa...\nCaption\npositive"
  },
  {
    "objectID": "notebooks/2023_12_01_BERTopic.html",
    "href": "notebooks/2023_12_01_BERTopic.html",
    "title": "Import CrowdTangle Data",
    "section": "",
    "text": "For this example we import a CrowdTangle dataframe, which has been preprocessing using the OCR Notebook. We are only dealing with one image per post, there are no videos (= no transcriptions). In this example, we have up to two text columns per Post, Description which contains the caption, and ocr_text. When exploring the textual content of the posts, we see each of those columns as one document. Thus, we transform our table and create new_df as a Text Table that contains a reference to the post (shortcode), the actual Text, and a Text Type column.\nimport pandas as pd\n\ndf = pd.read_csv('/content/drive/MyDrive/2023-11-30-Export-Posts-Crowd-Tangle.csv')\nNext, we want to transform the DataFrame from one post per row, to one text document per row (Think tidydata!)\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nAccount\nUser Name\nFollowers at Posting\nPost Created\nPost Created Date\nPost Created Time\nType\nTotal Interactions\nLikes\n...\nPhoto\nTitle\nDescription\nImage Text\nSponsor Id\nSponsor Name\nOverperforming Score (weighted ‚Äî Likes 1x Comments 1x )\nshortcode\nimage_file\nocr_text\n\n\n\n\n0\n0\nFREIE WAÃàHLER Bayern\nfw_bayern\n9138\n2023-10-09 20:10:19 CEST\n2023-10-09\n20:10:19\nPhoto\n566\n561\n...\nhttps://scontent-sea1-1.cdninstagram.com/v/t51...\nNaN\n#Landtagswahl23 ü§©üß°üôè #FREIEW√ÑHLER #Aiwanger #Da...\nFREIE WAHLER 15,8 %\nNaN\nNaN\n2.95\nCyMAe_tufcR\nmedia/images/fw_bayern/CyMAe_tufcR.jpg\nFREIE WAHLER 15,8 %\n\n\n1\n1\nJunge Liberale JuLis Bayern\njulisbayern\n4902\n2023-10-09 19:48:02 CEST\n2023-10-09\n19:48:02\nAlbum\n320\n310\n...\nhttps://scontent-sea1-1.cdninstagram.com/v/t51...\nNaN\nDie Landtagswahl war f√ºr uns als Liberale hart...\nNaN\nNaN\nNaN\n1.41\nCyL975vouHU\nmedia/images/julisbayern/CyL975vouHU.jpg\nFreie EDP Demokraten BDB FDP FB FDP DANKE F√úR ...\n\n\n2\n2\nJunge Union Deutschlands\njunge_union\n44414\n2023-10-09 19:31:59 CEST\n2023-10-09\n19:31:59\nPhoto\n929\n925\n...\nhttps://scontent-sea1-1.cdninstagram.com/v/t39...\nNaN\nNach einem starken Wahlkampf ein verdientes Er...\nHERZLICHEN GL√úCKWUNSCH! Unsere JUler im bayris...\nNaN\nNaN\n1.17\nCyL8GWWJmci\nmedia/images/junge_union/CyL8GWWJmci.jpg\nHERZLICHEN GL√úCKWUNSCH! Unsere JUler im bayris...\n\n\n3\n3\nKatharina Schulze\nkathaschulze\n37161\n2023-10-09 19:29:02 CEST\n2023-10-09\n19:29:02\nPhoto\n1,074\n1009\n...\nhttps://scontent-sea1-1.cdninstagram.com/v/t51...\nNaN\nSo viele Menschen am Odeonsplatz heute mit ein...\nNaN\nNaN\nNaN\n1.61\nCyL7wyJtTV5\nmedia/images/kathaschulze/CyL7wyJtTV5.jpg\nJuo I W\n\n\n4\n4\nJunge Union Deutschlands\njunge_union\n44414\n2023-10-09 18:01:34 CEST\n2023-10-09\n18:01:34\nAlbum\n1,655\n1644\n...\nhttps://scontent-sea1-1.cdninstagram.com/v/t39...\nNaN\nHerzlichen Gl√ºckwunsch zu diesem grandiosen Wa...\nNaN\nNaN\nNaN\n2.34\nCyLxwHuvR4Y\nmedia/images/junge_union/CyLxwHuvR4Y.jpg\n12/12 der hessischen JU-Kandidaten ziehen in d...\n\n\n\n\n\n5 rows √ó 25 columns\nWe restructure df to focus on two key text-based columns: ‚ÄòDescription‚Äô and ‚Äòocr_text‚Äô. The goal is to create a streamlined DataFrame where each row corresponds to an individual text entry, either from the ‚ÄòDescription‚Äô or the ‚Äòocr_text‚Äô fields. To achieve this, we first split the original DataFrame into two separate DataFrames, one for each of these columns. We then rename these columns to ‚ÄòText‚Äô for uniformity. Additionally, we introduce a new column, ‚ÄòText Type‚Äô, to categorize each text entry as either ‚ÄòCaption‚Äô (originating from ‚ÄòDescription‚Äô) or ‚ÄòOCR‚Äô (originating from ‚Äòocr_text‚Äô). The ‚Äòshortcode‚Äô column is retained as a unique identifier for each entry. Finally, we concatenate these two DataFrames into a single DataFrame, ensuring a clean and organized structure. This restructured DataFrame facilitates easier analysis and processing of the text data, segregating it by source while maintaining a link to its original post via the ‚Äòshortcode‚Äô. The code also includes a step to remove any rows with empty or NaN values in the ‚ÄòText‚Äô column, ensuring data integrity and cleanliness.\nimport pandas as pd\n\n# Creating two separate dataframes\ndf_description = df[['shortcode', 'Description']].copy()\ndf_ocr_text = df[['shortcode', 'ocr_text']].copy()\n\n# Renaming columns\ndf_description.rename(columns={'Description': 'Text'}, inplace=True)\ndf_ocr_text.rename(columns={'ocr_text': 'Text'}, inplace=True)\n\n# Adding 'Text Type' column\ndf_description['Text Type'] = 'Caption'\ndf_ocr_text['Text Type'] = 'OCR'\n\n# Concatenating the dataframes\nnew_df = pd.concat([df_description, df_ocr_text])\n\n# Dropping any rows where 'Text' is NaN or empty\nnew_df.dropna(subset=['Text'], inplace=True)\nnew_df = new_df[new_df['Text'].str.strip() != '']\n\n# Resetting the index\nnew_df.reset_index(drop=True, inplace=True)\nnew_df.head()\n\n\n  \n    \n\n\n\n\n\n\nshortcode\nText\nText Type\n\n\n\n\n0\nCyMAe_tufcR\n#Landtagswahl23 ü§©üß°üôè #FREIEW√ÑHLER #Aiwanger #Da...\nCaption\n\n\n1\nCyL975vouHU\nDie Landtagswahl war f√ºr uns als Liberale hart...\nCaption\n\n\n2\nCyL8GWWJmci\nNach einem starken Wahlkampf ein verdientes Er...\nCaption\n\n\n3\nCyL7wyJtTV5\nSo viele Menschen am Odeonsplatz heute mit ein...\nCaption\n\n\n4\nCyLxwHuvR4Y\nHerzlichen Gl√ºckwunsch zu diesem grandiosen Wa...\nCaption"
  },
  {
    "objectID": "notebooks/2023_12_01_BERTopic.html#extracting-topics",
    "href": "notebooks/2023_12_01_BERTopic.html#extracting-topics",
    "title": "Import CrowdTangle Data",
    "section": "Extracting Topics",
    "text": "Extracting Topics\nAfter fitting our model, we can start by looking at the results. Typically, we look at the most frequent topics first as they best represent the collection of documents.\n\nfreq = topic_model.get_topic_info(); freq.head(5)\n\n\n  \n    \n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nRepresentative_Docs\n\n\n\n\n0\n-1\n860\n-1_bayern_csu_uhr_mehr\n[bayern, csu, uhr, mehr, menschen, m√ºnchen, te...\n[Wir gehen mit #herzstatthetze in den Wahlkamp...\n\n\n1\n0\n137\n0_w√§hlen_fdp_hessen_heute\n[w√§hlen, fdp, hessen, heute, stimme, stimmen, ...\n[Unser Ministerpr√§sident @markus.soeder steigt...\n\n\n2\n1\n104\n1_energie_co2_klimaschutz_habeck\n[energie, co2, klimaschutz, habeck, wasserstof...\n[Habeck t√§uscht √ñffentlichkeit mit Zensur: R√ºc...\n\n\n3\n2\n103\n2_zuwanderung_migration_grenzpolizei_migration...\n[zuwanderung, migration, grenzpolizei, migrati...\n[Wir sagen Ja zu #Hilfe und #Arbeitsmigration,...\n\n\n4\n3\n89\n3_uhr_starke mitte_bayerns starke_bayerns\n[uhr, starke mitte, bayerns starke, bayerns, b...\n[\"Deutschland-Pakt\" aus Scholz der Krise komme...\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n-1 refers to all outliers and should typically be ignored. Next, let‚Äôs take a look at a frequent topic that were generated:\n\nlen(freq)\n\n52\n\n\nWe have a total of 52 topics\n\ntopic_model.get_topic(0)  # Select the most frequent topic\n\n[('w√§hlen', 0.01628736425293884),\n ('fdp', 0.01626632927971954),\n ('hessen', 0.013634118460503969),\n ('heute', 0.013441948777152065),\n ('stimme', 0.011907460231710654),\n ('stimmen', 0.011505832701270827),\n ('landtagswahl', 0.011272934711858047),\n ('wahlkampf', 0.01059385752962746),\n ('sonntag', 0.01057520846171656),\n ('bayern', 0.010322807358750668)]\n\n\n\nVisualize Topics\nAfter having trained our BERTopic model, we can iteratively go through perhaps a hundred topic to get a good understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. Instead, we can visualize the topics that were generated in a way very similar to LDAvis:\n\ntopic_model.visualize_topics()\n\n\n\n\nVisualize Terms\nWe can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation. Insights can be gained from the relative c-TF-IDF scores between and within topics. Moreover, you can easily compare topic representations to each other.\n\ntopic_model.visualize_barchart(top_n_topics=15)\n\n\n\n\nTopic Reduction\nWe can also reduce the number of topics after having trained a BERTopic model. The advantage of doing so, is that you can decide the number of topics after knowing how many are actually created. It is difficult to predict before training your model how many topics that are in your documents and how many will be extracted. Instead, we can decide afterwards how many topics seems realistic:\n\ntopic_model.reduce_topics(docs, nr_topics=15)\n\n2023-12-01 08:53:07,148 - BERTopic - Topic reduction - Reducing number of topics\n2023-12-01 08:53:07,642 - BERTopic - Topic reduction - Reduced number of topics from 52 to 15\n\n\n&lt;bertopic._bertopic.BERTopic at 0x794041658ca0&gt;\n\n\n\n\nVisualize Terms After Reduction\n\ntopic_model.visualize_barchart(top_n_topics=15)\n\n\n\n\nSaving the model\nThe model and its internal settings can easily be saved. Note that the documents and embeddings will not be saved. However, UMAP and HDBSCAN will be saved.\n\n# Save model\ntopic_model.save(\"/content/drive/MyDrive/2023-12-01-LTW23-CrowdTangle-Posts-model\")\n\n2023-12-01 08:53:54,135 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same."
  },
  {
    "objectID": "notebooks/firebase-interface-notebook.html",
    "href": "notebooks/firebase-interface-notebook.html",
    "title": "Project Creation",
    "section": "",
    "text": "The following lines of code assume that the firebase Credential File has been downloaded from GRIPS and uploaded to Colab / your Jupyter project path. First of all install the necessary packages:\n\n!pip -q install firebase-admin\n\nNext, we connect to our firebase project. Please update the credentials_path variable with the path to your credentials file (see above).\n\nimport firebase_admin\nfrom firebase_admin import credentials, firestore\n\ncredentials_path = '/content/XXXX-adminsdk-YYYYYY.json' \n\ncred = credentials.Certificate(credentials_path)\nfirebase_admin.initialize_app(cred)\ndb = firestore.client()\n\n\nProject Creation\nPlease provide an alert_email and project_name to create a new project on the backend. The backend checks hourly when the last stories have been uploaded to a project. If no story has been uploaded for more than 12 hours, an email alert will be triggered.\nRun the cell to create the new project on the backend. When successfull, the project id and api key will be displayed.\n\nfrom IPython.display import display, Markdown\nimport pandas as pd\n\nalert_email = 'michael@achmann.me'\nproject_name = 'Forschungsseminar23 Test'\n\n# Create Project\nimport uuid\n\n# Generate a UUID for the document\nproject_id = str(uuid.uuid4())\napi_key = str(uuid.uuid4())\n\n# Your data\ndata = {\n    \"api_key\": api_key,\n    \"email\": alert_email,\n    \"name\": project_name\n}\n\n# Add a new document with a UUID as the document name (ID)\ndoc_ref = db.collection('projects').document(project_id)\ndoc_ref.set(data)\n\ndisplay(Markdown(\"### Project Created:\"))\ndisplay(Markdown(f\"**Project Name:** {project_name}\"))\ndisplay(Markdown(f\"**Alert Email:** {alert_email}\"))\ndisplay(Markdown(f\"**Project ID:** {project_id}\"))\ndisplay(Markdown(f\"**API-Key:** {api_key}\"))\n\n\nProject Created:\nProject Name: Forschungsseminar23 Test\nAlert Email: michael@achmann.me\nProject ID: 959466fe-4088-4099-a6b2-3cbe058889d3\nAPI-Key: 554fbce8-fb15-44f1-bb4d-54cdc57554f2\n\n\n\nConfigure the Plugin\nConfigure Zeeschuimer-F using the above information after creating a project. In order to access the settings of Firefox plugins click on the puzzle tile on the top right of the browser. Click on Zeeschuimer F and the settings open.\n\n\n\nScreenshot of Firefox with open extensions menu\n\n\nFill in the Firebase Project field with the project id and aFirebase API Key with the api key provided after running the Project Creation. The Firebase Endopint URL will be provided via GRIPS (unless you‚Äôve installed your own instance).\n\n\n\nScreenshot of the Settings for Zeeschuimer-F\n\n\n1) Turn the IG Stories Switch on, 2) restart your browser for the values to be loaded correctly. Once the browser has started again, you‚Äôre ready to collect you first stories! Open the Instagram website and open any story.\n\n\n\nScreenshot of the switch\n\n\nCheck the extension settings page to see whether it is collecting stories while browsing. The counter should increase with each story visit. The remote collection process can currently only be checked through the Firebase Interface notebook. Follow the next steps to download the collected data.\n\n\nProject Export\nThe following code downloads all stories in JSON format and saves it locally (i.e.¬†on your colab instance). Provide the PROJECT_ID variable and an export_path to download all stories.\n\nfrom tqdm.auto import tqdm\nimport os\nimport json\n\nPROJECT_ID = '959466fe-4088-4099-a6b2-3cbe058889d3'\nexport_path = '/content/export' \n\n\ndef fetch_stories(project_id):\n    stories_ref = db.collection('projects').document(project_id).collection('stories')\n    docs = stories_ref.stream()\n\n    stories = []\n    for doc in docs:\n        stories.append(doc.to_dict())\n\n    return stories\n\ndb = fetch_stories(PROJECT_ID)\n\nif not os.path.exists('export'):\n    os.makedirs('export')\n\n# Iterate over each element in the database\nfor element in tqdm(db, desc='Exporting elements'):\n    # Serialize the element to JSON\n    element_json = json.dumps(element, indent=4)\n\n    # Write to a file named {id}.json\n    with open(os.path.join('export', f\"{element['id']}.json\"), 'w') as f:\n        f.write(element_json)\n\n\n\n\n\n\nConvert to DataFrame\nNext, we convert the exported JSON files to a pandas DataFrame and save the table as CSV. Provide the df_export_path variable for the location where to save the exported CSV file.\n\n\n\n\n\n\nWork-In-Progress\n\n\n\nThe DataFrame in the current version has a different structure than the one we created when downloading Instagram Posts.. In order to compare stories with posts we will might want to use the same data structure.\n\n\n\n\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n\ndf_export_path = '/content/2022-11-09-Stories-Exported.csv' \n\ndef process_instagram_story(data):\n\n    # Extract relevant information\n    story_info = {\n        'ID': data.get(\"id\"),\n        'Time of Posting': datetime.utcfromtimestamp(data['taken_at']).strftime('%Y-%m-%d %H:%M:%S'),\n        'Type of Content': 'Video' if 'video_duration' in data else 'Image',\n        'video_url': None,\n        'image_url': None,\n        'Username': data['user']['username'],\n        'Video Length (s)': data.get('video_duration', None),\n        'Expiration': (datetime.utcfromtimestamp(data['taken_at']) + timedelta(hours=24)).strftime('%Y-%m-%d %H:%M:%S'),\n        'Caption': data.get('caption', None),\n        'Is Verified': data['user']['is_verified'],\n        'Stickers': data.get('story_bloks_stickers', []),\n        'Accessibility Caption': data.get('accessibility_caption', ''),\n        'Attribution URL': data.get('attribution_content_url', '')\n    }\n\n    return story_info\n\nrows = []\nfor element in db:\n  rows.append(process_instagram_story(element))\n\ndf = pd.DataFrame(rows)\ndf.to_csv(df_export_path)\nprint(f\"Successfully exported {len(df)} rows as CSV.\")\n\nSuccessfully exported 22 rows as CSV.\n\n\nNow let‚Äôs take a look at the structure of the exported data:\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nID\nTime of Posting\nType of Content\nvideo_url\nimage_url\nUsername\nVideo Length (s)\nExpiration\nCaption\nIs Verified\nStickers\nAccessibility Caption\nAttribution URL\n\n\n\n\n0\n3231585718932790545_1483455177\n2023-11-08 14:50:59\nImage\n&lt;NA&gt;\nhttps://storage.googleapis.com/zeeschuimer-fb-...\nrmf24.pl\nNaN\n2023-11-09 14:50:59\nNone\nFalse\n[]\nPhoto by Fakty RMF FM | Rozmowy | Podcasty on ...\n\n\n\n1\n3231585778860997221_1483455177\n2023-11-08 14:51:06\nImage\n&lt;NA&gt;\nhttps://storage.googleapis.com/zeeschuimer-fb-...\nrmf24.pl\nNaN\n2023-11-09 14:51:06\nNone\nFalse\n[]\nPhoto by Fakty RMF FM | Rozmowy | Podcasty on ...\n\n\n\n2\n3231750838597692854_1349651722\n2023-11-08 20:19:00\nVideo\nhttps://storage.googleapis.com/zeeschuimer-fb-...\n&lt;NA&gt;\ntagesschau\n13.300\n2023-11-09 20:19:00\nNone\nTrue\n[]\n\n\n\n\n3\n3231750989408058657_1349651722\n2023-11-08 20:19:18\nVideo\nhttps://storage.googleapis.com/zeeschuimer-fb-...\n&lt;NA&gt;\ntagesschau\n15.267\n2023-11-09 20:19:18\nNone\nTrue\n[]\n\n\n\n\n4\n3231751135118088390_1349651722\n2023-11-08 20:19:35\nVideo\nhttps://storage.googleapis.com/zeeschuimer-fb-...\n&lt;NA&gt;\ntagesschau\n17.000\n2023-11-09 20:19:35\nNone\nTrue\n[]\n\n\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nDownload Images and Videos\nAll videos and images for our Instagram stories have been downloaded by our firebase backend. They are saved in a Cloud Bucket. The following cell helps with these two steps:\n\nCreate a signed link to each video and image\nDownload each file and saves it in the following structure: {media_export_path}/{image|video}/{username}/{ID.jpg|mp4}. It is important to keep a unique identifier (here ID) to map metadata and images for future data analysis.\n\nPlease provide a storage_bucket and media_export_path.\n\nstorage_bucket = \"XXXX.appspot.com\"  \nmedia_export_path =  '/content/media/'\n\nfrom firebase_admin import storage\nimport os\nimport requests\n\nbucket = storage.bucket(storage_bucket)\n\ndef generate_signed_url(username, content_id, file_type):\n    if file_type not in ['images', 'videos']:\n        raise ValueError(\"Invalid file type specified\")\n\n    ext = 'jpeg' if file_type == 'images' else 'mp4'\n    blob_path = f\"projects/{PROJECT_ID}/stories/{file_type}/{username}/{content_id}.{ext}\"\n    blob = bucket.blob(blob_path)\n    # Set the expiration of the link. Here, it's set to 24 hours.\n    return blob.generate_signed_url(expiration=timedelta(hours=24), method='GET')\n\n# Create a function to be applied across DataFrame rows\ndef apply_generate_signed_url(row):\n    image_url = generate_signed_url(row['Username'], row['ID'], 'images')\n    video_url = generate_signed_url(row['Username'], row['ID'], 'videos') if row['Type of Content'] == 'Video' else pd.NA\n    return pd.Series({'image_url': image_url, 'video_url': video_url})\n\n# Apply the function along the axis=1 (row-wise)\ndf[['image_url', 'video_url']] = df.apply(apply_generate_signed_url, axis=1)\n\n# Now, creating the lists for images and videos can be done more efficiently\ndata_images = df.loc[df['image_url'].notna(), ['ID', 'image_url', 'Username', 'Time of Posting']] \\\n               .rename(columns={'image_url': 'url', 'Time of Posting': 'datetime'}) \\\n               .to_dict('records')\n\ndata_videos = df.loc[df['video_url'].notna(), ['ID', 'video_url', 'Username', 'Time of Posting']] \\\n               .rename(columns={'video_url': 'url', 'Time of Posting': 'datetime'}) \\\n               .to_dict('records')\n\n\ndef create_directories(base_path, entries, subdir):\n    usernames = set(entry['Username'] for entry in entries)\n    for username in usernames:\n        os.makedirs(os.path.join(base_path, subdir, username), exist_ok=True)\n\ndef download_file(entry, media_type, media_export_path, session):\n    directory = os.path.join(media_export_path, media_type, entry['Username'])\n    ext = 'jpg' if media_type == 'images' else 'mp4'\n    filename = os.path.join(directory, f\"{entry['ID']}.{ext}\")\n\n    with session.get(entry['url'], stream=True) as response:\n        if response.status_code == 200:\n            with open(filename, 'wb') as file:\n                for chunk in response.iter_content(8192):\n                    file.write(chunk)\n        else:\n            print(f\"Failed to download {entry['url']}. Status code: {response.status_code}\")\n\nsession = requests.Session()\n# Pre-create directories\ncreate_directories(media_export_path, data_images, 'images')\ncreate_directories(media_export_path, data_videos, 'videos')\n\n# Download images\nfor entry in tqdm(data_images, desc=\"Downloading Images\", unit=\"file\"):\n    download_file(entry, 'images', media_export_path, session)\n\n# Download videos\nfor entry in tqdm(data_videos, desc=\"Downloading Videos\", unit=\"file\"):\n    download_file(entry, 'videos', media_export_path, session)\n\nprint(\"Download complete!\")\n\n\n\n\n\n\n\nDownload complete!\n\n\n\n\nPrepare Downloadable ZIP\nRun the following to ZIP all files. Optionally copy them to Google Drive.\n\n!zip -r 2023-11-09-Story-Media-Export.zip media/*\n\n\n!cp 2023-11-09-Story-Media-Export.zip /content/drive/MyDrive/"
  },
  {
    "objectID": "notebooks/literature-review.html",
    "href": "notebooks/literature-review.html",
    "title": "GPT Literature Review Assistant",
    "section": "",
    "text": "This notebook demonstrates how to work with the GPT-API based on a simple use case. First, we are going to import search results of our literature review with Publish or Perish. Next, we are going to explore how we could use python in combination with Publish or Perish to speed up our review process: We will manually code the relevance, using the Jupyter notebook as our labelling interface. Afterwards we are going to add a GPT-API call to extract features from the abstract, the first step towards our assistant guiding our literature review process.\nOverall, this notebooks is a simple implementation demonstrating how prompts work and how easy it is to use GPT in Jupyter notebooks. The notebook is available in the supplement repository, you can clone the notebook to your Colab account with one click."
  },
  {
    "objectID": "notebooks/literature-review.html#setup",
    "href": "notebooks/literature-review.html#setup",
    "title": "GPT Literature Review Assistant",
    "section": "Setup",
    "text": "Setup\nAt first we need to install necessary packages. Hit run and wait.\n\nprint(\"Install Packages\")\n!pip install -q openai crossref-commons"
  },
  {
    "objectID": "notebooks/literature-review.html#import-publish-or-perish-data.",
    "href": "notebooks/literature-review.html#import-publish-or-perish-data.",
    "title": "GPT Literature Review Assistant",
    "section": "Import Publish or Perish Data.",
    "text": "Import Publish or Perish Data.\nIf this is the start of your review process, upload the csv file exported from Publish or Perish in the left-hand Files pane. Enter the filename in publish_or_perish_file_name. Define the output name in file_name. If you want to save the imported file in the google drive add /content/drive/MyDrive/ to the path.  Skip this cell if you want to work with a file that has been imported in the past.\n\n\n\n\n\n\nWarning\n\n\n\nWe delete rows with missing DOIs. Without a DOI our code cannot retrieve abstracts. When importing the Publish or Perish file, the following code will display the number of rows that have been deleted due to missing DOIs. When using this notebook for real-world projects, you should be aware of the missing rows and manually review them!\n\n\n\n#@title Import from Publish or Perish Data.\n#@markdown If this is the start of your review process, upload the `csv` file exported from [Publish or Perish](https://harzing.com/resources/publish-or-perish) in the left-hand *Files* pane. Enter the filename in `publish_or_perish_file_name`. Define the output name in `file_name`. If you want to save the imported file in the google drive add `/content/drive/MyDrive/` to the path. &lt;br/&gt; **Skip this cell if you want to work with a file that has been imported in the past.**\n\nimport pandas as pd\nimport numpy as np\nimport io\n\npublish_or_perish_file_name = \"scholar.csv\" # @param {type: \"string\"}\nfile_name = \"2023-10-31-Literature-Review.csv\" # @param {type: \"string\"}\n\n# Initialize empty DataFrame\nall_data = pd.DataFrame()\n\n\ntry:\n    all_data = pd.read_csv(publish_or_perish_file_name)\n\n    # Remove Duplicates\n    initial_len = len(all_data)\n    all_data = all_data.drop_duplicates(subset='DOI', keep='first')\n    removed_len = initial_len - len(all_data)\n    print(f'Removed {removed_len} duplicates based on DOI.')\n\n    # Remove missing DOIs\n    initial_len = len(all_data)\n    all_data = all_data[~pd.isna(all_data['DOI'])]\n    removed_len = initial_len - len(all_data)\n    print(f'Removed {removed_len} rows without DOI.')\n\n    all_data = all_data.sort_values(by='Cites', ascending=False).reset_index(drop=True)\n\n    print('Sorted Table by Cites.')\n\n    # Create empty columns for Literature Review\n    all_data[\"Relevant\"] = \"\"\n    all_data[\"Notes\"] = \"\"\n    all_data[\"Checked\"] = False\n\n    print('Initialized Columns')\n\n    all_data.to_csv(file_name)\n    print(f\"Success: Saved data to {file_name}\")\n\n    print(f'Success: Data loaded from File \"{file_name}\".')\nexcept Exception as e:\n    print(f\"Error: Failed to load data from File. {str(e)}\")\n\nRemoved 172 duplicates based on DOI.\nRemoved 1 rows without DOI.\nSorted Table by Cites.\nInitializes Columns\nSuccess: Saved data to 2023-10-31-Literature-Review.csv\nSuccess: Data loaded from File \"2023-10-31-Literature-Review.csv\"."
  },
  {
    "objectID": "notebooks/literature-review.html#read-previously-imported-file",
    "href": "notebooks/literature-review.html#read-previously-imported-file",
    "title": "GPT Literature Review Assistant",
    "section": "Read previously imported File",
    "text": "Read previously imported File\nIf you want to keep going with a former review process, we can read an uploaded file / a file from google drive. Only run one cell, this one or the above.\n\n#@title Read previously imported File\n#@markdown If you want to keep going with a former review process, we can read an uploaded file / a file from google drive. **Only run one cell, this one or the above.**\nimport pandas as pd\nimport numpy as np\nimport io\n\nfile_name = \"2023-10-31-Literature-Review.csv\" # @param {type: \"string\"}\n\ntry:\n    all_data = pd.read_csv(file_name)\n\n    print(f'Success: Data loaded from File \"{file_name}\".')\nexcept Exception as e:\n    print(f\"Error: Failed to load data from File. {str(e)}\")\n\nSuccess: Data loaded from File \"2023-10-31-Literature-Review.csv\".\n\n\n\nIn this example we‚Äôve saved the file locally. When working with Colab, the file will be deleted when we disconnect. For colab you should link your google drive (open the files pane on the left, click the Google Drive button). Once connected, save the file in the folder /content/drive/MyDrive/YOUR-FILENAME.csv. It will be accessible through Drive, and Colab is from now on going to connect automatically to drive.\nCheck the imported data. We‚Äôre using pandas, the imported data is saved in the all_datavariable. head(2)displays the two top rows of the table. Additionally, we have added three columns: Relevant, Notes, and Checked. We are going to make use of them to keep track of our progress.\n\n# Check the structure (and content) of the file\nall_data.head(2)\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0.2\nUnnamed: 0.1\nUnnamed: 0\nCites\nAuthors\nTitle\nYear\nSource\nPublisher\nArticleURL\n...\nAge\nAbstract\nFullTextURL\nRelatedURL\nbabbage_similarity\nbabbage_search\nsimilarities\nRelevant\nNotes\nChecked\n\n\n\n\n0\n0\n746\n844\n21\nFlorian Arendt\nSuicide on Instagram ‚Äì Content Analysis of a G...\n2019.0\nCrisis\nHogrefe Publishing Group\nhttp://dx.doi.org/10.1027/0227-5910/a000529\n...\n3.0\nAbstract. Background: Suicide is the second le...\nhttps://econtent.hogrefe.com/doi/pdf/10.1027/0...\nNaN\n[-0.0018475924152880907, 0.022463073953986168,...\n[-0.014954154379665852, 0.026176564395427704, ...\n-1\nNaN\nNaN\nFalse\n\n\n1\n1\n770\n868\n4\nPaloma de H. S√°nchez-Cobarro, Francisco-Jose M...\nThe Brand-Generated Content Interaction of Ins...\n2020.0\nJournal of Theoretical and Applied Electronic ...\nMDPI AG\nhttp://dx.doi.org/10.3390/jtaer16030031\n...\n2.0\nThe last decade has seen a considerable increa...\nhttps://www.mdpi.com/0718-1876/16/3/31/pdf\nNaN\n[-0.0029447057750076056, 0.01190990675240755, ...\n[-0.01012819167226553, 0.02539714053273201, -0...\n-1\nNaN\nNaN\nFalse\n\n\n\n\n\n2 rows √ó 35 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nIn the next step we are going to start our literature review:\n\nWe filter for the first unchecked row, ordered by the cite count.\nWe retrieve the abstract from CrossRef API using the DOI.\nWe display all information\nWe answer whether the paper appear to be relevant by entering y or n for yes or no.\n\nFor our session, the cell only runs through one row and finishes afterwards. For a real world application you‚Äôd probably like to add some kind of loop.\n\nfrom crossref_commons.retrieval import get_publication_as_json\nimport json\nimport openai\nimport textwrap\nimport IPython\nimport re\n\n# Get one row: Not checked, highest Citation count.\nhighest_cites_unchecked = all_data[all_data['Checked'] == False].sort_values(by=\"Cites\", ascending=False).iloc[0]\nindex = highest_cites_unchecked.name\n\n# Retrieve Abstract from Crossref\nresponse = get_publication_as_json(highest_cites_unchecked['DOI'])\nabstract = response.get(\"abstract\", \"\")\n\n# Remove XML\nabstract = re.sub(r'&lt;[^&gt;]+&gt;', '', abstract)\nall_data.loc[index, 'Abstract'] = abstract\n\n\n# Display all information\nIPython.display.clear_output(wait=True)\ntitle_disp = IPython.display.HTML(\"&lt;h2&gt;{}&lt;/h2&gt;\".format(highest_cites_unchecked['Title']))\nauthors_disp = IPython.display.HTML(\"&lt;p&gt;{}&lt;/p&gt;\".format(highest_cites_unchecked['Authors']))\ndoi_disp = IPython.display.HTML(\"&lt;p&gt;&lt;a target='_blank' href='https://doi.org/{}'&gt;{}&lt;/a&gt;&lt;/p&gt;\".format(highest_cites_unchecked['DOI'],highest_cites_unchecked['DOI']))\ndisplay(title_disp, authors_disp, doi_disp)\nprint(textwrap.fill(abstract, 80))\nrelevant_input = input('Relevant? (y/n): ').lower().strip() == 'y'\n\n# Save user input\nall_data.loc[index, 'Checked'] = True\nall_data.loc[index, 'Relevant'] = relevant_input\n\nSuicide on Instagram ‚Äì Content Analysis of a German Suicide-Related Hashtag\n\n\nFlorian Arendt\n\n\n10.1027/0227-5910/a000529\n\n\nAbstract. Background: Suicide is the second leading cause of death among\n15‚Äì29-year-olds globally. Unfortunately, the suicide-related content on\nInstagram, a popular social media platform for youth, has not received the\nscholarly attention it deserves. Method: The present study provides a content\nanalysis of posts tagged as #selbstmord, a German suicide-related hashtag. These\nposts were created between July 5 and July¬†11, 2017. Results: Approximately half\nof all posts included words or visuals related to suicide. Cutting was by far\nthe most prominent method. Although sadness was the dominant emotion, self-hate\nand loneliness also appeared regularly. Importantly, inconsistency ‚Äì a gap\nbetween one's inner mental state (e.g., sadness) and one's overtly expressed\nbehavior (e.g., smiling) ‚Äì was also a recurring theme. Conversely, help-seeking,\ndeath wishes, and professional awareness‚Äìintervention material were very rare.\nAn explorative analysis revealed that some videos relied on very fast cutting\ntechniques. We provide tentative evidence that users may be exposed to\npurposefully inserted suicide-related subliminal messages (i.e., exposure to\ncontent without the user's conscious awareness). Limitations: We only\ninvestigated the content of posts on one German hashtag, and the sample size was\nrather small. Conclusion: Suicide prevention organizations may consider posting\nmore awareness‚Äìintervention materials. Future research should investigate\nsuicide-related subliminal messages in social media video posts. Although\ntentative, this finding should raise a warning flag for suicide prevention\nscholars.\nRelevant? (y/n): y\n\n\nNext, we check whether our input has been saved:\n\n# Check the result\nall_data.iloc[index]\n\nUnnamed: 0.2                                                          0\nUnnamed: 0.1                                                        746\nUnnamed: 0                                                          844\nCites                                                                21\nAuthors                                                  Florian Arendt\nTitle                 Suicide on Instagram ‚Äì Content Analysis of a G...\nYear                                                             2019.0\nSource                                                           Crisis\nPublisher                                      Hogrefe Publishing Group\nArticleURL                  http://dx.doi.org/10.1027/0227-5910/a000529\nCitesURL                                                            NaN\nGSRank                                                               26\nQueryDate                                           2022-09-08 10:44:44\nType                                                    journal-article\nDOI                                           10.1027/0227-5910/a000529\nISSN                                                          0227-5910\nCitationURL                                                         NaN\nVolume                                                             40.0\nIssue                                                               1.0\nStartPage                                                          36.0\nEndPage                                                            41.0\nECC                                                                  21\nCitesPerYear                                                        7.0\nCitesPerAuthor                                                       21\nAuthorCount                                                           1\nAge                                                                 3.0\nAbstract              Abstract. Background: Suicide is the second le...\nFullTextURL           https://econtent.hogrefe.com/doi/pdf/10.1027/0...\nRelatedURL                                                          NaN\nbabbage_similarity    [-0.0018475924152880907, 0.022463073953986168,...\nbabbage_search        [-0.014954154379665852, 0.026176564395427704, ...\nsimilarities                                                         -1\nRelevant                                                           True\nNotes                                                               NaN\nChecked                                                            True\nName: 0, dtype: object"
  },
  {
    "objectID": "notebooks/literature-review.html#using-gpt-to-extract-information-from-abstracts",
    "href": "notebooks/literature-review.html#using-gpt-to-extract-information-from-abstracts",
    "title": "GPT Literature Review Assistant",
    "section": "Using GPT to extract information from abstracts",
    "text": "Using GPT to extract information from abstracts\nNow for the fun part: Is it possible to use GPT to help us during the review process? We are going to try and extract text features automatically. For the moment we are going to use gpt3.5-turbo.\nNote: Please feel free to test different prompts and questions. The Promptingguide is a good resource to learn more about different prompting techniques. Use the ChatGPT interface to cheaply test prompts prior to using them with the API. Use the OpenAI Playground to optimize your prompts with a visual user interface for different settings and a prompting history (trust me, this can save your life!).\nPrompts: We‚Äôre going to use the system prompt for our instructions, and the user prompt to send our content.\n\n\n\n\n\n\nCaution\n\n\n\nA word of warning: You should not trust the quality of the GPT output at this stage. The prompt has not been evaluated, overall LLMs produce output that appears meaningful most of the times. Sometimes, however, it is Hallucinations. Thus, before using prompts and LLMs for production, we have to make sure we can trust their outputs. We will dive deeper into this topic in the classification sessions.\n\n\n\nsystem_prompt = \"\"\"\nYou're an advanced AI research assistant. Your task is to extract **research questions**, **operationalization**, **data sources**, **population**, and **scientific disciplines** from user input. Return \"None\" if you can't find the information in user input.\n\n**Formatting**\nReturn a markdown table, one row for each extracted feature: **research questions**, **operationalization**, **data sources**, **population**, and **scientific disciplines**.\n\"\"\"\n\nPlease enter your API-Code in the next code cell for the openai.api_key variable. We have changed the cell to include the gpt_prompt variable, which sends the title and abstract as a user prompt. We‚Äôre using the openai.ChatCompletion.create() method to send our request to the API. We expect the response in api_response['choices'][0]['message']['content'] to be markdown (see prompt above), as such we display the markdown in our notebook.\n\nfrom crossref_commons.retrieval import get_publication_as_json\nimport json\nimport openai\nimport textwrap\nimport IPython\nimport re\n\n# Enter OpenAI API-Code\nopenai.api_key = \"sk-XXXXXXXXX\"\n\n# Get one row: Not checked, highest Citation count.\nhighest_cites_unchecked = all_data[all_data['Checked'] == False].sort_values(by=\"Cites\", ascending=False).iloc[0]\nindex = highest_cites_unchecked.name\n\n# Retrieve Abstract from Crossref\nresponse = get_publication_as_json(highest_cites_unchecked['DOI'])\nabstract = response.get(\"abstract\", \"\")\n\n# Remove XML\nabstract = re.sub(r'&lt;[^&gt;]+&gt;', '', abstract)\n\nall_data.loc[index, 'Abstract'] = abstract\n\n# Display all information (before we send the request to OpenAI)\nIPython.display.clear_output(wait=True)\ntitle_disp = IPython.display.HTML(\"&lt;h2&gt;{}&lt;/h2&gt;\".format(highest_cites_unchecked['Title']))\nauthors_disp = IPython.display.HTML(\"&lt;p&gt;{}&lt;/p&gt;\".format(highest_cites_unchecked['Authors']))\ndoi_disp = IPython.display.HTML(\"&lt;p&gt;&lt;a target='_blank' href='https://doi.org/{}'&gt;{}&lt;/a&gt;&lt;/p&gt;\".format(highest_cites_unchecked['DOI'],highest_cites_unchecked['DOI']))\ndisplay(title_disp, authors_disp, doi_disp)\nprint(textwrap.fill(abstract, 80))\n\ngpt_prompt = f\"\"\"\n**Title**: {highest_cites_unchecked['Title']}\n**Abstract**: {abstract}\n\"\"\"\n\n# Sending request, takes a moment. In the meantime you may read the abstract.\nmessages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": abstract}\n]\n\ntry:\n  api_response = openai.ChatCompletion.create(\n      model=\"gpt-3.5-turbo\",\n      messages=messages,\n      temperature=0,\n      timeout=30\n    )\n\n  gpt_result = api_response['choices'][0]['message']['content']\n\n  # Display the GPT result\n  display(IPython.display.HTML(f\"&lt;h3&gt;GPT Extracted Data&lt;/h3&gt;\"))\n  display(IPython.display.Markdown(gpt_result))\nexcept:\n  print(\"GPT API Error\")\n\nrelevant_input = input('Relevant? (y/n): ').lower().strip() == 'y'\n\n# Save user input\nall_data.loc[index, 'Checked'] = True\nall_data.loc[index, 'Relevant'] = relevant_input\n\nThe Brand-Generated Content Interaction of Instagram Stories and Publications: A Comparison between Retailers and Manufacturers\n\n\nPaloma de H. S√°nchez-Cobarro, Francisco-Jose Molina-Castillo, Cristina Alcazar-Caceres\n\n\n10.3390/jtaer16030031\n\n\nThe last decade has seen a considerable increase in entertainment-oriented\ncommunication techniques. Likewise, the rise of social networks has evolved,\noffering different formats such as publication and stories. Hence, there has\nbeen a growing interest in knowing which strategies have the greatest social\nimpact to help position organizations in the mind of the consumer. This research\naims to analyze the different impact that stories and publications can have on\nthe Instagram social network as a tool for generating branded content. To this\nend, it analyses the impact of the different Instagram stories and publications\nin various sectors using a methodology of structural equations with composite\nconstructs. The results obtained, based on 800 stories and publications in four\ndifferent companies (retailers and manufacturers), show that the reach of the\nstory generally explains the interaction with Instagram stories. In contrast, in\nthe case of publications, impressions are of greater importance in explaining\nthe interaction with the publication. Among the main contributions of the work,\nwe find that traditional pull communication techniques have been losing\neffectiveness in front of new formats of brand content generation that have been\noccupying the time in the relationship between users and brands.\nRelevant? (y/n): y\n\n\nGPT Extracted Data\n\n\n\n\n\n\n\n\n\nFeature\nValue\n\n\n\n\nResearch questions\n- What strategies have the greatest social impact on Instagram?- How do stories and publications on Instagram impact the consumer‚Äôs perception of brands?- What is the relationship between reach and interaction with Instagram stories?- What is the relationship between impressions and interaction with Instagram publications?\n\n\nOperationalization\n- Analyzing the impact of Instagram stories and publications in various sectors- Using a methodology of structural equations with composite constructs\n\n\nData sources\n- 800 stories and publications on Instagram\n\n\nPopulation\n- Four different companies (retailers and manufacturers)\n\n\nScientific disciplines\n- Marketing- Communication\n\n\n\n\n\n\nThe above output shows a formatted table listing all extracted features. In this short warm-up session on GPT we have seen one use case of the LLM: The extraction of text feautures. In future sessions we are going to dive deeper into this topic.\n\n\n\n\n\n\nNote\n\n\n\nDid you create an excellent prompt? Share it with us! Enter your prompt into this Excel Sheet"
  },
  {
    "objectID": "notebooks/literature-review.html#save-your-progress",
    "href": "notebooks/literature-review.html#save-your-progress",
    "title": "GPT Literature Review Assistant",
    "section": "Save your Progress",
    "text": "Save your Progress\nThe following line saves all progress to file_name. If file_name is a path to Google Drive you will be able to pick up your work later on.\n\nall_data.to_csv(file_name)"
  },
  {
    "objectID": "notebooks/ocr-notebook.html",
    "href": "notebooks/ocr-notebook.html",
    "title": "Social Media Lab",
    "section": "",
    "text": "We‚Äôre using easyocr. See the documentation for more complex configurations. Using CPU only this process takes from minutes to hours (depends on the amount of images). OCR may also be outsourced (e.g.¬†using Google Vision API), see future sessions (and Memespector) for this.\n\n!pip -q install easyocr\n\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.9/2.9 MB 29.7 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 908.3/908.3 kB 57.5 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 307.2/307.2 kB 29.6 MB/s eta 0:00:00\n\n\n\n# Imports for OCR\nimport easyocr\nreader = easyocr.Reader(['de','en'])\n\nWARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\nWARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\nWARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n\n\nProgress: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% CompleteProgress: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% Complete\n\n\nWe define a very simple method to receive one string for all text recognized: The readtextmethod returns a list of text areas, in this example we concatenate the string, therefore the order of words is sometimes not correct.\nAlso, we save the file to Google Drive to save our results.\n\ndef run_ocr(image_path):\n    ocr_result = reader.readtext(image_path, detail = 0)\n    ocr_text = \" \".join(ocr_result)\n    return ocr_text\n\ndf['ocr_text'] = df['image_file'].apply(run_ocr)\n\n# Saving Results to Drive\ndf.to_csv('/content/drive/MyDrive/2022-11-09-Stories-Exported.csv')\n\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0.1\nUnnamed: 0\nID\nTime of Posting\nType of Content\nvideo_url\nimage_url\nUsername\nVideo Length (s)\nExpiration\n...\nIs Verified\nStickers\nAccessibility Caption\nAttribution URL\nvideo_file\naudio_file\nduration\nsampling_rate\nimage_file\nocr_text\n\n\n\n\n0\n0\n0\n3234500408402516260_1383567706\n2023-11-12 15:21:53\nImage\nNaN\nNaN\nnews24\nNaN\n2023-11-13 15:21:53\n...\nTrue\n[]\nPhoto by News24 on November 12, 2023. May be a...\nhttps://www.threads.net/t/CzjB80Zqme0\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234500408402516260_1383567706.jpg\nKeee WEEKEND NEWS24 PLUS: TESTING FORDS RANGER...\n\n\n1\n1\n1\n3234502795095897337_8537434\n2023-11-12 15:26:39\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:26:39\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234502795095897337_8537434.jpg\nDieses Auto ist einfach der Horror Du glaubst ...\n\n\n2\n2\n2\n3234503046678453705_8537434\n2023-11-12 15:27:10\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:27:10\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234503046678453705_8537434.jpg\nTouchdown bei Taylor Swift und Travis Kelce De...\n\n\n3\n3\n3\n3234503930728728807_8537434\n2023-11-12 15:28:55\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:28:55\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234503930728728807_8537434.jpg\nHorror-Diagnose f√ºr Barton Cowperthwaite Netfl...\n\n\n4\n4\n4\n3234504185910204562_8537434\n2023-11-12 15:29:25\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:29:25\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234504185910204562_8537434.jpg\n3v Bilde GG JJ Besorgniserregende Ufo-Aktivit√§...\n\n\n\n\n\n5 rows √ó 21 columns"
  },
  {
    "objectID": "notebooks/2023_12_11_GPT_Text_Classification.html",
    "href": "notebooks/2023_12_11_GPT_Text_Classification.html",
    "title": "GPT Text Classification",
    "section": "",
    "text": "Let‚Äôs read last week‚Äôs Text DataFrame\n\nimport pandas as pd\n\ndf = pd.read_csv('/content/drive/MyDrive/2023-12-01-Export-Posts-Text-Master.csv')\n\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nshortcode\nText\nText Type\nPolicy Issues\n\n\n\n\n0\n0\nCyMAe_tufcR\n#Landtagswahl23 ü§©üß°üôè #FREIEW√ÑHLER #Aiwanger #Da...\nCaption\n['1. Political parties:\\n- FREIEW√ÑHLER\\n- Aiwa...\n\n\n1\n1\nCyL975vouHU\nDie Landtagswahl war f√ºr uns als Liberale hart...\nCaption\n['Landtagswahl']\n\n\n2\n2\nCyL8GWWJmci\nNach einem starken Wahlkampf ein verdientes Er...\nCaption\n['1. Wahlkampf und Wahlergebnis:\\n- Wahlkampf\\...\n\n\n3\n3\nCyL7wyJtTV5\nSo viele Menschen am Odeonsplatz heute mit ein...\nCaption\n['Israel', 'Terrorismus', 'Hamas', 'Entwicklun...\n\n\n4\n4\nCyLxwHuvR4Y\nHerzlichen Gl√ºckwunsch zu diesem grandiosen Wa...\nCaption\n['1. Wahlsieg und Parlamentseinstieg\\n- Wahlsi...\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nSetup for GPT\n\n!pip install -q openai backoff gpt-cost-estimator\n\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 221.4/221.4 kB 3.2 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75.0/75.0 kB 7.9 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.0/2.0 MB 12.1 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 76.9/76.9 kB 7.8 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 58.3/58.3 kB 6.2 MB/s eta 0:00:00\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nllmx 0.0.15a0 requires cohere, which is not installed.\n\n\nWe‚Äôre using the new Colab Feature to store keys safely within the Colab Environment. Click on the key on the left to add your API key and enable it for this notebook. Enter the name of your API-Key in the api_key_name variable.\n\nimport openai\nfrom openai import OpenAI\nfrom google.colab import userdata\nimport backoff\nfrom gpt_cost_estimator import CostEstimator\n\napi_key_name = \"openai-lehrstuhl-api\"\napi_key = userdata.get(api_key_name)\n\n# Initialize OpenAI using the key\nclient = OpenAI(\n    api_key=api_key\n)\n\n@CostEstimator()\ndef query_openai(model, temperature, messages, mock=True, completion_tokens=10):\n    return client.chat.completions.create(\n                      model=model,\n                      temperature=temperature,\n                      messages=messages,\n                      max_tokens=600)\n\n# We define the run_request method to wrap it with the @backoff decorator\n@backoff.on_exception(backoff.expo, (openai.RateLimitError, openai.APIError))\ndef run_request(system_prompt, user_prompt, model, mock):\n  messages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": user_prompt}\n  ]\n\n  return query_openai(\n          model=model,\n          temperature=0.0,\n          messages=messages,\n          mock=mock\n        )\n\nNext, we create a system prompt describing what we want to classify. For further examples of prompts and advice on prompt engineering see e.g.¬†the prompting guide and further resources linked at the bottom of the page.\nFor the moment we are going to use the prompt from the literature.\nDo not forget the Prompt Archive when experimenting. Share your successfull prompt with us!\n\nsystem_prompt = \"\"\"\nYou are an advanced classifying AI. Your task is to classify the sentiment of a text. Sentiment can be either ‚Äòpositive‚Äô, ‚Äònegative‚Äô, or ‚Äòneutral‚Äô.\n\"\"\"\n\n\nprompt = \"\"\"\nPlease classify the following social media comment into either ‚Äònegative‚Äô, ‚Äòneutral‚Äô, or ‚Äòpositive‚Äô. Your answer MUST be one of [‚Äònegative‚Äô, ‚Äòneutral‚Äô, ‚Äòpositive‚Äô], and it should be presented in lowercase.\nText: [TEXT]\n\"\"\"\n\n\n\nRunning the request.\nThe following code snippet uses my gpt-cost-estimator package to simulate API requests and calculate a cost estimate. Please run the estimation whne possible to asses the price-tag before sending requests to OpenAI! Make sure run_request and system_prompt (see Setup for GPT) are defined before this block by running the two blocks above!\nFill in the MOCK, RESET_COST, COLUMN, SAMPLE_SIZE, and MODEL variables as needed (see comments above each variable.)\n\nfrom tqdm.auto import tqdm\n\n#@markdown Do you want to mock the OpenAI request (dry run) to calculate the estimated price?\nMOCK = False # @param {type: \"boolean\"}\n#@markdown Do you want to reset the cost estimation when running the query?\nRESET_COST = True # @param {type: \"boolean\"}\n#@markdown What's the column name to save the results of the data extraction task to?\nCOLUMN = 'Sentiment' # @param {type: \"string\"}\n#@markdown Do you want to run the request on a smaller sample of the whole data? (Useful for testing). Enter 0 to run on the whole dataset.\nSAMPLE_SIZE = 25 # @param {type: \"number\", min: 0}\n\n#@markdown Which model do you want to use?\nMODEL = \"gpt-3.5-turbo-0613\" # @param [\"gpt-3.5-turbo-0613\", \"gpt-4-1106-preview\", \"gpt-4-0613\"] {allow-input: true}\n\n\n# Initializing the empty column\nif COLUMN not in df.columns:\n  df[COLUMN] = None\n\n# Reset Estimates\nCostEstimator.reset()\nprint(\"Reset Cost Estimation\")\n\nfiltered_df = df.copy()\n\n# Skip previously annotated rows\nfiltered_df = filtered_df[pd.isna(filtered_df[COLUMN])]\n\nif SAMPLE_SIZE &gt; 0:\n  filtered_df = filtered_df.sample(SAMPLE_SIZE)\n\nfor index, row in tqdm(filtered_df.iterrows(), total=len(filtered_df)):\n    try:\n        p = prompt.replace('[TEXT]', row['Text'])\n        response = run_request(system_prompt, p, MODEL, MOCK)\n\n        if not MOCK:\n          # Extract the response content\n          # Adjust the following line according to the structure of the response\n          r = response.choices[0].message.content\n\n          # Update the 'new_df' DataFrame\n          df.at[index, COLUMN] = r\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        # Optionally, handle the error (e.g., by logging or by setting a default value)\n\nprint()\n\nReset Cost Estimation\nCost: $0.0002 | Total: $0.0069\n\n\n\n\n\n\ndf[~pd.isna(df['Sentiment'])].head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nshortcode\nText\nText Type\nPolicy Issues\nSentiment\n\n\n\n\n6\n6\nCyLt56wtNgV\nViele gemischte Gef√ºhle waren das gestern Aben...\nCaption\n['Demokratie']\nnegative\n\n\n27\n27\nCyKwo3Ft6tp\nSwipe dich r√ºckw√§rts durch die Kampagne ‚ú®\\n\\nü§Ø...\nCaption\n['Soziale Gerechtigkeit']\npositive\n\n\n29\n29\nCyKwBKcqi31\n#FREIEW√ÑHLER jetzt zweite Kraft in Bayern! Gro...\nCaption\n['St√§rkung der Demokratie', 'Sorgen der B√ºrger...\npositive\n\n\n66\n66\nCyIjC3QogWT\nIn einer gemeinsamen Erkl√§rung der Parteivorsi...\nCaption\n['Israel']\npositive\n\n\n212\n212\nCyAmHU7qlVc\n#FREIEW√ÑHLER #Aiwanger\nCaption\nNaN\nneutral\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# Save Results\ndf.to_csv('/content/drive/MyDrive/2023-12-01-Export-Posts-Text-Master.csv')\n\nLet‚Äôs plot the result for a first big picture\n\n\nimport matplotlib.pyplot as plt\n\n# Count the occurrences of each sentiment\nsentiment_counts = df['Sentiment'].value_counts()\n\n# Create a bar chart\nsentiment_counts.plot(kind='bar')\n\n# Adding labels and title\nplt.xlabel('Sentiment')\nplt.ylabel('Count')\nplt.title('Sentiment Counts')\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "evaluation/agreement.html",
    "href": "evaluation/agreement.html",
    "title": "Agreement & Evaluation",
    "section": "",
    "text": "Warning\n\n\n\nThis chapter will be available on January 8th 2024.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{achmann-denkler2024,\n  author = {Achmann-Denkler, Michael},\n  title = {Agreement \\& {Evaluation}},\n  date = {2024-01-08},\n  url = {https://social-media-lab.net/evaluation/agreement.html},\n  doi = {10.5281/zenodo.10039756},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAchmann-Denkler, Michael. 2024. ‚ÄúAgreement &\nEvaluation.‚Äù January 8, 2024. https://doi.org/10.5281/zenodo.10039756."
  },
  {
    "objectID": "getting-started/literature-review-assistant.html",
    "href": "getting-started/literature-review-assistant.html",
    "title": "GPT Literature Review Assistant",
    "section": "",
    "text": "This notebook demonstrates how to work with the GPT-API based on a simple use case. First, we are going to import search results of our literature review with Publish or Perish. Next, we are going to explore how we could use python in combination with Publish or Perish to speed up our review process: We will manually code the relevance, using the Jupyter notebook as our labelling interface. Afterwards we are going to add a GPT-API call to extract features from the abstract, the first step towards our assistant guiding our literature review process.\nOverall, this notebooks is a simple implementation demonstrating how prompts work and how easy it is to use GPT in Jupyter notebooks. The notebook is available in the supplement repository, you can clone the notebook to your Colab account with one click.\n\n\nAt first we need to install necessary packages. Hit run and wait.\n\nprint(\"Install Packages\")\n!pip install -q openai crossref-commons\n\n\n\n\nIf this is the start of your review process, upload the csv file exported from Publish or Perish in the left-hand Files pane. Enter the filename in publish_or_perish_file_name. Define the output name in file_name. If you want to save the imported file in the google drive add /content/drive/MyDrive/ to the path.  Skip this cell if you want to work with a file that has been imported in the past.\n\n\n\n\n\n\nWarning\n\n\n\nWe delete rows with missing DOIs. Without a DOI our code cannot retrieve abstracts. When importing the Publish or Perish file, the following code will display the number of rows that have been deleted due to missing DOIs. When using this notebook for real-world projects, you should be aware of the missing rows and manually review them!\n\n\n\n#@title Import from Publish or Perish Data.\n#@markdown If this is the start of your review process, upload the `csv` file exported from [Publish or Perish](https://harzing.com/resources/publish-or-perish) in the left-hand *Files* pane. Enter the filename in `publish_or_perish_file_name`. Define the output name in `file_name`. If you want to save the imported file in the google drive add `/content/drive/MyDrive/` to the path. &lt;br/&gt; **Skip this cell if you want to work with a file that has been imported in the past.**\n\nimport pandas as pd\nimport numpy as np\nimport io\n\npublish_or_perish_file_name = \"scholar.csv\" # @param {type: \"string\"}\nfile_name = \"2023-10-31-Literature-Review.csv\" # @param {type: \"string\"}\n\n# Initialize empty DataFrame\nall_data = pd.DataFrame()\n\n\ntry:\n    all_data = pd.read_csv(publish_or_perish_file_name)\n\n    # Remove Duplicates\n    initial_len = len(all_data)\n    all_data = all_data.drop_duplicates(subset='DOI', keep='first')\n    removed_len = initial_len - len(all_data)\n    print(f'Removed {removed_len} duplicates based on DOI.')\n\n    # Remove missing DOIs\n    initial_len = len(all_data)\n    all_data = all_data[~pd.isna(all_data['DOI'])]\n    removed_len = initial_len - len(all_data)\n    print(f'Removed {removed_len} rows without DOI.')\n\n    all_data = all_data.sort_values(by='Cites', ascending=False).reset_index(drop=True)\n\n    print('Sorted Table by Cites.')\n\n    # Create empty columns for Literature Review\n    all_data[\"Relevant\"] = \"\"\n    all_data[\"Notes\"] = \"\"\n    all_data[\"Checked\"] = False\n\n    print('Initialized Columns')\n\n    all_data.to_csv(file_name)\n    print(f\"Success: Saved data to {file_name}\")\n\n    print(f'Success: Data loaded from File \"{file_name}\".')\nexcept Exception as e:\n    print(f\"Error: Failed to load data from File. {str(e)}\")\n\nRemoved 172 duplicates based on DOI.\nRemoved 1 rows without DOI.\nSorted Table by Cites.\nInitializes Columns\nSuccess: Saved data to 2023-10-31-Literature-Review.csv\nSuccess: Data loaded from File \"2023-10-31-Literature-Review.csv\".\n\n\n\n\n\nIf you want to keep going with a former review process, we can read an uploaded file / a file from google drive. Only run one cell, this one or the above.\n\n#@title Read previously imported File\n#@markdown If you want to keep going with a former review process, we can read an uploaded file / a file from google drive. **Only run one cell, this one or the above.**\nimport pandas as pd\nimport numpy as np\nimport io\n\nfile_name = \"2023-10-31-Literature-Review.csv\" # @param {type: \"string\"}\n\ntry:\n    all_data = pd.read_csv(file_name)\n\n    print(f'Success: Data loaded from File \"{file_name}\".')\nexcept Exception as e:\n    print(f\"Error: Failed to load data from File. {str(e)}\")\n\nSuccess: Data loaded from File \"2023-10-31-Literature-Review.csv\".\n\n\n\nIn this example we‚Äôve saved the file locally. When working with Colab, the file will be deleted when we disconnect. For colab you should link your google drive (open the files pane on the left, click the Google Drive button). Once connected, save the file in the folder /content/drive/MyDrive/YOUR-FILENAME.csv. It will be accessible through Drive, and Colab is from now on going to connect automatically to drive.\nCheck the imported data. We‚Äôre using pandas, the imported data is saved in the all_datavariable. head(2)displays the two top rows of the table. Additionally, we have added three columns: Relevant, Notes, and Checked. We are going to make use of them to keep track of our progress.\n\n# Check the structure (and content) of the file\nall_data.head(2)\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0.2\nUnnamed: 0.1\nUnnamed: 0\nCites\nAuthors\nTitle\nYear\nSource\nPublisher\nArticleURL\n...\nAge\nAbstract\nFullTextURL\nRelatedURL\nbabbage_similarity\nbabbage_search\nsimilarities\nRelevant\nNotes\nChecked\n\n\n\n\n0\n0\n746\n844\n21\nFlorian Arendt\nSuicide on Instagram ‚Äì Content Analysis of a G...\n2019.0\nCrisis\nHogrefe Publishing Group\nhttp://dx.doi.org/10.1027/0227-5910/a000529\n...\n3.0\nAbstract. Background: Suicide is the second le...\nhttps://econtent.hogrefe.com/doi/pdf/10.1027/0...\nNaN\n[-0.0018475924152880907, 0.022463073953986168,...\n[-0.014954154379665852, 0.026176564395427704, ...\n-1\nNaN\nNaN\nFalse\n\n\n1\n1\n770\n868\n4\nPaloma de H. S√°nchez-Cobarro, Francisco-Jose M...\nThe Brand-Generated Content Interaction of Ins...\n2020.0\nJournal of Theoretical and Applied Electronic ...\nMDPI AG\nhttp://dx.doi.org/10.3390/jtaer16030031\n...\n2.0\nThe last decade has seen a considerable increa...\nhttps://www.mdpi.com/0718-1876/16/3/31/pdf\nNaN\n[-0.0029447057750076056, 0.01190990675240755, ...\n[-0.01012819167226553, 0.02539714053273201, -0...\n-1\nNaN\nNaN\nFalse\n\n\n\n\n\n2 rows √ó 35 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nIn the next step we are going to start our literature review:\n\nWe filter for the first unchecked row, ordered by the cite count.\nWe retrieve the abstract from CrossRef API using the DOI.\nWe display all information\nWe answer whether the paper appear to be relevant by entering y or n for yes or no.\n\nFor our session, the cell only runs through one row and finishes afterwards. For a real world application you‚Äôd probably like to add some kind of loop.\n\nfrom crossref_commons.retrieval import get_publication_as_json\nimport json\nimport openai\nimport textwrap\nimport IPython\nimport re\n\n# Get one row: Not checked, highest Citation count.\nhighest_cites_unchecked = all_data[all_data['Checked'] == False].sort_values(by=\"Cites\", ascending=False).iloc[0]\nindex = highest_cites_unchecked.name\n\n# Retrieve Abstract from Crossref\nresponse = get_publication_as_json(highest_cites_unchecked['DOI'])\nabstract = response.get(\"abstract\", \"\")\n\n# Remove XML\nabstract = re.sub(r'&lt;[^&gt;]+&gt;', '', abstract)\nall_data.loc[index, 'Abstract'] = abstract\n\n\n# Display all information\nIPython.display.clear_output(wait=True)\ntitle_disp = IPython.display.HTML(\"&lt;h2&gt;{}&lt;/h2&gt;\".format(highest_cites_unchecked['Title']))\nauthors_disp = IPython.display.HTML(\"&lt;p&gt;{}&lt;/p&gt;\".format(highest_cites_unchecked['Authors']))\ndoi_disp = IPython.display.HTML(\"&lt;p&gt;&lt;a target='_blank' href='https://doi.org/{}'&gt;{}&lt;/a&gt;&lt;/p&gt;\".format(highest_cites_unchecked['DOI'],highest_cites_unchecked['DOI']))\ndisplay(title_disp, authors_disp, doi_disp)\nprint(textwrap.fill(abstract, 80))\nrelevant_input = input('Relevant? (y/n): ').lower().strip() == 'y'\n\n# Save user input\nall_data.loc[index, 'Checked'] = True\nall_data.loc[index, 'Relevant'] = relevant_input\n\n\n\n\nFlorian Arendt\n\n\n10.1027/0227-5910/a000529\n\n\nAbstract. Background: Suicide is the second leading cause of death among\n15‚Äì29-year-olds globally. Unfortunately, the suicide-related content on\nInstagram, a popular social media platform for youth, has not received the\nscholarly attention it deserves. Method: The present study provides a content\nanalysis of posts tagged as #selbstmord, a German suicide-related hashtag. These\nposts were created between July 5 and July¬†11, 2017. Results: Approximately half\nof all posts included words or visuals related to suicide. Cutting was by far\nthe most prominent method. Although sadness was the dominant emotion, self-hate\nand loneliness also appeared regularly. Importantly, inconsistency ‚Äì a gap\nbetween one's inner mental state (e.g., sadness) and one's overtly expressed\nbehavior (e.g., smiling) ‚Äì was also a recurring theme. Conversely, help-seeking,\ndeath wishes, and professional awareness‚Äìintervention material were very rare.\nAn explorative analysis revealed that some videos relied on very fast cutting\ntechniques. We provide tentative evidence that users may be exposed to\npurposefully inserted suicide-related subliminal messages (i.e., exposure to\ncontent without the user's conscious awareness). Limitations: We only\ninvestigated the content of posts on one German hashtag, and the sample size was\nrather small. Conclusion: Suicide prevention organizations may consider posting\nmore awareness‚Äìintervention materials. Future research should investigate\nsuicide-related subliminal messages in social media video posts. Although\ntentative, this finding should raise a warning flag for suicide prevention\nscholars.\nRelevant? (y/n): y\n\n\nNext, we check whether our input has been saved:\n\n# Check the result\nall_data.iloc[index]\n\nUnnamed: 0.2                                                          0\nUnnamed: 0.1                                                        746\nUnnamed: 0                                                          844\nCites                                                                21\nAuthors                                                  Florian Arendt\nTitle                 Suicide on Instagram ‚Äì Content Analysis of a G...\nYear                                                             2019.0\nSource                                                           Crisis\nPublisher                                      Hogrefe Publishing Group\nArticleURL                  http://dx.doi.org/10.1027/0227-5910/a000529\nCitesURL                                                            NaN\nGSRank                                                               26\nQueryDate                                           2022-09-08 10:44:44\nType                                                    journal-article\nDOI                                           10.1027/0227-5910/a000529\nISSN                                                          0227-5910\nCitationURL                                                         NaN\nVolume                                                             40.0\nIssue                                                               1.0\nStartPage                                                          36.0\nEndPage                                                            41.0\nECC                                                                  21\nCitesPerYear                                                        7.0\nCitesPerAuthor                                                       21\nAuthorCount                                                           1\nAge                                                                 3.0\nAbstract              Abstract. Background: Suicide is the second le...\nFullTextURL           https://econtent.hogrefe.com/doi/pdf/10.1027/0...\nRelatedURL                                                          NaN\nbabbage_similarity    [-0.0018475924152880907, 0.022463073953986168,...\nbabbage_search        [-0.014954154379665852, 0.026176564395427704, ...\nsimilarities                                                         -1\nRelevant                                                           True\nNotes                                                               NaN\nChecked                                                            True\nName: 0, dtype: object\n\n\n\n\n\nNow for the fun part: Is it possible to use GPT to help us during the review process? We are going to try and extract text features automatically. For the moment we are going to use gpt3.5-turbo.\nNote: Please feel free to test different prompts and questions. The Promptingguide is a good resource to learn more about different prompting techniques. Use the ChatGPT interface to cheaply test prompts prior to using them with the API. Use the OpenAI Playground to optimize your prompts with a visual user interface for different settings and a prompting history (trust me, this can save your life!).\nPrompts: We‚Äôre going to use the system prompt for our instructions, and the user prompt to send our content.\n\n\n\n\n\n\nCaution\n\n\n\nA word of warning: You should not trust the quality of the GPT output at this stage. The prompt has not been evaluated, overall LLMs produce output that appears meaningful most of the times. Sometimes, however, it is Hallucinations. Thus, before using prompts and LLMs for production, we have to make sure we can trust their outputs. We will dive deeper into this topic in the classification sessions.\n\n\n\nsystem_prompt = \"\"\"\nYou're an advanced AI research assistant. Your task is to extract **research questions**, **operationalization**, **data sources**, **population**, and **scientific disciplines** from user input. Return \"None\" if you can't find the information in user input.\n\n**Formatting**\nReturn a markdown table, one row for each extracted feature: **research questions**, **operationalization**, **data sources**, **population**, and **scientific disciplines**.\n\"\"\"\n\nPlease enter your API-Code in the next code cell for the openai.api_key variable. We have changed the cell to include the gpt_prompt variable, which sends the title and abstract as a user prompt. We‚Äôre using the openai.ChatCompletion.create() method to send our request to the API. We expect the response in api_response['choices'][0]['message']['content'] to be markdown (see prompt above), as such we display the markdown in our notebook.\n\nfrom crossref_commons.retrieval import get_publication_as_json\nimport json\nimport openai\nimport textwrap\nimport IPython\nimport re\n\n# Enter OpenAI API-Code\nopenai.api_key = \"sk-XXXXXXXXX\"\n\n# Get one row: Not checked, highest Citation count.\nhighest_cites_unchecked = all_data[all_data['Checked'] == False].sort_values(by=\"Cites\", ascending=False).iloc[0]\nindex = highest_cites_unchecked.name\n\n# Retrieve Abstract from Crossref\nresponse = get_publication_as_json(highest_cites_unchecked['DOI'])\nabstract = response.get(\"abstract\", \"\")\n\n# Remove XML\nabstract = re.sub(r'&lt;[^&gt;]+&gt;', '', abstract)\n\nall_data.loc[index, 'Abstract'] = abstract\n\n# Display all information (before we send the request to OpenAI)\nIPython.display.clear_output(wait=True)\ntitle_disp = IPython.display.HTML(\"&lt;h2&gt;{}&lt;/h2&gt;\".format(highest_cites_unchecked['Title']))\nauthors_disp = IPython.display.HTML(\"&lt;p&gt;{}&lt;/p&gt;\".format(highest_cites_unchecked['Authors']))\ndoi_disp = IPython.display.HTML(\"&lt;p&gt;&lt;a target='_blank' href='https://doi.org/{}'&gt;{}&lt;/a&gt;&lt;/p&gt;\".format(highest_cites_unchecked['DOI'],highest_cites_unchecked['DOI']))\ndisplay(title_disp, authors_disp, doi_disp)\nprint(textwrap.fill(abstract, 80))\n\ngpt_prompt = f\"\"\"\n**Title**: {highest_cites_unchecked['Title']}\n**Abstract**: {abstract}\n\"\"\"\n\n# Sending request, takes a moment. In the meantime you may read the abstract.\nmessages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": abstract}\n]\n\ntry:\n  api_response = openai.ChatCompletion.create(\n      model=\"gpt-3.5-turbo\",\n      messages=messages,\n      temperature=0,\n      timeout=30\n    )\n\n  gpt_result = api_response['choices'][0]['message']['content']\n\n  # Display the GPT result\n  display(IPython.display.HTML(f\"&lt;h3&gt;GPT Extracted Data&lt;/h3&gt;\"))\n  display(IPython.display.Markdown(gpt_result))\nexcept:\n  print(\"GPT API Error\")\n\nrelevant_input = input('Relevant? (y/n): ').lower().strip() == 'y'\n\n# Save user input\nall_data.loc[index, 'Checked'] = True\nall_data.loc[index, 'Relevant'] = relevant_input\n\n\n\n\nPaloma de H. S√°nchez-Cobarro, Francisco-Jose Molina-Castillo, Cristina Alcazar-Caceres\n\n\n10.3390/jtaer16030031\n\n\nThe last decade has seen a considerable increase in entertainment-oriented\ncommunication techniques. Likewise, the rise of social networks has evolved,\noffering different formats such as publication and stories. Hence, there has\nbeen a growing interest in knowing which strategies have the greatest social\nimpact to help position organizations in the mind of the consumer. This research\naims to analyze the different impact that stories and publications can have on\nthe Instagram social network as a tool for generating branded content. To this\nend, it analyses the impact of the different Instagram stories and publications\nin various sectors using a methodology of structural equations with composite\nconstructs. The results obtained, based on 800 stories and publications in four\ndifferent companies (retailers and manufacturers), show that the reach of the\nstory generally explains the interaction with Instagram stories. In contrast, in\nthe case of publications, impressions are of greater importance in explaining\nthe interaction with the publication. Among the main contributions of the work,\nwe find that traditional pull communication techniques have been losing\neffectiveness in front of new formats of brand content generation that have been\noccupying the time in the relationship between users and brands.\nRelevant? (y/n): y\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nValue\n\n\n\n\nResearch questions\n- What strategies have the greatest social impact on Instagram?- How do stories and publications on Instagram impact the consumer‚Äôs perception of brands?- What is the relationship between reach and interaction with Instagram stories?- What is the relationship between impressions and interaction with Instagram publications?\n\n\nOperationalization\n- Analyzing the impact of Instagram stories and publications in various sectors- Using a methodology of structural equations with composite constructs\n\n\nData sources\n- 800 stories and publications on Instagram\n\n\nPopulation\n- Four different companies (retailers and manufacturers)\n\n\nScientific disciplines\n- Marketing- Communication\n\n\n\n\n\n\nThe above output shows a formatted table listing all extracted features. In this short warm-up session on GPT we have seen one use case of the LLM: The extraction of text feautures. In future sessions we are going to dive deeper into this topic.\n\n\n\n\n\n\nNote\n\n\n\nDid you create an excellent prompt? Share it with us! Enter your prompt into this Excel Sheet\n\n\n\n\n\nThe following line saves all progress to file_name. If file_name is a path to Google Drive you will be able to pick up your work later on.\n\nall_data.to_csv(file_name)\n\n\nSource: GPT Literature Review Assistant"
  },
  {
    "objectID": "getting-started/literature-review-assistant.html#setup",
    "href": "getting-started/literature-review-assistant.html#setup",
    "title": "GPT Literature Review Assistant",
    "section": "",
    "text": "At first we need to install necessary packages. Hit run and wait.\n\nprint(\"Install Packages\")\n!pip install -q openai crossref-commons"
  },
  {
    "objectID": "getting-started/literature-review-assistant.html#import-publish-or-perish-data.",
    "href": "getting-started/literature-review-assistant.html#import-publish-or-perish-data.",
    "title": "GPT Literature Review Assistant",
    "section": "",
    "text": "If this is the start of your review process, upload the csv file exported from Publish or Perish in the left-hand Files pane. Enter the filename in publish_or_perish_file_name. Define the output name in file_name. If you want to save the imported file in the google drive add /content/drive/MyDrive/ to the path.  Skip this cell if you want to work with a file that has been imported in the past.\n\n\n\n\n\n\nWarning\n\n\n\nWe delete rows with missing DOIs. Without a DOI our code cannot retrieve abstracts. When importing the Publish or Perish file, the following code will display the number of rows that have been deleted due to missing DOIs. When using this notebook for real-world projects, you should be aware of the missing rows and manually review them!\n\n\n\n#@title Import from Publish or Perish Data.\n#@markdown If this is the start of your review process, upload the `csv` file exported from [Publish or Perish](https://harzing.com/resources/publish-or-perish) in the left-hand *Files* pane. Enter the filename in `publish_or_perish_file_name`. Define the output name in `file_name`. If you want to save the imported file in the google drive add `/content/drive/MyDrive/` to the path. &lt;br/&gt; **Skip this cell if you want to work with a file that has been imported in the past.**\n\nimport pandas as pd\nimport numpy as np\nimport io\n\npublish_or_perish_file_name = \"scholar.csv\" # @param {type: \"string\"}\nfile_name = \"2023-10-31-Literature-Review.csv\" # @param {type: \"string\"}\n\n# Initialize empty DataFrame\nall_data = pd.DataFrame()\n\n\ntry:\n    all_data = pd.read_csv(publish_or_perish_file_name)\n\n    # Remove Duplicates\n    initial_len = len(all_data)\n    all_data = all_data.drop_duplicates(subset='DOI', keep='first')\n    removed_len = initial_len - len(all_data)\n    print(f'Removed {removed_len} duplicates based on DOI.')\n\n    # Remove missing DOIs\n    initial_len = len(all_data)\n    all_data = all_data[~pd.isna(all_data['DOI'])]\n    removed_len = initial_len - len(all_data)\n    print(f'Removed {removed_len} rows without DOI.')\n\n    all_data = all_data.sort_values(by='Cites', ascending=False).reset_index(drop=True)\n\n    print('Sorted Table by Cites.')\n\n    # Create empty columns for Literature Review\n    all_data[\"Relevant\"] = \"\"\n    all_data[\"Notes\"] = \"\"\n    all_data[\"Checked\"] = False\n\n    print('Initialized Columns')\n\n    all_data.to_csv(file_name)\n    print(f\"Success: Saved data to {file_name}\")\n\n    print(f'Success: Data loaded from File \"{file_name}\".')\nexcept Exception as e:\n    print(f\"Error: Failed to load data from File. {str(e)}\")\n\nRemoved 172 duplicates based on DOI.\nRemoved 1 rows without DOI.\nSorted Table by Cites.\nInitializes Columns\nSuccess: Saved data to 2023-10-31-Literature-Review.csv\nSuccess: Data loaded from File \"2023-10-31-Literature-Review.csv\"."
  },
  {
    "objectID": "getting-started/literature-review-assistant.html#read-previously-imported-file",
    "href": "getting-started/literature-review-assistant.html#read-previously-imported-file",
    "title": "GPT Literature Review Assistant",
    "section": "",
    "text": "If you want to keep going with a former review process, we can read an uploaded file / a file from google drive. Only run one cell, this one or the above.\n\n#@title Read previously imported File\n#@markdown If you want to keep going with a former review process, we can read an uploaded file / a file from google drive. **Only run one cell, this one or the above.**\nimport pandas as pd\nimport numpy as np\nimport io\n\nfile_name = \"2023-10-31-Literature-Review.csv\" # @param {type: \"string\"}\n\ntry:\n    all_data = pd.read_csv(file_name)\n\n    print(f'Success: Data loaded from File \"{file_name}\".')\nexcept Exception as e:\n    print(f\"Error: Failed to load data from File. {str(e)}\")\n\nSuccess: Data loaded from File \"2023-10-31-Literature-Review.csv\".\n\n\n\nIn this example we‚Äôve saved the file locally. When working with Colab, the file will be deleted when we disconnect. For colab you should link your google drive (open the files pane on the left, click the Google Drive button). Once connected, save the file in the folder /content/drive/MyDrive/YOUR-FILENAME.csv. It will be accessible through Drive, and Colab is from now on going to connect automatically to drive.\nCheck the imported data. We‚Äôre using pandas, the imported data is saved in the all_datavariable. head(2)displays the two top rows of the table. Additionally, we have added three columns: Relevant, Notes, and Checked. We are going to make use of them to keep track of our progress.\n\n# Check the structure (and content) of the file\nall_data.head(2)\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0.2\nUnnamed: 0.1\nUnnamed: 0\nCites\nAuthors\nTitle\nYear\nSource\nPublisher\nArticleURL\n...\nAge\nAbstract\nFullTextURL\nRelatedURL\nbabbage_similarity\nbabbage_search\nsimilarities\nRelevant\nNotes\nChecked\n\n\n\n\n0\n0\n746\n844\n21\nFlorian Arendt\nSuicide on Instagram ‚Äì Content Analysis of a G...\n2019.0\nCrisis\nHogrefe Publishing Group\nhttp://dx.doi.org/10.1027/0227-5910/a000529\n...\n3.0\nAbstract. Background: Suicide is the second le...\nhttps://econtent.hogrefe.com/doi/pdf/10.1027/0...\nNaN\n[-0.0018475924152880907, 0.022463073953986168,...\n[-0.014954154379665852, 0.026176564395427704, ...\n-1\nNaN\nNaN\nFalse\n\n\n1\n1\n770\n868\n4\nPaloma de H. S√°nchez-Cobarro, Francisco-Jose M...\nThe Brand-Generated Content Interaction of Ins...\n2020.0\nJournal of Theoretical and Applied Electronic ...\nMDPI AG\nhttp://dx.doi.org/10.3390/jtaer16030031\n...\n2.0\nThe last decade has seen a considerable increa...\nhttps://www.mdpi.com/0718-1876/16/3/31/pdf\nNaN\n[-0.0029447057750076056, 0.01190990675240755, ...\n[-0.01012819167226553, 0.02539714053273201, -0...\n-1\nNaN\nNaN\nFalse\n\n\n\n\n\n2 rows √ó 35 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nIn the next step we are going to start our literature review:\n\nWe filter for the first unchecked row, ordered by the cite count.\nWe retrieve the abstract from CrossRef API using the DOI.\nWe display all information\nWe answer whether the paper appear to be relevant by entering y or n for yes or no.\n\nFor our session, the cell only runs through one row and finishes afterwards. For a real world application you‚Äôd probably like to add some kind of loop.\n\nfrom crossref_commons.retrieval import get_publication_as_json\nimport json\nimport openai\nimport textwrap\nimport IPython\nimport re\n\n# Get one row: Not checked, highest Citation count.\nhighest_cites_unchecked = all_data[all_data['Checked'] == False].sort_values(by=\"Cites\", ascending=False).iloc[0]\nindex = highest_cites_unchecked.name\n\n# Retrieve Abstract from Crossref\nresponse = get_publication_as_json(highest_cites_unchecked['DOI'])\nabstract = response.get(\"abstract\", \"\")\n\n# Remove XML\nabstract = re.sub(r'&lt;[^&gt;]+&gt;', '', abstract)\nall_data.loc[index, 'Abstract'] = abstract\n\n\n# Display all information\nIPython.display.clear_output(wait=True)\ntitle_disp = IPython.display.HTML(\"&lt;h2&gt;{}&lt;/h2&gt;\".format(highest_cites_unchecked['Title']))\nauthors_disp = IPython.display.HTML(\"&lt;p&gt;{}&lt;/p&gt;\".format(highest_cites_unchecked['Authors']))\ndoi_disp = IPython.display.HTML(\"&lt;p&gt;&lt;a target='_blank' href='https://doi.org/{}'&gt;{}&lt;/a&gt;&lt;/p&gt;\".format(highest_cites_unchecked['DOI'],highest_cites_unchecked['DOI']))\ndisplay(title_disp, authors_disp, doi_disp)\nprint(textwrap.fill(abstract, 80))\nrelevant_input = input('Relevant? (y/n): ').lower().strip() == 'y'\n\n# Save user input\nall_data.loc[index, 'Checked'] = True\nall_data.loc[index, 'Relevant'] = relevant_input\n\n\n\n\nFlorian Arendt\n\n\n10.1027/0227-5910/a000529\n\n\nAbstract. Background: Suicide is the second leading cause of death among\n15‚Äì29-year-olds globally. Unfortunately, the suicide-related content on\nInstagram, a popular social media platform for youth, has not received the\nscholarly attention it deserves. Method: The present study provides a content\nanalysis of posts tagged as #selbstmord, a German suicide-related hashtag. These\nposts were created between July 5 and July¬†11, 2017. Results: Approximately half\nof all posts included words or visuals related to suicide. Cutting was by far\nthe most prominent method. Although sadness was the dominant emotion, self-hate\nand loneliness also appeared regularly. Importantly, inconsistency ‚Äì a gap\nbetween one's inner mental state (e.g., sadness) and one's overtly expressed\nbehavior (e.g., smiling) ‚Äì was also a recurring theme. Conversely, help-seeking,\ndeath wishes, and professional awareness‚Äìintervention material were very rare.\nAn explorative analysis revealed that some videos relied on very fast cutting\ntechniques. We provide tentative evidence that users may be exposed to\npurposefully inserted suicide-related subliminal messages (i.e., exposure to\ncontent without the user's conscious awareness). Limitations: We only\ninvestigated the content of posts on one German hashtag, and the sample size was\nrather small. Conclusion: Suicide prevention organizations may consider posting\nmore awareness‚Äìintervention materials. Future research should investigate\nsuicide-related subliminal messages in social media video posts. Although\ntentative, this finding should raise a warning flag for suicide prevention\nscholars.\nRelevant? (y/n): y\n\n\nNext, we check whether our input has been saved:\n\n# Check the result\nall_data.iloc[index]\n\nUnnamed: 0.2                                                          0\nUnnamed: 0.1                                                        746\nUnnamed: 0                                                          844\nCites                                                                21\nAuthors                                                  Florian Arendt\nTitle                 Suicide on Instagram ‚Äì Content Analysis of a G...\nYear                                                             2019.0\nSource                                                           Crisis\nPublisher                                      Hogrefe Publishing Group\nArticleURL                  http://dx.doi.org/10.1027/0227-5910/a000529\nCitesURL                                                            NaN\nGSRank                                                               26\nQueryDate                                           2022-09-08 10:44:44\nType                                                    journal-article\nDOI                                           10.1027/0227-5910/a000529\nISSN                                                          0227-5910\nCitationURL                                                         NaN\nVolume                                                             40.0\nIssue                                                               1.0\nStartPage                                                          36.0\nEndPage                                                            41.0\nECC                                                                  21\nCitesPerYear                                                        7.0\nCitesPerAuthor                                                       21\nAuthorCount                                                           1\nAge                                                                 3.0\nAbstract              Abstract. Background: Suicide is the second le...\nFullTextURL           https://econtent.hogrefe.com/doi/pdf/10.1027/0...\nRelatedURL                                                          NaN\nbabbage_similarity    [-0.0018475924152880907, 0.022463073953986168,...\nbabbage_search        [-0.014954154379665852, 0.026176564395427704, ...\nsimilarities                                                         -1\nRelevant                                                           True\nNotes                                                               NaN\nChecked                                                            True\nName: 0, dtype: object"
  },
  {
    "objectID": "getting-started/literature-review-assistant.html#using-gpt-to-extract-information-from-abstracts",
    "href": "getting-started/literature-review-assistant.html#using-gpt-to-extract-information-from-abstracts",
    "title": "GPT Literature Review Assistant",
    "section": "",
    "text": "Now for the fun part: Is it possible to use GPT to help us during the review process? We are going to try and extract text features automatically. For the moment we are going to use gpt3.5-turbo.\nNote: Please feel free to test different prompts and questions. The Promptingguide is a good resource to learn more about different prompting techniques. Use the ChatGPT interface to cheaply test prompts prior to using them with the API. Use the OpenAI Playground to optimize your prompts with a visual user interface for different settings and a prompting history (trust me, this can save your life!).\nPrompts: We‚Äôre going to use the system prompt for our instructions, and the user prompt to send our content.\n\n\n\n\n\n\nCaution\n\n\n\nA word of warning: You should not trust the quality of the GPT output at this stage. The prompt has not been evaluated, overall LLMs produce output that appears meaningful most of the times. Sometimes, however, it is Hallucinations. Thus, before using prompts and LLMs for production, we have to make sure we can trust their outputs. We will dive deeper into this topic in the classification sessions.\n\n\n\nsystem_prompt = \"\"\"\nYou're an advanced AI research assistant. Your task is to extract **research questions**, **operationalization**, **data sources**, **population**, and **scientific disciplines** from user input. Return \"None\" if you can't find the information in user input.\n\n**Formatting**\nReturn a markdown table, one row for each extracted feature: **research questions**, **operationalization**, **data sources**, **population**, and **scientific disciplines**.\n\"\"\"\n\nPlease enter your API-Code in the next code cell for the openai.api_key variable. We have changed the cell to include the gpt_prompt variable, which sends the title and abstract as a user prompt. We‚Äôre using the openai.ChatCompletion.create() method to send our request to the API. We expect the response in api_response['choices'][0]['message']['content'] to be markdown (see prompt above), as such we display the markdown in our notebook.\n\nfrom crossref_commons.retrieval import get_publication_as_json\nimport json\nimport openai\nimport textwrap\nimport IPython\nimport re\n\n# Enter OpenAI API-Code\nopenai.api_key = \"sk-XXXXXXXXX\"\n\n# Get one row: Not checked, highest Citation count.\nhighest_cites_unchecked = all_data[all_data['Checked'] == False].sort_values(by=\"Cites\", ascending=False).iloc[0]\nindex = highest_cites_unchecked.name\n\n# Retrieve Abstract from Crossref\nresponse = get_publication_as_json(highest_cites_unchecked['DOI'])\nabstract = response.get(\"abstract\", \"\")\n\n# Remove XML\nabstract = re.sub(r'&lt;[^&gt;]+&gt;', '', abstract)\n\nall_data.loc[index, 'Abstract'] = abstract\n\n# Display all information (before we send the request to OpenAI)\nIPython.display.clear_output(wait=True)\ntitle_disp = IPython.display.HTML(\"&lt;h2&gt;{}&lt;/h2&gt;\".format(highest_cites_unchecked['Title']))\nauthors_disp = IPython.display.HTML(\"&lt;p&gt;{}&lt;/p&gt;\".format(highest_cites_unchecked['Authors']))\ndoi_disp = IPython.display.HTML(\"&lt;p&gt;&lt;a target='_blank' href='https://doi.org/{}'&gt;{}&lt;/a&gt;&lt;/p&gt;\".format(highest_cites_unchecked['DOI'],highest_cites_unchecked['DOI']))\ndisplay(title_disp, authors_disp, doi_disp)\nprint(textwrap.fill(abstract, 80))\n\ngpt_prompt = f\"\"\"\n**Title**: {highest_cites_unchecked['Title']}\n**Abstract**: {abstract}\n\"\"\"\n\n# Sending request, takes a moment. In the meantime you may read the abstract.\nmessages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": abstract}\n]\n\ntry:\n  api_response = openai.ChatCompletion.create(\n      model=\"gpt-3.5-turbo\",\n      messages=messages,\n      temperature=0,\n      timeout=30\n    )\n\n  gpt_result = api_response['choices'][0]['message']['content']\n\n  # Display the GPT result\n  display(IPython.display.HTML(f\"&lt;h3&gt;GPT Extracted Data&lt;/h3&gt;\"))\n  display(IPython.display.Markdown(gpt_result))\nexcept:\n  print(\"GPT API Error\")\n\nrelevant_input = input('Relevant? (y/n): ').lower().strip() == 'y'\n\n# Save user input\nall_data.loc[index, 'Checked'] = True\nall_data.loc[index, 'Relevant'] = relevant_input\n\n\n\n\nPaloma de H. S√°nchez-Cobarro, Francisco-Jose Molina-Castillo, Cristina Alcazar-Caceres\n\n\n10.3390/jtaer16030031\n\n\nThe last decade has seen a considerable increase in entertainment-oriented\ncommunication techniques. Likewise, the rise of social networks has evolved,\noffering different formats such as publication and stories. Hence, there has\nbeen a growing interest in knowing which strategies have the greatest social\nimpact to help position organizations in the mind of the consumer. This research\naims to analyze the different impact that stories and publications can have on\nthe Instagram social network as a tool for generating branded content. To this\nend, it analyses the impact of the different Instagram stories and publications\nin various sectors using a methodology of structural equations with composite\nconstructs. The results obtained, based on 800 stories and publications in four\ndifferent companies (retailers and manufacturers), show that the reach of the\nstory generally explains the interaction with Instagram stories. In contrast, in\nthe case of publications, impressions are of greater importance in explaining\nthe interaction with the publication. Among the main contributions of the work,\nwe find that traditional pull communication techniques have been losing\neffectiveness in front of new formats of brand content generation that have been\noccupying the time in the relationship between users and brands.\nRelevant? (y/n): y\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nValue\n\n\n\n\nResearch questions\n- What strategies have the greatest social impact on Instagram?- How do stories and publications on Instagram impact the consumer‚Äôs perception of brands?- What is the relationship between reach and interaction with Instagram stories?- What is the relationship between impressions and interaction with Instagram publications?\n\n\nOperationalization\n- Analyzing the impact of Instagram stories and publications in various sectors- Using a methodology of structural equations with composite constructs\n\n\nData sources\n- 800 stories and publications on Instagram\n\n\nPopulation\n- Four different companies (retailers and manufacturers)\n\n\nScientific disciplines\n- Marketing- Communication\n\n\n\n\n\n\nThe above output shows a formatted table listing all extracted features. In this short warm-up session on GPT we have seen one use case of the LLM: The extraction of text feautures. In future sessions we are going to dive deeper into this topic.\n\n\n\n\n\n\nNote\n\n\n\nDid you create an excellent prompt? Share it with us! Enter your prompt into this Excel Sheet"
  },
  {
    "objectID": "getting-started/literature-review-assistant.html#save-your-progress",
    "href": "getting-started/literature-review-assistant.html#save-your-progress",
    "title": "GPT Literature Review Assistant",
    "section": "",
    "text": "The following line saves all progress to file_name. If file_name is a path to Google Drive you will be able to pick up your work later on.\n\nall_data.to_csv(file_name)"
  },
  {
    "objectID": "getting-started/theory.html",
    "href": "getting-started/theory.html",
    "title": "Introduction to SMA",
    "section": "",
    "text": "Social Media Analyses (SMA) are used both, in academia and in professional settings. Depending on the research agenda, different methodologies may be applied (Kanthawala et al. 2022; Rejeb et al. 2022). In our course, we focus on the academic exploration of Social Media. We place particular emphasis on questions related to media, politics, and society. This represents a confluence of communication science and political science, intertwined with computational methods."
  },
  {
    "objectID": "getting-started/theory.html#social-media-analyses-in-different-contexts",
    "href": "getting-started/theory.html#social-media-analyses-in-different-contexts",
    "title": "Introduction to SMA",
    "section": "Social Media Analyses in different contexts",
    "text": "Social Media Analyses in different contexts\nBridging this discussion, there are several disciplines pivotal to the academic analysis of social media data at this intersection: Lazer et al. (2009) outlined in an influencial article computational social science as an emerging field that built on the ability to collect and analyze vast amounts of data. The goal of the computational social science, according to this article, is to reveal patterns in human interactions, benefiting from various data sources such as emails, phone records, online social networks, and other digital traces left by individuals. We are going to concentrate on social media data, a type of data described by Quan-Haase and Sloan (2022a) as incidental, since the data exists and is being created, no matter the researchers observing them ‚Äì or not. One special type of data, Instagram stories, even have an ephemeral character. 24 hours after posting the story expires ‚Äì becoming invisible for followers and researchers alike (see also Leaver, Highfield, and Abidin 2020 on the importance of stories). Atteveldt and Peng (2018) noted a surge in the use of computational methods in communication science, attributing it to three primary factors: the availability of digital data, sophisticated data analysis tools, and the emergence of cost-effective, potent processing capabilities complemented by accessible computing infrastructure. Building on this perspective, Haim (2023) sees the computational communication science as a sub-discipline of communication science that addresses digitally altered objects of research, which require computational approaches to tackle to amount and complexity of this special type of data.\nIn the realm of digital humanities, computational approaches to text analysis have a long history, influenced by concepts such as distant reading (Moretti 2000) and macroanalysis (Jockers 2013). Manovich picks up these concepts in his cultural analytics, see below. Lately also distant viewing has been outlined, as ‚Äúa methodological and theoretical framework for the study of large collections of visual materials‚Äù (Arnold and Tilton 2019). I see potential in integrating approaches and methods from the digital humanities into social media analysis. Vice versa, there‚Äôs also potential in utilizing methods used for social media analysis to address questions in the humanities.\nChallenges for social media analyses have been outlined by Quan-Haase and Sloan (2022a): the role of theory, representativeness of data, scale, multimodality, data accessability, and legal and ethical considerations. Through our semester we are going to work on several of those challenges: In the Operationalization session we are going to talk about data-driven approaches (bearing in mind Anderson et al. 2008), as well as theories as basis for your research questions and operationalizations. The representativeness of data will be the challenge for our data collection sessions: We will not just answer how to collect data, but also what data to collect. The two challenges left are at the centre of our seminar: Our answer for the challenge of scale is to apply computational methods for data analysis, to process data at scale. Multimodality is another key issues of this seminar: We want to computationally process visual (or multimodal) data. We will talk about accessability problems throughout our data collection classes, and talk about legal and ethical issues on this page.\nKeeping these introductory considerations in mind, we immerse into a short outline of two theories: Cultural Analytics and Digital Methods, as foundational elements for social media research. Subsequently, we‚Äôll address the ethical and legal challenges associated with analyzing social media. We‚Äôll conclude the chapter by presenting an array of methodologies. In the related work chapter, you‚Äôll find an overview of research on Instagram and TikTok content, even extending beyond our primary topics of interest.\n\n\n\n\n\n\nNote\n\n\n\nThe intent of this article is to provide a brief introduction to the field of computational social media analysis, tailored for my Winter 2023/24 seminar. It offers only a cursory glance at various theories and methodologies. As such, please do not regard the content of this page as a definitive scientific piece. Instead, view it as a compass to guide and inspire your own research endeavors. For a deeper dive into the theory of Digital Media in Politics and Society see the lecture by Prof.¬†Jungherr."
  },
  {
    "objectID": "getting-started/theory.html#cultural-analytics",
    "href": "getting-started/theory.html#cultural-analytics",
    "title": "Introduction to SMA",
    "section": "Cultural Analytics",
    "text": "Cultural Analytics\nCultural analytics, as explained in the introductory chapter of the book ‚ÄúCultural Analytics‚Äù by Lev Manovich, is a field that uses computers to analyze and understand large amounts of cultural information or ‚Äúbig cultural data‚Äù. This might include exploring big collections of images, videos, or other media data to see patterns and trends that are happening in digital culture. Manovich talks about some key questions and challenges in cultural analytics. For example, one big question is whether we should focus on finding common themes and patterns in our data, or whether we should pay more attention to things that are unusual or rare. Also, while cultural analytics can be a powerful tool for understanding aspects of culture, especially in the digital world, Manovich tells us to be aware of its limits. He says that computers and data analysis can tell us a lot, but they can‚Äôt understand culture in the rich and deep way that humans can, especially when it comes to understanding things like aesthetics (beauty, style, etc.). So, while cultural analytics can help us see large scale patterns and trends in culture, Manovich advises us to also appreciate and be aware of what it can‚Äôt see or understand. The field of cultural analytics then becomes a space where we use computational tools to explore and question culture, while also being mindful of the limitations and challenges of using these tools (Manovich 2020)."
  },
  {
    "objectID": "getting-started/theory.html#digital-methods",
    "href": "getting-started/theory.html#digital-methods",
    "title": "Introduction to SMA",
    "section": "Digital Methods",
    "text": "Digital Methods\n‚ÄúDigital Methods,‚Äù as introduced by Rogers (2013), proposes a paradigm wherein the internet is both a site and a source for research, especially for social media studies. Unlike conventional research approaches that see the internet merely as a tool or data source, Rogers advocates for a methodology that is intrinsically web-centric, understanding and employing the unique dynamics and mechanics of the digital medium itself. An example for a digital methods research project is understanding algorithmic operations, especially of search engines like Google, and comprehending their impact on digital culture, information accessibility, and user engagement. This perspective is important to explore the foundations of how information is organized, ranked, and accessed online. Studying the digital medium itself means to study web-native phenomena such as hyperlink networks, search engine behaviors, and social media activities to uncover patterns, tendencies, and hierarchical structures within digital cultures and societies.\nThe concepts of cultural analytics and digital methods will guide us through our semester and our projects: We borrow the idea to use computational methods in order to understand ‚Äúbig cultural data‚Äù form Manovich and the concept of studying the digital medium itself from Rogers. Throughout the semester will enrich our projects through your own literature and theory based on the research interests. Beyond these foundations, we will borrow from i.e.¬†the Computational Social Sciences (Lazer et al. 2009), the concept of Distant Viewing (Arnold and Tilton 2019), or Grammars of Action (Agre 1994; Gerlitz and Rieder 2018; Bainotti, Caliandro, and Gandini 2020; Omena, Rabello, and Mintz 2020), and Platform Vernaculars (Gibbs et al. 2015)."
  },
  {
    "objectID": "getting-started/theory.html#legal-ethical-challenges",
    "href": "getting-started/theory.html#legal-ethical-challenges",
    "title": "Introduction to SMA",
    "section": "Legal & Ethical Challenges",
    "text": "Legal & Ethical Challenges\n\n\n\n\n\n\nWarning\n\n\n\nThis subchapter scratches the surface. Recommended reading: Haim (2023) pp.¬†62‚Äì69; 126‚Äì128.\n\n\nWhen working with social media data, we‚Äôre dealing with personal information. As such we need to take into account legal and ethical considerations. From the legal perspective we need to focus on two aspects: The ownership of the data, and ‚Äì when dealing with personal data ‚Äì the GDPR. For the latter we need to take into account consent and should think about pseudonymisation or anonymisation of our data (Haim 2023). Further, the German Urheberrecht, the equivalent of the anglo-saxon copyright law (there are important differences, see Bundeszentrale f√ºr politische Bildung for a synopsis), defines exceptions for scientific research: I recommend the publication by Rat f√ºr Sozial- und Wirtschaftsdaten (RatSWD) (2019) which takes a closer look at the database law and provides some practical guidance (more in our slides).\nThe importance of the legal perspective social media research grew recently: Following the Cambridge Analytica scandal Meta platforms (like Instagram) started closing down on APIs, which would have offered a legal and accepted (by the plattform) point of access for researchers. I recommend to read McCrow-Young‚Äôs (2021) article, as she demonstrates how academic research may be interrupted by platform changes, like the closure of the Instagram-API in the wake of above incident. Post-API social media research found creative ways to access the data: Bainotti, Caliandro, and Gandini (2020), for example, took a unique approach for data collection by capturing Instagram content through YouTube videos. Recent publications on Instagram analyses, and most approaches in our future session, rely on crawling and scraping. Venturini and Rogers (2019) see a chance in the API-closure and argue that these techniques are ‚Äúmore than a ‚Äònecessary evil‚Äô‚Äù, as it might force researchers to come back to (digital) field work.\nFinally a word about reserach ethics. While the GDPR provides a rigid legal framework for dealing with personal information, I‚Äôd like to recommend the article ‚ÄúBut the Data is Already Public‚Äù by Zimmer (2010). The article documents how, in a matter of days, an anonymous dataset of 1700 facebook profiles became (partly) deanonymized. Based on this case study, the author compiles ethical concerns for future research, which we should also incorporate into our work."
  },
  {
    "objectID": "getting-started/theory.html#methodology",
    "href": "getting-started/theory.html#methodology",
    "title": "Introduction to SMA",
    "section": "Methodology",
    "text": "Methodology\nIn this chapter we are going to take a look at different methods for use with social media research, and particularly, with our projects. We are going to use (Visual) Content Analysis to understand the content of posts and stories. The concept of Plattform Affordances will help us understand these posts and stories as embedded in the platform and its available functions and options. Finally, the idea of Platform Vernaculars & Grammars serves as a guide to wire everything up, to discover patterns and trends in how users communicate and engage on these platforms.\n\n(Visual) Content Analysis\nWe are going to apply quantitative content analyses to our corpora. For a quantitative approach we are going to operationalize our theory-based interests and questions using formal and / or content features. Next, we need to apply the operationalization to the documents, in form of human annotations or computational coding (see D√∂ring and Bortz 2016). D√∂ring and Bortz (2016) outline a general approach to content analysis, Rose (2016) in contrast concentrates on visual content analyses. She suggests four steps:\n\n\n‚ÄúFinding your Images.\nDevising your categories for coding.\nCoding the images.\nAnalysing the results.‚Äù ‚Äì (Rose 2016 ch.¬†5)\n\n\nThe challenge of the first step is the sampling: Even with computational approaches, is it feasible to collect everything? The cultural analytics approach suggests such a goal, e.g.¬†in order to obtain data and traces of subcultures. Due to practical limitations also Manovich‚Äôs works use an approach to break the large amount of available data into a smaller portion (see Hochman and Manovich 2013). This approach is called sampling, Rose (2016) introduces several sampling approaches like random, stratified, systematic, or cluster sampling. D√∂ring and Bortz (2016) provide a deeper look into sampling strategies.\nThe codes, for the second step, may be devised from a qualitative exploration of the data or theories and related work. In context of our projects we are going to use both approaches: We will annotate a subset of our data as ground-truth while coding the total data using computational approaches. On code development there exists another large body of literature, like the Grounded Theory (e.g. Corbin and Strauss 2008) and Ethnic Coding Approach (Altheide 1987).\nFor the final analysis we are going to apply statistical data analyses. For an initial understanding of our data we will start with some exploratory analyses, e.g.¬†plotting the data. In combination with the two approaches below, the platform affordances and platform vernaculars & grammars we may discover patterns of social media use. In most cases, our projects will compare different groups: These groups might be different user types (e.g.¬†Politician Accounts vs.¬†Party Accounts), or different Posts types (e.g.¬†Posts vs.¬†Stories), or different platforms (e.g.¬†Instagram vs.¬†TikTok).\n\n\nPlatform Affordances\nBossetta (2018) provides an overview of the concept of affordances and their application in social media analyses. He traces the term back to boyd and Papacharissi & Yuan who argued ‚Äúthat digital communication tech- nologies provide structural affordances to agents‚Äù (p.¬†473 Bossetta 2018). There are two important take-aways from his work: 1) The concept of affordances is not used consistently, and 2) the platforms shape affordances and thereby how users interact with the platform. Bainotti, Caliandro, and Gandini (2020) used the ‚ÄúInstagram-specific digital objects‚Äù as codes for their analysis of stories, linking the concept of affordances in the context of Instagram to the use of stickers.\nIn the context of our seminar we might consider the following elements as platform affordances:\n\n\n\nTikTok\nIG ‚Äì Posts\nIG ‚Äì Stories\n\n\n\n\nLikes\nLikes\nSliders\n\n\nComments\nComments\nVotes\n\n\nShares\nViews\nQuestions\n\n\nMusic\nMentions\nMentions\n\n\nHashtags\nHashtags\nHashtags\n\n\n‚Ä¶\n‚Ä¶\nLocations\n\n\n\n\n‚Ä¶\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nDid you spot the difference between some of the listed affordances? Likes and comments, for instance, are reactions to posts. Would you consider these features as affordances? Let‚Äôs discuss this is in class!\n\n\n\n\nPlatform Vernaculars & Grammars\nPrevious studies have looked into ‚Äògrammars‚Äô in Instagram stories. Originally linked to research on privacy (Agre 1994), grammars classify activities using specific types, making data collection and analysis easier. This uncovers patterns in user behavior, beneficial for purposes such as advertising. To the best of my knowledge, this concept was first used for social media data by Gerlitz and Rieder (2018) in a Twitter study.\nOmena, Rabello, and Mintz (2020) discussed a ‚Äúgrammar of hashtags‚Äù, referring to the rules of hashtag use and how they‚Äôre organized on platforms. They suggest that hashtags, content visibility, and the nature of the content itself are essential in understanding hashtag use. Meanwhile, Bainotti, Caliandro, and Gandini (2020) used grammars to understand Instagram Stories, focusing on visual elements and their cultural meanings.\nLastly, Gibbs et al. (2015) examined the unique styles and logics of social media, termed ‚Äúplatform vernaculars‚Äù. These are influenced both by platform features and user habits."
  },
  {
    "objectID": "getting-started/theory.html#summary",
    "href": "getting-started/theory.html#summary",
    "title": "Introduction to SMA",
    "section": "Summary",
    "text": "Summary\nIn this chapter we have positioned ourselves between several disciplines: The computational social science, computational communication science, and digital humanities. In this position, we see social media data as trace data of human and social behaviour. The digitalness of our subject is, however, just one side of the coin: Follwing the theoretical frameworks of Digital Methods and Cultural Analytics, we want to conduct our analyses computationally with the aim to uncover patterns and trends of user behaviour on social media plattforms. Methodologically we can draw from quantitative content analysis, and the concept of platform affordances as features, and apply the concept of platform vernaculars and grammars to make sense of these features."
  },
  {
    "objectID": "getting-started/theory.html#additional-resources",
    "href": "getting-started/theory.html#additional-resources",
    "title": "Introduction to SMA",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nConferences\n\nInternational Conference on Social Media & Society\nIC¬≤S¬≤ 2022\nICWSM\nAoIR\nWebSci\nInternational Conference on CMC and Social Media Corpora for the Humanities\n\n\n\nJournals\n\nNew Media & Society\nBig Data & Society\n\n\n\nTextbooks\n\nRose (2016): Visual Methodologies: An Introduction to Researching with Visual Materials.\nHaim (2023): Computational Communication Science: Eine Einf√ºhrung.\nQuan-Haase and Sloan (2022b): The SAGE handbook of social media research methods.\n\n\n\nOnline Resources\n\nRichard Rogers: Social Media Research with Digital Methods (YouTube)\n\n\n\n\n\n\n\nNote\n\n\n\nDo you know of any ressources to be added to this list? Drop me a line: michael.achmann@ur.de.\n\n\n\n\n\nReferences\n\n\nAgre, Philip E. 1994. ‚ÄúSurveillance and capture: Two models of privacy.‚Äù The Information Society 10 (2): 101‚Äì27. https://doi.org/10.1080/01972243.1994.9960162.\n\n\nAltheide, David L. 1987. ‚ÄúReflections: Ethnographic content analysis.‚Äù Qualitative Sociology 10 (1): 65‚Äì77. https://doi.org/10.1007/BF00988269.\n\n\nAnderson, Chris, Medea Giordano, Matt Jancer, Philip Ball, Will Knight, Sassafras Lowrey, and Laurence Scott. 2008. ‚ÄúThe End of Theory: The Data Deluge Makes the Scientific Method Obsolete.‚Äù Wired, June. https://www.wired.com/2008/06/pb-theory/.\n\n\nArnold, Taylor, and Lauren Tilton. 2019. ‚ÄúDistant viewing: analyzing large visual corpora.‚Äù Digital Scholarship in the Humanities 34 (Supplement_1): i3‚Äì16. https://doi.org/10.1093/llc/fqz013.\n\n\nAtteveldt, Wouter van, and Tai-Quan Peng. 2018. ‚ÄúWhen Communication Meets Computation: Opportunities, Challenges, and Pitfalls in Computational Communication Science.‚Äù Communication Methods and Measures 12 (2-3): 81‚Äì92. https://doi.org/10.1080/19312458.2018.1458084.\n\n\nBainotti, Lucia, Alessandro Caliandro, and Alessandro Gandini. 2020. ‚ÄúFrom archive cultures to ephemeral content, and back: Studying Instagram Stories with digital methods.‚Äù New Media & Society, September, 1461444820960071. https://doi.org/10.1177/1461444820960071.\n\n\nBossetta, Michael. 2018. ‚ÄúThe Digital Architectures of Social Media: Comparing Political Campaigning on Facebook, Twitter, Instagram, and Snapchat in the 2016 U.S. Election.‚Äù Journalism & Mass Communication Quarterly 95 (2): 471‚Äì96. https://doi.org/10.1177/1077699018763307.\n\n\nCorbin, Juliet M, and Anselm L Strauss. 2008. Basics of qualitative research: techniques and procedures for developing grounded theory. Sage Publications, Inc.\n\n\nD√∂ring, Nicola, and J√ºrgen Bortz. 2016. Forschungsmethoden und Evaluation in den Sozial- und Humanwissenschaften. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-41089-5.\n\n\nGerlitz, and Rieder. 2018. ‚ÄúTweets are not created equal: Investigating Twitter‚Äôs client ecosystem.‚Äù International Journal of Communication Systems, no. 12: 528‚Äì47. https://pure.uva.nl/ws/files/23266519/5974_30096_2_PB.pdf.\n\n\nGibbs, Martin, James Meese, Michael Arnold, Bjorn Nansen, and Marcus Carter. 2015. ‚Äú#Funeral and Instagram: death, social media, and platform vernacular.‚Äù Information, Communication and Society 18 (3): 255‚Äì68. https://doi.org/10.1080/1369118X.2014.987152.\n\n\nHaim, Mario. 2023. Computational Communication Science: Eine Einf√ºhrung. Springer Fachmedien Wiesbaden.\n\n\nHochman, Nadav, and Lev Manovich. 2013. ‚ÄúZooming into an Instagram City: Reading the local through social media.‚Äù First Monday, June. https://doi.org/10.5210/fm.v18i7.4711.\n\n\nJockers, Matthew L. 2013. Macroanalysis: Digital Methods and Literary History. University of Illinois Press.\n\n\nKanthawala, Shaheen, Kelley Cotter, Kali Foyle, and J R Decook. 2022. Proceedings of the 55th Hawaii international conference on system sciences. Proceedings of the ... Annual Hawaii International Conference on System Sciences. Annual Hawaii International Conference on System Sciences. Hawaii International Conference on System Sciences. https://doi.org/10.24251/hicss.2022.000.\n\n\nLazer, David, Alex Pentland, Lada Adamic, Sinan Aral, Albert-Laszlo Barabasi, Devon Brewer, Nicholas Christakis, et al. 2009. ‚ÄúSocial science. Computational social science.‚Äù Science 323 (5915): 721‚Äì23. https://doi.org/10.1126/science.1167742.\n\n\nLeaver, Tama, Tim Highfield, and Crystal Abidin. 2020. Instagram: Visual Social Media Cultures. John Wiley & Sons.\n\n\nManovich, Lev. 2020. Cultural Analytics. MIT Press.\n\n\nMcCrow-Young, Ally. 2021. ‚ÄúApproaching Instagram data: reflections on accessing, archiving and anonymising visual social media.‚Äù Communication Research and Practice 7 (1): 21‚Äì34. https://doi.org/10.1080/22041451.2020.1847820.\n\n\nMoretti, Franco. 2000. ‚ÄúConjectures on World Literature.‚Äù New Left Review II (1): 54‚Äì68. https://newleftreview.org/issues/ii1/articles/franco-moretti-conjectures-on-world-literature.\n\n\nOmena, Janna Joceli, Elaine Teixeira Rabello, and Andr√© Goes Mintz. 2020. ‚ÄúDigital Methods for Hashtag Engagement Research.‚Äù Social Media + Society 6 (3): 2056305120940697. https://doi.org/10.1177/2056305120940697.\n\n\nQuan-Haase, Anabel, and Luke Sloan. 2022a. ‚ÄúChapter 1: Introduction.‚Äù In The SAGE handbook of social media research methods, edited by Anabel Quan-Haase and Luke Sloan, 2nd ed., 1‚Äì9. London, England: SAGE Publications. https://doi.org/10.4135/9781529782943.\n\n\n‚Äî‚Äî‚Äî. 2022b. The SAGE handbook of social media research methods. Edited by Anabel Quan-Haase and Luke Sloan. 2nd ed. London, England: SAGE Publications. https://doi.org/10.4135/9781529782943.\n\n\nRat f√ºr Sozial- und Wirtschaftsdaten (RatSWD). 2019. ‚ÄúBig Data in den Sozial-, Verhaltens- und Wirtschaftswissenschaften: Datenzugang und Forschungsdatenmanagement - Mit Gutachten \"Web Scraping in der unabh√§ngigen wissenschaftlichen Forschung\".‚Äù RatSWD Output. German Data Forum ( RatSWD). https://doi.org/10.17620/02671.39.\n\n\nRejeb, Abderahman, Karim Rejeb, Alireza Abdollahi, and Horst Treiblmaier. 2022. ‚ÄúThe big picture on Instagram research: Insights from a bibliometric analysis.‚Äù Telematics and Informatics 73 (September): 101876. https://doi.org/10.1016/j.tele.2022.101876.\n\n\nRogers, Richard. 2013. Digital Methods. MIT Press.\n\n\nRose, Gillian. 2016. Visual Methodologies: An Introduction to Researching with Visual Materials. SAGE Publications.\n\n\nVenturini, Tommaso, and Richard Rogers. 2019. ‚Äú‚ÄòAPI-Based Research‚Äô or How can Digital Sociology and Journalism Studies Learn from the Facebook and Cambridge Analytica Data Breach.‚Äù Digital Journalism 7 (4): 532‚Äì40. https://doi.org/10.1080/21670811.2019.1591927.\n\n\nZimmer, Michael. 2010. ‚Äú\"But the Data is Already Public\": On the Ethics of Research in Facebook.‚Äù Ethics and Information Technology 12 (4): 313‚Äì25. https://doi.org/10.1007/s10676-010-9227-5."
  },
  {
    "objectID": "getting-started/related-work.html",
    "href": "getting-started/related-work.html",
    "title": "Related Work",
    "section": "",
    "text": "While the two visual platforms Instagram and TikTok, are relatively new, plenty of research has already been published about both platforms. A naive search on google scholar for the term instagram analysis results in 4.180.000 results, for tiktok analysis in 54.800 results. We are going to take a look at current literature review studies, concentrating on Instagram. The goal for this chapter is to identify major research areas in (visual) social media research. Beyond themes, trends, and topics, review studies also offer methodological overviews on how to study social media platforms.\nAdditionally, we will explore tools like Publish or Perish that help in creating one‚Äôs own literature review. The Related Work section forms a pivotal foundation for high-quality scientific research, and a successful project report."
  },
  {
    "objectID": "getting-started/related-work.html#literature-reviews",
    "href": "getting-started/related-work.html#literature-reviews",
    "title": "Related Work",
    "section": "Literature Reviews",
    "text": "Literature Reviews\nRejeb et al. (2022) compiled a bibliometric analysis of 2,242 publications collected from the Web of Science1 database. They cover publications dated from 2013‚Äì2021 and outline 22 prior review studies, most of them concentrating on a smaller scope. Topics of these reviews include: Health, Psychology, Journalism, Mental Health, Body Image, and Marketing. Overall, their bibliographic study found similar themes in the current research: Some articles analyse the use of Instagram in the context of business, marketing, and travel. Others take a psychological angle and look into personality traits or health issues. They also found scholarly articles on privacy concerns and Instagram. Here‚Äôs some research interests they encountered in their review:\n\n\nHow does Instagram affect social and health issues, such as social comparison, eating disorders, addiction, and suicidal ideation?\nHow does Instagram facilitate and transform healthcare?\nWhat are the security and privacy concerns that result from the use of Instagram?\nHow does Instagram inter-relate with other social media platforms, such as Facebook and Twitter?\nWhat are the emerging research trends and frontiers in Instagram research?\n\n\nThey found researchers to use a multitude of methods, including surveys and questionnaires; content analysis to examine user-generated content; experimental designs to test the effects of Instagram use on users‚Äô psychological states and behaviors; and qualitative methods, such as interviews and focus groups, to gain in-depth insights into users‚Äô experiences with Instagram.\nInterestingly the bibliometric study seems to overlook a larger portion of research covering political communication on Instagram. Bast (2021) concentrates on this exact topic, she reviewed 37 studies on Instagram usage by politicians, parties, and governments. 30 studies were concerned with the Instagram use of political actors. They explored different aspects, like the self-presentation of politicians, mobilization and campaign information or whether they used Instagram to talk about political issues or interact with voters. Some of the studies use a comparative approach, e.g.¬†comparing the Instagram activity of multiple actors, others compared the Instagram usage of political actors across different countries, political systems, or election/non-election periods (Bast 2021).\nFrom a methodological point of view the review of visual content analysis for image-based social media by Milanesi and Guercini (2020) is quite interesting: They included 29 articles in their study and explored the platforms, that have been invastigated as well as the approach, whether the analysis was manual or automated. Outstanding at first is the large share of projects that have been classified as using automated approaches. Upon closer inspection, they have also classified the use of qualitative data analysis software like NVivo, as automation. Few projects, however, have already been using deep learning and computer vision based approaches for image analysis. Finally, the paper suggests that a mixed methodology that combines a netnographic approach, a research methodology that adapts ethnographic research techniques to the study of online communities, for textual and visual data collection in online communities and textual and visual content analysis may provide new insights for branding or destination management research. Overall, they argue for a combined analysis of textual and visual data. It should be noted, that their review focuses on literature from marketing research.\nOverall, each of the outlined reviews has a different focus. Taken together, they display a large variety of different fields and questions, which Instagram content helps to answer. We can use these literature reviews in two way: We can identify patterns of how to approach social media content, how to operationalize, what questions to ask, what methods to use, and ‚Äì looking at the future work sections of the reviews and the reviewed papers, where to pick up! Secondly, the literature review helps us to identify interesting literature for our own related work section and reading."
  },
  {
    "objectID": "getting-started/related-work.html#selected-articles",
    "href": "getting-started/related-work.html#selected-articles",
    "title": "Related Work",
    "section": "Selected Articles",
    "text": "Selected Articles\nThrought the next passages I‚Äôd like to introduce few interesting pieces. First, I‚Äôll outline some of the first papers concerned with Instagram content. Thereafter we proceed to take a look at Instagram stories and ephemeral content in social media.\nOne of the first analyses of Instagram content was published in 2013: The article explores how the interfaces of social media platforms like Instagram shape user interactions and the creation and sharing of media. Through computational analysis and visualizations of Instagram content, the authors study social and cultural patterns. They compare visual data from 13 global cities and provide a detailed analysis of photos from Tel Aviv, Israel, showing how such visualizations can offer insights into social, cultural, and political activities in specific locales over time‚Äã (Hochman and Manovich 2013).\n\n\n\nScreenshot of the phototrails website visualizing 50.000 images per city.\n\n\nShortly afterwards, in 2014, one of the most cited studies about Instagram was published. It provides a comprehensive analysis of Instagram photo content and user types, using computer vision techniques and clustering. The authors collected Instagram data using the Instagram API and developed a coding scheme for categorizing the photos. They identified eight popular photo categories and five distinct types of Instagram users in terms of their posted photos. They also found that a user‚Äôs audience (number of followers) is independent of their shared photos on Instagram. This study was the first in-depth analysis of content and users on Instagram (Hu, Manikonda, and Kambhampati 2014).\n\nInstagram Stories\nStories, as a special format due to their ephemeral nature, and have often been evaded academic research. The freature has been introducted of Instagram Stories in 2016 Leaver, Highfield, and Abidin (2020). An early analysis of stories is part of a master thesis on Snapchat and Instagram: Through qualitative content analysis, observation and in-depth interviews Amancio (2017) found four narrative elements used by Snapchat and Instagram storytellers to tell their stories and construct a narrative. Looking at Instagram specifically, Bainotti, Caliandro, and Gandini (2020) investigated 292 Stories by private users using an ethnographic coding approach. They claim to have identified specific grammars by matching the content and context-of-use, the two main ones are: ‚Äúa grammar for documentation and a grammar for interaction‚Äù. Other areas of interest for stories were ephemeral journalism (V√°zquez-Herrero, Direito-Rebollal, and L√≥pez-Garcƒ±ÃÅa 2019) and Female Atheletes‚Äô self-presentation (Li et al. 2021). Finally, just recently Towner and Mu√±oz (2022) published a first analysis of political communication in Instagram Stories, studying the stories published by the two U.S. presidential candidates in the 2020 campaign. The authors took a marketing perspective, and identified several flaws of the campaign: missed opportunities to share user-generated content, and inconsistencies to communication norms of the ephemeral format.\nOverall, stories have been explored by researchers from different domains. The ephemeral character sticks out in a world where the effort for deleting photos may be more expensive than keeping them (Mayer-Sch√∂nberger 2011). Thus, I see potential for many different cultural and societal questions to be answered by looking at this type of content, and great potential for using stories in our semester projects. In contrast to most other social media content, stories need to be collected in real time. This is a challenge for research and limits our questions to material that we may collect throughout the seminar.\n\n\nTikTok\n\n\n\n\n\n\nWork-In-Progress\n\n\n\nI have yet concentrated on literature about Instagram. An update for this section will be the outcome of our seminar!\n\n\n\n\nRecommended Reading\n\n\n\n\n\n\n\n\nReference\nTitle\nNote\n\n\n\n\nBainotti, Caliandro, and Gandini (2020)\nFrom archive cultures to ephemeral content, and back: Studying Instagram Stories with digital methods\nThis paper explores Instagram stories and their collection. The authors conduct a content analysis and derive different grammars for private Instagram stories.\n\n\nHa√üler, K√ºmpel, and Keller (2021)\nInstagram and political campaigning in the 2017 German federal election. A quantitative content analysis of German top politicians‚Äô and parliamentary parties‚Äô posts\nA detailed analysis of the 2017 election campaign showcasing theory-driven operationalization and (manual) content analysis.\n\n\nOmena, Rabello, and Mintz (2020)\nDigital Methods for Hashtag Engagement Research\nIntroduction of a multilayer hashtags engagement research framework paired with the concept of grammars of action. Demonstrates an interesting concept of grouping users.\n\n\nRettberg (2018)\nSnapchat: Phatic Communication and Ephemeral Social Media\nOne of the first scholarly articles on ephemeral stories, originally introduced by Snapchat.\n\n\nS√°nchez-Querubƒ±ÃÅn et al. (2023)\nPolitical TikTok: Playful performance, ambivalent critique and event-commentary\nAn interesting blueprint for doing research of political communication on TikTok; take a special look at the coding variables!\n\n\n‚Ä¶\nto be continued!\n‚Ä¶"
  },
  {
    "objectID": "getting-started/related-work.html#writing-the-related-work-section",
    "href": "getting-started/related-work.html#writing-the-related-work-section",
    "title": "Related Work",
    "section": "Writing the Related Work section",
    "text": "Writing the Related Work section\nThe aim of our project paper diverges somewhat from a comprehensive literature review, such as those that commonly serve as the start for dissertations. Nevertheless, the ‚ÄúRelated Work‚Äù section of your paper is as an important element of your research project. The goal here is to showcase a thorough understanding of the existing literature in your field of study. This enables you to position your research within the broader academic context, highlighting its relevance and identifying gaps that your project seeks to address. It is important to discuss your findings in this section, offering insights into the methodologies, findings, and limitations of the studies you review. Here are some steps to follow:\n\nAsk yourself: What is your research interest?\n\nWrite down key-words for your research interest.\nUsing the key-words, start your initial search with e.g.¬†the Quick and Dirty strategy. Using the first results, start an in-depth search based on other strategies.\nWrite notes to retain search terms and selected results. Tools like Obsidian or Notion are excelent tools for notes, Excel or Google Sheets are simple, yet efficient, tools to structure your searches and selected literature (and we can export the data as csv files to process them using Python). Publish or Perish is a great tool to help in this stage, as it retains a protocol of your searches and offers the data export of search results.\nConcentrate on reading the abstract in your initial searches. We have to work efficiently, the abstract should contain the most relevant information about a given article for a first evaluation of its importance for your project.\nUse literature management software like Paperpile, Zotero, or Citavi to organize your reading. You might start using the software already at the skimming and abstract reading stage, once the reading starts, however, I would absolutely recommend to add the read articles to the managment software: Keep the PDFs organized using the software, keep your annotations in there, keep your notes in there!\nAt the end of your literature search process, you should be able to write the related work section of your project report and methods section. The related work section is part of your introduction and should include a summary and analysis of the relevant studies and research that has been conducted on your topic. Furthermore, your method section should ideally contain references to previous studies that have used similar methods or approaches.\n\n\n\n\n\n\n\nWork-In-Progress\n\n\n\nAs a warm-up with ChatGPT / GPT we will extract information from abstracts in out tools session. The notebook will be added shortly, you will be able to use this approach with Publish or Perish lists.\n\n\n\nPublish or Perish\nPublish or Perish is a neat piece of software, that helps documenting your literature review process. It provides a unified interface to a majority of databases. Each search can be saved, multiple searches can be organized into folders. Additionally, the results can be exported to different formats. Thus, Publish or Perish is also a good starting point for AI-Assisted literature reviews.\n\n\n\n\n\n\n\nConnected Papers\nConnected Papers is one of my favorite tools for literature reseraches. Paste any DOI into the search field and the tool will create a graph of the article, linking the cited literature as well as incorporating newer literature that cites the work that you‚Äôve been looking for. Using colors and node sizes all data is visualised neatly.\n\n\n\nAI Tools\nOver the past months, several AI Literature Review tools have been released:\n\nPerplexity incorporates GPT-3 (GPT-4 and Claude-2 in the pro version) and offers a chat interface. You can ask any question, it starts answering your question based on sources which are provided in the interface.\nElicit works somewhat differently, it expects you to ask a research question and tries to answer you question based on papers and has the ability to extract different type of information from papers automatically. In my experience the system does not work that well for social science questions.\nChatPDF is one of many tools that allow to upload PDF files, process them, and allow to chat with their content. In my experience it works rather well. However, as with all AI tools, we should be careful to manually verify the responses. The tool returns a link to the text anchor it refers to for answers. Overall, I recommend this tool for refinding information in papers that you‚Äôve already read, or as a companion for skimming papers ‚Äì although you might miss out important information!\nLangChain and LlamaIndex are python package that help building applications like ChatPDF yourself.\n\n\n\n\nReferences\n\n\nAmancio, Marina. 2017. ‚Äú‚ÄòPut it in your Story‚Äô: Digital Storytelling in Instagram and Snapchat Stories.‚Äù PhD thesis. https://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1111663&dswid=-5700.\n\n\nBainotti, Lucia, Alessandro Caliandro, and Alessandro Gandini. 2020. ‚ÄúFrom archive cultures to ephemeral content, and back: Studying Instagram Stories with digital methods.‚Äù New Media & Society, September, 1461444820960071. https://doi.org/10.1177/1461444820960071.\n\n\nBast, Jennifer. 2021. ‚ÄúPoliticians, Parties, and Government Representatives on Instagram: A Review of Research Approaches, Usage Patterns, and Effects.‚Äù Review of Communication Research 9 (July). https://www.rcommunicationr.org/index.php/rcr/article/view/108.\n\n\nHa√üler, J√∂rg, Anna Sophie K√ºmpel, and Jessica Keller. 2021. ‚ÄúInstagram and political campaigning in the 2017 German federal election. A quantitative content analysis of German top politicians‚Äô and parliamentary parties‚Äô posts.‚Äù Information, Communication and Society, July, 1‚Äì21. https://doi.org/10.1080/1369118X.2021.1954974.\n\n\nHochman, Nadav, and Lev Manovich. 2013. ‚ÄúZooming into an Instagram City: Reading the local through social media.‚Äù First Monday, June. https://doi.org/10.5210/fm.v18i7.4711.\n\n\nHu, Yuheng, Lydia Manikonda, and Subbarao Kambhampati. 2014. ‚ÄúWhat We Instagram: A First Analysis of Instagram Photo Content and User Types.‚Äù Proceedings of the International AAAI Conference on Web and Social Media 8 (1): 595‚Äì98. https://doi.org/10.1609/icwsm.v8i1.14578.\n\n\nLeaver, Tama, Tim Highfield, and Crystal Abidin. 2020. Instagram: Visual Social Media Cultures. John Wiley & Sons.\n\n\nLi, Bo, Olan K M Scott, Michael L Naraine, and Brody J Ruihley. 2021. ‚ÄúTell Me a Story: Exploring Elite Female Athletes‚Äô Self-Presentation via an Analysis of Instagram Stories.‚Äù Journal of Interactive Advertising 21 (2): 108‚Äì20. https://doi.org/10.1080/15252019.2020.1837038.\n\n\nMayer-Sch√∂nberger, Viktor. 2011. Delete: The Virtue of Forgetting in the Digital Age. Princeton University Press.\n\n\nMilanesi, Matilde, and Simone Guercini. 2020. ‚ÄúImage-based Social Media and Visual Content Analysis: Insights from a Literature Review.‚Äù Micro & Macro Marketing, no. 3: 537‚Äì58. https://ideas.repec.org/a/mul/jyf1hn/doi10.1431-97640y2020i3p537-558.html.\n\n\nOmena, Janna Joceli, Elaine Teixeira Rabello, and Andr√© Goes Mintz. 2020. ‚ÄúDigital Methods for Hashtag Engagement Research.‚Äù Social Media + Society 6 (3): 2056305120940697. https://doi.org/10.1177/2056305120940697.\n\n\nRejeb, Abderahman, Karim Rejeb, Alireza Abdollahi, and Horst Treiblmaier. 2022. ‚ÄúThe big picture on Instagram research: Insights from a bibliometric analysis.‚Äù Telematics and Informatics 73 (September): 101876. https://doi.org/10.1016/j.tele.2022.101876.\n\n\nRettberg, Jill Walker. 2018. ‚ÄúSnapchat: Phatic Communication and Ephemeral Social Media.‚Äù In Appified: Culture in the Age of Apps, edited by Jeremy Wade Morris and Sarah Murray, 188‚Äì95. ‚ÄúUniversity of Michigan Press.‚Äù\n\n\nS√°nchez-Querubƒ±ÃÅn, Natalia, Shuaishuai Wang, Briar Dickey, and Andrea Benedetti. 2023. ‚ÄúPolitical TikTok: Playful performance, ambivalent critique and event-commentary.‚Äù In The Propagation of Misinformation in Social Media, edited by Richard Rogers, 187‚Äì206. A Cross-Platform Analysis. Amsterdam University Press. https://doi.org/10.2307/jj.1231864.12.\n\n\nTowner, Terri L, and Caroline Lego Mu√±oz. 2022. ‚ÄúA Long Story Short: An Analysis of Instagram Stories during the 2020 Campaigns.‚Äù Journal of Political Marketing, July, 1‚Äì14. https://doi.org/10.1080/15377857.2022.2099579.\n\n\nV√°zquez-Herrero, Jorge, Sabela Direito-Rebollal, and Xos√© L√≥pez-Garcƒ±ÃÅa. 2019. ‚ÄúEphemeral Journalism: News Distribution Through Instagram Stories.‚Äù Social Media + Society 5 (4): 2056305119888657. https://doi.org/10.1177/2056305119888657."
  },
  {
    "objectID": "getting-started/related-work.html#footnotes",
    "href": "getting-started/related-work.html#footnotes",
    "title": "Related Work",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAccessible via VPN / on campus (when connected to eduroam).‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on Computational Social Media Research",
    "section": "",
    "text": "Welcome to this collection of notes on social media analysis with a special focus on computational methods. It is a work-in-progress website, created as part of my PhD project and teaching at the Media Informatics Group at the University of Regensburg, Germany. My name is Michael Achmann-Denkler and I‚Äôm currently experimenting with computational approaches for multimodal analysis of social media content, like Instagram posts and stories. My aim for this website is to develop a collection of notes exploring various methodologies, techniques, and tools for social media research. As a first milestone, the website will accompany my research seminar Computational Analysis of Visual Social Media in the 2023/24 winter semester."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Notes on Computational Social Media Research",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\nDate\nContent\n\n\n\n\n16.10.23\nCourse Organization\n\n\n23.10.23\nIntroduction to Social Media Analysis\n\n\n30.10.23\nProjects & Groups,  Getting Started: Tools\n\n\n06.11.23\nData Collection: IG Posts & Stories\n\n\n13.11.23\nData Collection: TikTok\n\n\n20.11.23\n- entfallen -\n\n\n27.11.23\nData Preprocessing: OCR & Whisper\n\n\n04.12.23\nExploration of Textual Data using GPT\n\n\n11.12.23\nOperationalization I & Computational Text Classification using GPT\n\n\n18.12.23\nData Annotation: LabelStudio & Annotation Guides\n\n\n08.01.24\nEvaluation I: Optimizing Text Classification\n\n\n15.01.24\nExploration of Visual Data\n\n\n22.01.24\nOperationalization II & Computational Image Classification using CLIP\n\n\n29.01.24\nEvaluation II: Optimizing Image Classification\n\n\n05.02.24\nData Analysis as a Conversation: Exploring trends using ChatGPT Visual Presentation of your Data & Results: RAWGraphs and more."
  },
  {
    "objectID": "index.html#citation-and-licences",
    "href": "index.html#citation-and-licences",
    "title": "Notes on Computational Social Media Research",
    "section": "Citation and Licences",
    "text": "Citation and Licences\nThe website repository is available on GitHub and registered with Zenodo . Please use the citation data provided by Zenodo when quoting parts of this website in academic work. Code examples and computational notebooks are published on the supplement repository, which is also registered with Zenodo . All text content on this website is published under the creative commons attribution (CC-BY) license. All code is released under the GNU GPLv3."
  },
  {
    "objectID": "processing/index.html",
    "href": "processing/index.html",
    "title": "Text as Data",
    "section": "",
    "text": "The analysis of texutal data has a long tradition under the term Natural Language Processing (NLP). As noted by Bengfort, Bilbro, and Ojeda (2018), ‚ÄúLanguage is unstructured data that has been produced by people to be understood by other people‚Äù. This characterization of language as unstructured data highlights its contrast with structured or semi-structured data. Unlike structured data, which is organized in a way that computers can easily parse and analyze, unstructured data like language requires more complex methods to be processed and understood. In the context of e.g.¬†Instagram, CrowdTangle exports contain structured data columns such as ‚ÄòUser Name‚Äô, ‚ÄòLike Count‚Äô, or ‚ÄòComment Count‚Äô. These pieces of data are quantifiable and can be easily sorted, filtered, or counted, e.g.¬†using tools like Excel or Python‚Äôs pandas library. For instance, we can quickly determine the most active users by counting the number of rows associated with each username. In contrast, unstructured data is not organized in a predefined manner and is typically more challenging to process and analyze. The ‚ÄòDescription‚Äô column in our dataset, which contains the captions of Instagram posts, is a prime example of unstructured data. These captions, composed of paragraphs or sentences, require different analytical approaches to extract meaningful insights. Unlike structured data, we cannot simply count or sort these texts in a straightforward manner. In our context, we often refer to the collection of texts we analyze as a ‚ÄúCorpus‚Äù. Each individual piece of text is called a ‚ÄúDocument‚Äù. Each document can be broken down into smaller units known as ‚Äúfeatures‚Äù. Features can be words, phrases, or even patterns of words, which we then use to quantify and analyze the text (compare p.¬†230 Haim 2023). For the goal of our research seminar, we can follow the three technical perspectives inspired by Haim (2023): 1. Frequency Analysis, 2. Contextual Analysis, and 3. Content Analysis."
  },
  {
    "objectID": "processing/index.html#schedule",
    "href": "processing/index.html#schedule",
    "title": "Text as Data",
    "section": "Schedule",
    "text": "Schedule\n\nIn our first session, we begin with frequency analyses of our corpus, which involves counting words or phrases to identify the most common elements. This method provides a foundational understanding of the prominent themes or topics. Additionally, we learn to convert embedded text in images and videos into machine-readable format, using OCR, and automated audio transcription.\nNext, we will engage in explorative text analysis. This step enhances our understanding of the corpus and lays the groundwork for quantitative content analysis. We plan to utilize tools like GPT (and possibly BERTopic for an in-depth exploration of our documents.\nFinally, we move towards more complex methods like classification or coding. These techniques allow us to categorize text into predefined groups or themes, enabling a more nuanced and quantitative understanding of the content. By applying these methods, we can, for example, classify Instagram captions into categories such as ‚Äòpromotional‚Äô, ‚Äòpersonal‚Äô, ‚Äòinformative‚Äô, etc., based on their content and context."
  },
  {
    "objectID": "processing/index.html#hands-on",
    "href": "processing/index.html#hands-on",
    "title": "Text as Data",
    "section": "Hands-On",
    "text": "Hands-On\nWe are working with Python and pandas, our data is structured in tables, also known as DataFrames. Each DataFrame (df) consists of rows and columns. We can store and structure data differently using these two dimensions, one concept for storing research data using tables is Tidy Data (Wickham 2014). According to this standard\n\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\n\n\nVisualization of the tidy data components, source: R for Data Science.\n\n\nWhat, in context of social media data, is an obersavtion? Is it a post? I suggest to start by seeing posts as observations, i.e.¬†rows. Thus, we have one table for our corpus, consisting of one row per post with multiple columns for different variables, including an ID, possibly a link, a referrence to the image / video, and one or more text variables for each post. When dealing with Instagram or TikTok posts, we might have three text columns: caption / description, OCR, and transcription. When dealing with stories two: OCR and transcription.\n\n\n\n\n\n\nNote\n\n\n\nWhen dealing with more complex data, e.g.¬†Instagram albums that may contain multiple images per post, we will have to reconsider this choice. In this case we might consider each observation to be one image / video, which has variables like OCR and transcription. Keeping the ID column for images and videos, we have a fixed reference to the original post, thus we may re-merge the data later on with the post metadata or combine variables across media for one post.\n\n\nAll data exported from CrowdTangle, 4CAT, and Zeeschuimer-F are saved as CSV files. Throughout the semester, we keep using this file format to save our progress. We work with multiple Jupyter notebooks, generally one notebook per task. This helps to keep a good structure of our projects. Each time we modified the df, we save the CSV file to our Google Drive / Harddrive. In the two examples below we add an OCR and a Transcription column to our DataFrame, for each task we use one notebook. After completing each task, we store the results in a file. While Google Drive provides file versioning to mitigate data loss in certain scenarios, I recommend to save your results to a new file during the experimental phase. This practice ensures data safety until you have fully verified the functionality of your code. Additionally, I recommend naming your files in a YYYY-MM-DD-descriptive-name.csv fashion. When working with colab notebooks I recommend to keep track of notebooks using notes / lists, e.g.¬†using the Dataloom plugin for Obsidian.\n\n\n\nKeeping track of Colab notebooks with Obsidian and the Dataloom plugin.\n\n\nThe CSV files contain only metadata, the actual media files (images / videos) are saved to different locations. The OCR and Transcription notebooks below contain code to import media files from 4CAT and Zeeschuimer-F. I suggest to save the files to media/videos or media/images. Both notebooks introduce a column image_file or video_file where the relative location of the media files is written to. Creating a new ZIP file using the new folder structure and saving the file to Google Drive allows us to use the media files in future notebooks (e.g.¬†for image classification) without modifying the image_file or video_file columns again.\n\n\n\n\n\n\nNote\n\n\n\nThis page and all referenced notebooks deal with 4CAT and Zeeschuimer-F metadata and media files. Generally all information applies to instaloader as well. Its advisable to use the --filename-pattern command line parameter to control the filename of the media files. Mapping JSON metadata to actual media objects becomes easier this way. Once all posts / stories have been loaded using instaloader, I recommend to read all JSON files in a loop and create a DataFrame (see Data Collection / Posts / Instaloader for more information and code examples).\n\n\nKey Take-Aways\n\nWe organize our data inspired by TidyData\n\nOne row per post\nOne column per variable\n\nWe use one notebook per task\nWe save our progress to CSV files, either on our harddrive or Google Drive\nWe keep a reference to media files as a relative reference in our DataFrame\nWe keep our media files in the structure media/videos, and media/images, which we compress to ZIP and keep on our Google Drive (or central HDD location)\nWhen working with experimental code, keep backups of your data file, do not overwrite the original file!"
  },
  {
    "objectID": "processing/index.html#from-images-videos-to-text",
    "href": "processing/index.html#from-images-videos-to-text",
    "title": "Text as Data",
    "section": "From Images / Videos to Text",
    "text": "From Images / Videos to Text\nComputational approaches for text analyses are established as part of computational sociales science research (Baden et al. 2022), which we may utilize when dealing with visual and multimodal social media. Instagram posts often contain embedded text, TikTok posts often contain an audio layer, both of which we can transform to computer readable text. For the first, we are going to use OCR, for the second we apply Whisper. The following subchapters demonstrate the application of these technique in order to extract textual content from images and videos. In the thirs subchapter, I demonstrate a simple application of corpus analytics for a first analysis of the social media content based on word frequencies.\n\nOCR\n\nWe‚Äôre using easyocr. See the documentation for more complex configurations. Using CPU only this process takes from minutes to hours (depends on the amount of images). OCR may also be outsourced (e.g.¬†using Google Vision API), see future sessions (and Memespector) for this.\n\n!pip -q install easyocr\n\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.9/2.9 MB 29.7 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 908.3/908.3 kB 57.5 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 307.2/307.2 kB 29.6 MB/s eta 0:00:00\n\n\n\n# Imports for OCR\nimport easyocr\nreader = easyocr.Reader(['de','en'])\n\nProgress: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% CompleteProgress: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% Complete\n\n\nWe define a very simple method to receive one string for all text recognized: The readtextmethod returns a list of text areas, in this example we concatenate the string, therefore the order of words is sometimes not correct.\nAlso, we save the file to Google Drive to save our results.\n\ndef run_ocr(image_path):\n    ocr_result = reader.readtext(image_path, detail = 0)\n    ocr_text = \" \".join(ocr_result)\n    return ocr_text\n\ndf['ocr_text'] = df['image_file'].apply(run_ocr)\n\n# Saving Results to Drive\ndf.to_csv('/content/drive/MyDrive/2022-11-09-Stories-Exported.csv')\n\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0.1\nUnnamed: 0\nID\nTime of Posting\nType of Content\nvideo_url\nimage_url\nUsername\nVideo Length (s)\nExpiration\n...\nIs Verified\nStickers\nAccessibility Caption\nAttribution URL\nvideo_file\naudio_file\nduration\nsampling_rate\nimage_file\nocr_text\n\n\n\n\n0\n0\n0\n3234500408402516260_1383567706\n2023-11-12 15:21:53\nImage\nNaN\nNaN\nnews24\nNaN\n2023-11-13 15:21:53\n...\nTrue\n[]\nPhoto by News24 on November 12, 2023. May be a...\nhttps://www.threads.net/t/CzjB80Zqme0\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234500408402516260_1383567706.jpg\nKeee WEEKEND NEWS24 PLUS: TESTING FORDS RANGER...\n\n\n1\n1\n1\n3234502795095897337_8537434\n2023-11-12 15:26:39\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:26:39\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234502795095897337_8537434.jpg\nDieses Auto ist einfach der Horror Du glaubst ...\n\n\n2\n2\n2\n3234503046678453705_8537434\n2023-11-12 15:27:10\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:27:10\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234503046678453705_8537434.jpg\nTouchdown bei Taylor Swift und Travis Kelce De...\n\n\n3\n3\n3\n3234503930728728807_8537434\n2023-11-12 15:28:55\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:28:55\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234503930728728807_8537434.jpg\nHorror-Diagnose f√ºr Barton Cowperthwaite Netfl...\n\n\n4\n4\n4\n3234504185910204562_8537434\n2023-11-12 15:29:25\nImage\nNaN\nNaN\nbild\nNaN\n2023-11-13 15:29:25\n...\nTrue\n[]\nPhoto by BILD on November 12, 2023. May be an ...\nNaN\nNaN\nNaN\nNaN\nNaN\nmedia/images/3234504185910204562_8537434.jpg\n3v Bilde GG JJ Besorgniserregende Ufo-Aktivit√§...\n\n\n\n\n\n5 rows √ó 21 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nSource: OCR using easyocr\n\n\nAutomated Audio Transcription (Whisper)\n\nExtract Audio from Video File\nAfter loading the metadta and media files from the Google Drive, we extract the audio from each video file to prepare the automated transcription.\n\n!pip install -q moviepy\n\n\nimport os\n\n# Set audio directory path\naudio_path = \"media/audio/\"\n\n# Check if the directory exists\nif not os.path.exists(audio_path):\n    # Create the directory if it does not exist\n    os.makedirs(audio_path)\n\n\nfrom moviepy.editor import *\n\nfor index, row in df.iterrows():\n    if row['video_file'] != \"\":\n        # Load the video file\n        video = VideoFileClip(row['video_file'])\n        filename = row['video_file'].split('/')[-1]\n\n        # Extract the audio from the video file\n        audio = video.audio\n\n        if audio is not None:\n            sampling_rate = audio.fps\n            current_suffix = filename.split(\".\")[-1]\n            new_filename = filename.replace(current_suffix, \"mp3\")\n\n            # Save the audio to a file\n            audio.write_audiofile(\"{}{}\".format(audio_path, new_filename))\n        else:\n            new_filename = \"No Audio\"\n            sampling_rate = -1\n\n        # Update DataFrame inplace\n        df.at[index, 'audio_file'] = new_filename\n        df.at[index, 'duration'] = video.duration\n        df.at[index, 'sampling_rate'] = sampling_rate\n\n        df.at[index, 'video_file'] = row['video_file'].split('/')[-1]\n\n        # Close the video file\n        video.close()\n\nMoviePy - Writing audio in media/audio/CzD93SEIi-E.mp3\nMoviePy - Done.\n\n\nWe‚Äôve extracted the audio content of each video file to a mp3 file in the media/audio folder. The files keep the name of the video file. We added new columns to the metadata for audio duration and sampling_rate. In case the video did not include an audio file, smapling_rateis set to -1, which we use to filter the df when transcribing the files.\n\ndf[df['video_file'] != \"\"].head()\n\n\n  \n    \n\n\n\n\n\n\nid\nthread_id\nparent_id\nbody\nauthor\nauthor_fullname\nauthor_avatar_url\ntimestamp\ntype\nurl\n...\nnum_comments\nnum_media\nlocation_name\nlocation_latlong\nlocation_city\nunix_timestamp\nvideo_file\naudio_file\nduration\nsampling_rate\n\n\n\n\n4\nCzD93SEIi-E\nCzD93SEIi-E\nCzD93SEIi-E\nMitzuarbeiten f√ºr unser Land, Bayern zu entwic...\nmarkus.soeder\nMarkus SoÃàder\nhttps://scontent-fra3-1.cdninstagram.com/v/t51...\n2023-10-31 12:06:23\nvideo\nhttps://www.instagram.com/p/CzD93SEIi-E\n...\n227\n1\nNaN\nNaN\nNaN\n1698753983\nCzD93SEIi-E.mp4\nCzD93SEIi-E.mp3\n67.89\n44100.0\n\n\n\n\n\n1 rows √ó 24 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\nLet‚Äôs update the ZIPed folder to include the audio files.\n\n!zip -r /content/drive/MyDrive/2023-11-24-4CAT-Images-Clean.zip media\n\nupdating: media/ (stored 0%)\nupdating: media/videos/ (stored 0%)\nupdating: media/videos/CzD93SEIi-E.mp4 (deflated 0%)\n  adding: media/audio/ (stored 0%)\n  adding: media/audio/CzD93SEIi-E.mp3 (deflated 1%)\n\n\nAnd save the updated metadata file. Change filename when importing stories here!\n\ndf.to_csv(four_cat_file_path)\n\nTranscriptions using Whisper\n\nThe Whisper model was proposed in Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.\n\n\nThe abstract from the paper is the following:\n\n\n\nWe study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.\n\n\n‚Äì https://huggingface.co/docs/transformers/model_doc/whisper\n\n!pip install -q transformers\n\nThe next code snippet initializes the Whisper model. The transcribe_aduio method is applied to each row of the dataframe where sampling_rate &gt; 0, thus only to those lines with referencees to audio files. Each audio file is transcribed using Whisper, the result, one text string, is saved to the transcript column.\nAdjust the language variable according to your needs! The model is also capable of automated translation, e.g.¬†setting language to english when processing German content results in an English translation of the speech. (Additionally, the task variable accepts translate).\n\nimport torch\nfrom transformers import pipeline, WhisperProcessor, WhisperForConditionalGeneration\nimport librosa\n\n# Set device to GPU if available, else use CPU\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize the Whisper model pipeline for automatic speech recognition\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=\"openai/whisper-large\",\n    chunk_length_s=30,\n    device=device,\n)\n\n# Load model and processor for multilingual support\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n\n# Function to read, transcribe, and handle longer audio files in different languages\ndef transcribe_audio(filename, language='german'):\n    try:\n        # Load and resample audio file\n        audio_path = f\"{audio_folder}/{filename}\"\n        waveform, original_sample_rate = librosa.load(audio_path, sr=None, mono=True)\n        waveform_resampled = librosa.resample(waveform, orig_sr=original_sample_rate, target_sr=16000)\n\n        # Get forced decoder IDs for the specified language\n        forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=\"transcribe\")\n\n        # Process the audio file in chunks and transcribe\n        transcription = \"\"\n        for i in range(0, len(waveform_resampled), 16000 * 30):  # 30 seconds chunks\n            chunk = waveform_resampled[i:i + 16000 * 30]\n            input_features = processor(chunk, sampling_rate=16000, return_tensors=\"pt\").input_features\n            predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n            chunk_transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n            transcription += \" \" + chunk_transcription\n\n        return transcription.strip()\n    except Exception as e:\n        print(f\"Error processing file {filename}: {e}\")\n        return \"\"\n\n\n# Filter the DataFrame (sampling_rates &lt; 0 identify items without audio)\nfiltered_index = df['sampling_rate'] &gt; 0\n\n# Apply the transcription function to each row in the filtered DataFrame\ndf.loc[filtered_index, 'transcript'] = df.loc[filtered_index, 'audio_file'].apply(transcribe_audio)\n\n\ndf[df['video_file'] != \"\"].head()\n\n\n  \n    \n\n\n\n\n\n\nid\nthread_id\nparent_id\nbody\nauthor\nauthor_fullname\nauthor_avatar_url\ntimestamp\ntype\nurl\n...\nnum_media\nlocation_name\nlocation_latlong\nlocation_city\nunix_timestamp\nvideo_file\naudio_file\nduration\nsampling_rate\ntranscript\n\n\n\n\n4\nCzD93SEIi-E\nCzD93SEIi-E\nCzD93SEIi-E\nMitzuarbeiten f√ºr unser Land, Bayern zu entwic...\nmarkus.soeder\nMarkus SoÃàder\nhttps://scontent-fra3-1.cdninstagram.com/v/t51...\n2023-10-31 12:06:23\nvideo\nhttps://www.instagram.com/p/CzD93SEIi-E\n...\n1\nNaN\nNaN\nNaN\n1698753983\nCzD93SEIi-E.mp4\nCzD93SEIi-E.mp3\n67.89\n44100.0\nIch bitte auf den abgelagerten Vortrag der Maa...\n\n\n\n\n\n1 rows √ó 25 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\n\ndf.loc[4, 'transcript']\n\n'Ich bitte auf den abgelagerten Vortrag der Maa√üen-S√∂der-Entf√ºhlen ein.  Erf√ºllung meiner Amtspflichten, so wahr mir Gott helfe. Ich schw√∂re Treue der Verfassung des Freistaates Bayern, Gehorsam den Gesetzen und gewissenhafte Erf√ºllung meiner Amtspflichten, so wahr mir Gott helfe. Herr Ministerpr√§sident, ich darf Ihnen im Namen des ganzen Hauses ganz pers√∂nlich die herzlichsten Gl√ºckw√ºnsche aussprechen und w√ºnsche Ihnen viel Erfolg und gute Nerven auch bei Ihrer Aufgabe. Herzlichen Dank.  Applaus'\n\n\nOverall, the transcriptions work well. The first sentence above, however, shows that we still can expect misinterpretations.\nSource: Transcription using Whisper\n\n\nAnalyzing Corpus and Word Frequencies\n\nAmong a variety of possibilities, we can, for example, look at the frequencies of the words contained in the corpus or examine the corpus for recurring themes it contains.\nFirst we need to import all the required libraries once again. The Natural Language Toolkit (NLTK) gives us access to a variety of natural language processing functions (e.g.¬†tokenisation, stop word removal, part-of-speech tagging, ‚Ä¶).\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport requests\nimport pandas as pd\n\nWhen analysing word frequencies, we can use stop word lists to ignore words that occur frequently but are not relevant to us. We can easily download such a list. However, this can also be individually adapted to the purpose.\n\n# Retrieve Stopwords from Github\nsw_json = requests.get('https://github.com/stopwords-iso/stopwords-de/raw/master/stopwords-de.json')\n\nNow we can tokenise the existing text, remove the stop words or punctuation marks they contain, convert the words to lower case, or use bi-grams in addition to single-word tokens.\nWe then sum up the occurrences of the individual words and make the results available in a DataFrame.\n\ndef word_freq(text, punctuation=False, stop_words = False, lowercasing = False, bigrams = False):\n\n    if punctuation:\n        # Tokenizing, removing punctuation\n        tokens = RegexpTokenizer(r'\\w+').tokenize(text) # https://regexr.com/\n    else:\n        # Tokenizing, w/o removing punctuation\n        # tokens = text.split()\n        tokens = word_tokenize(text)\n\n    if stop_words:\n        # Removing Stopwords\n        tokens = [w for w in tokens if not w.lower() in stop_words]\n\n    if lowercasing:\n        # Lower-Casing\n        tokens = [w.lower() for w in tokens]\n\n    if bigrams:\n        # Converting text tokens into bigrams\n        tokens = nltk.bigrams(tokens)\n\n    # Creating Data Frame\n    freq = nltk.FreqDist(tokens) # display(freq)\n    df = pd.DataFrame.from_dict(freq, orient='index')\n    df.columns = ['Frequency']\n    df.index.name = 'Term'\n\n    # Here we calculate the total number of tokens in our Frequency List\n    total_tokens = sum(freq.values()) # sum([2,3,4,5,6])\n\n    # Here we add a new column `Relative` (*100 for percentage)\n    df['Relative'] = (df['Frequency'] / total_tokens) * 100\n\n    return df\n\n\nfrom pathlib import Path\nimport os\n\n#@markdown Do you want bigrams included?\nbigrams = True #@param {type:\"boolean\"}\n\n#@markdown Should all words get lower cased before counting the occurances?\nlowercasing = True #@param {type:\"boolean\"}\n\n#@markdown Do you want to exclude stopwords in your result list?\nstopwords = True #@param {type:\"boolean\"}\n\n#@markdown Do you want to remove punctuation before counting the occurances?\npunctuation = True #@param {type:\"boolean\"}\n\n\n# Load stopwords file if necessary\nif stopwords:\n    stopwords = sw_json.json()\n\n# Read source file and concat all texts\ntext = ' '.join(list(df[text_column]))\n\n# Call word_freq() with specified parameters\ndf_freq = word_freq(text, punctuation = punctuation, stop_words = stopwords, lowercasing = lowercasing, bigrams = bigrams)\n\n# Sort results for descending values\ndf_freq = df_freq.sort_values(\"Relative\", ascending = False)\n\ndisplay(df_freq[0:10])\n\n\n  \n    \n\n\n\n\n\n\nFrequency\nRelative\n\n\nTerm\n\n\n\n\n\n\n(j√ºdisches, leben)\n5\n1.259446\n\n\n(allerheiligen, allerseelen)\n4\n1.007557\n\n\n(ilse, aigner)\n3\n0.755668\n\n\n(bayerischer, landtag)\n3\n0.755668\n\n\n(klare, haltung)\n2\n0.503778\n\n\n(w√ºnschen, einfach)\n2\n0.503778\n\n\n(vaters, freundschaftliche)\n2\n0.503778\n\n\n(tod, vaters)\n2\n0.503778\n\n\n(g√ºnter, tod)\n2\n0.503778\n\n\n(schwiegervater, g√ºnter)\n2\n0.503778\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWordcloud\nOne way to visualise word frequencies and recurring themes of texts are word clouds. These basically show the most frequently occurring words in the text (similar to the table created earlier), but more frequently occurring words are depicted larger than less frequently occurring words.\nFirst, we have to install the necessary library wordcloud.\n\n!pip install -q wordcloud\n\nThe actual implementation of this approach is relatively simple. We need to combine all the texts into a single text, as we did in the previous step with the frequency analysis, and pass it to the imported library.\n\nfrom wordcloud import WordCloud, STOPWORDS\n\ndef generate_wordcloud(text, path):\n\n    text = ' '.join(list(text))\n\n    # Generate a word cloud image\n    wordcloud = WordCloud(background_color=\"white\",width=1920, height=1080).generate(text)\n\n    # Dazugeh√∂rige Grafik erstellen\n    plt.imshow(wordcloud, interpolation=\"bilinear\") # Aufl√∂sung/Interpolation der Grafik\n    plt.axis(\"off\")\n    plt.figtext(0.5, 0.1, wordcloud_subcaption, wrap=True, horizontalalignment='center', fontsize=12)\n    plt.savefig(path, dpi=300)\n    plt.show()\n\nOnce again, we have the option of adjusting various parameters. Remember to specify the right file path, file name and column of your text data!\n\n#@markdown Input for additional stopwords; whitespace separated\nstopwords_extension_wc = '' #@param {type: \"string\"}\n\n#@markdown Subcaption for the wordcloud, leave blank to ignore\nwordcloud_subcaption = 'Markus S\\xF6der' #@param {type: \"string\"}\n\nNow all we have to do is load the stop word file, add our own additions and then trigger the creation of the word cloud using the function we created at the beginning.\nThe result image is saved in the defined data_path.\n\nimport matplotlib.pyplot as plt\nimport requests\n\n# Retrieve Stopwords from Github\nr = requests.get('https://github.com/stopwords-iso/stopwords-de/raw/master/stopwords-de.json')\nstop_words = r.json()\n\n# Convert input into list\nstopwords_extension_wc_list = stopwords_extension_wc.split(' ')\nstop_words.extend(stopwords_extension_wc_list)\n\n# Stopw√∂rter in die WordCloud laden\nSTOPWORDS.update(stop_words)\n\n\ngenerate_wordcloud(df[text_column], 'wordcloud.png')\n\n\n\n\nSource: Introduction to Corpus Analysis"
  },
  {
    "objectID": "processing/index.html#conclusion",
    "href": "processing/index.html#conclusion",
    "title": "Text as Data",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, this session provides us with the practical skills to use Python, pandas, and Jupyter notebooks for the computational analysis of multimodal social media data. Our adherence to Tidy Data principles and the integration of technologies like OCR and Whisper are integral to extract and analyze textual content from multimedia sources. In the next session we will keep exploring the content through a textual lens. Further, we will use prompting as a technique to classify texts as part of a computational content analysis."
  },
  {
    "objectID": "processing/index.html#more-resources",
    "href": "processing/index.html#more-resources",
    "title": "Text as Data",
    "section": "More Resources",
    "text": "More Resources\nPython & Computational Social Sciences\n\nPython for Computational Social Science and Digital Humanities (YouTube)\nIntroduction to Computational Social Science methods with Python (Online)\nIntroduction to Data Science: A Python Approach to Concepts, Techniques and Applications (E-Book)\nR for Data Science (2nd edition) ‚Äì not Python, but the principles can easily be migrated to pandas.\n\nPython & NLP\n\nNatural Language Processing (Notebook, GESIS CSS)\nWord Frequencies (Online)\nIntroduction Jupyter Notebooks (Online)\nKonchady (2016): Text Mining Application Programming (Somewhat older, still an interesting reading for the basics of computational corpus analysis)"
  },
  {
    "objectID": "processing/exploration.html",
    "href": "processing/exploration.html",
    "title": "Data Import",
    "section": "",
    "text": "In the previous session we talked about text as data, suggesting that text data offers a rich source of insights. This chapter concentrates on the advanced tools for exploring these textual dimensions: BERTopic and the Generative Pre-trained Transformer (GPT). These technologies stand at the forefront of computational text analysis and are intersting tools to unlock the meanings and patterns hidden within the vast textual content of social media."
  },
  {
    "objectID": "processing/exploration.html#topic-modeling-with-bertopic",
    "href": "processing/exploration.html#topic-modeling-with-bertopic",
    "title": "Data Import",
    "section": "Topic Modeling with BERTopic",
    "text": "Topic Modeling with BERTopic\n\n\n\nBERTopic (Grootendorst 2022) is a transformer-based topic modeling tool. It uses the BERT (Bidirectional Encoder Representations from Transformers) framework, an advanced method for natural language processing (NLP) that understands the context of words in text. BERTopic is adept at identifying and clustering topics within short text documents Egger and Yu (2022), making it an interesting tool to analyze and categorize text data from social media. The author is actively working on the documentation and keeps improving the topic modeling technique to adapt the latest advances of Large Language Models (LLMs), just recently a Zero-Shot topic modeling approach has been added. I have used BERTopic for a first exploration of stories and posts published by politicians and parties during the 2021 Federal Election in Germany (Achmann and Wolff 2023). Past research has used LDA, another topic modeling algorithm, to explore themes and topics in Instagram posts by politicians (Rodina and Dligach 2019).\n\nFor this example we import a CrowdTangle dataframe, which has been preprocessing using the OCR Notebook. We are only dealing with one image per post, there are no videos (= no transcriptions). In this example, we have up to two text columns per Post, Description which contains the caption, and ocr_text. When exploring the textual content of the posts, we see each of those columns as one document. Thus, we transform our table and create new_df as a Text Table that contains a reference to the post (shortcode), the actual Text, and a Text Type column.\n\nimport pandas as pd\n\ndf = pd.read_csv('/content/drive/MyDrive/2023-11-30-Export-Posts-Crowd-Tangle.csv')\n\nNext, we want to transform the DataFrame from one post per row, to one text document per row (Think tidydata!)\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nAccount\nUser Name\nFollowers at Posting\nPost Created\nPost Created Date\nPost Created Time\nType\nTotal Interactions\nLikes\n...\nPhoto\nTitle\nDescription\nImage Text\nSponsor Id\nSponsor Name\nOverperforming Score (weighted ‚Äî Likes 1x Comments 1x )\nshortcode\nimage_file\nocr_text\n\n\n\n\n0\n0\nFREIE WAÃàHLER Bayern\nfw_bayern\n9138\n2023-10-09 20:10:19 CEST\n2023-10-09\n20:10:19\nPhoto\n566\n561\n...\nhttps://scontent-sea1-1.cdninstagram.com/v/t51...\nNaN\n#Landtagswahl23 ü§©üß°üôè #FREIEW√ÑHLER #Aiwanger #Da...\nFREIE WAHLER 15,8 %\nNaN\nNaN\n2.95\nCyMAe_tufcR\nmedia/images/fw_bayern/CyMAe_tufcR.jpg\nFREIE WAHLER 15,8 %\n\n\n1\n1\nJunge Liberale JuLis Bayern\njulisbayern\n4902\n2023-10-09 19:48:02 CEST\n2023-10-09\n19:48:02\nAlbum\n320\n310\n...\nhttps://scontent-sea1-1.cdninstagram.com/v/t51...\nNaN\nDie Landtagswahl war f√ºr uns als Liberale hart...\nNaN\nNaN\nNaN\n1.41\nCyL975vouHU\nmedia/images/julisbayern/CyL975vouHU.jpg\nFreie EDP Demokraten BDB FDP FB FDP DANKE F√úR ...\n\n\n2\n2\nJunge Union Deutschlands\njunge_union\n44414\n2023-10-09 19:31:59 CEST\n2023-10-09\n19:31:59\nPhoto\n929\n925\n...\nhttps://scontent-sea1-1.cdninstagram.com/v/t39...\nNaN\nNach einem starken Wahlkampf ein verdientes Er...\nHERZLICHEN GL√úCKWUNSCH! Unsere JUler im bayris...\nNaN\nNaN\n1.17\nCyL8GWWJmci\nmedia/images/junge_union/CyL8GWWJmci.jpg\nHERZLICHEN GL√úCKWUNSCH! Unsere JUler im bayris...\n\n\n3\n3\nKatharina Schulze\nkathaschulze\n37161\n2023-10-09 19:29:02 CEST\n2023-10-09\n19:29:02\nPhoto\n1,074\n1009\n...\nhttps://scontent-sea1-1.cdninstagram.com/v/t51...\nNaN\nSo viele Menschen am Odeonsplatz heute mit ein...\nNaN\nNaN\nNaN\n1.61\nCyL7wyJtTV5\nmedia/images/kathaschulze/CyL7wyJtTV5.jpg\nJuo I W\n\n\n4\n4\nJunge Union Deutschlands\njunge_union\n44414\n2023-10-09 18:01:34 CEST\n2023-10-09\n18:01:34\nAlbum\n1,655\n1644\n...\nhttps://scontent-sea1-1.cdninstagram.com/v/t39...\nNaN\nHerzlichen Gl√ºckwunsch zu diesem grandiosen Wa...\nNaN\nNaN\nNaN\n2.34\nCyLxwHuvR4Y\nmedia/images/junge_union/CyLxwHuvR4Y.jpg\n12/12 der hessischen JU-Kandidaten ziehen in d...\n\n\n\n\n\n5 rows √ó 25 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe restructure df to focus on two key text-based columns: ‚ÄòDescription‚Äô and ‚Äòocr_text‚Äô. The goal is to create a streamlined DataFrame where each row corresponds to an individual text entry, either from the ‚ÄòDescription‚Äô or the ‚Äòocr_text‚Äô fields. To achieve this, we first split the original DataFrame into two separate DataFrames, one for each of these columns. We then rename these columns to ‚ÄòText‚Äô for uniformity. Additionally, we introduce a new column, ‚ÄòText Type‚Äô, to categorize each text entry as either ‚ÄòCaption‚Äô (originating from ‚ÄòDescription‚Äô) or ‚ÄòOCR‚Äô (originating from ‚Äòocr_text‚Äô). The ‚Äòshortcode‚Äô column is retained as a unique identifier for each entry. Finally, we concatenate these two DataFrames into a single DataFrame, ensuring a clean and organized structure. This restructured DataFrame facilitates easier analysis and processing of the text data, segregating it by source while maintaining a link to its original post via the ‚Äòshortcode‚Äô. The code also includes a step to remove any rows with empty or NaN values in the ‚ÄòText‚Äô column, ensuring data integrity and cleanliness.\n\nimport pandas as pd\n\n# Creating two separate dataframes\ndf_description = df[['shortcode', 'Description']].copy()\ndf_ocr_text = df[['shortcode', 'ocr_text']].copy()\n\n# Renaming columns\ndf_description.rename(columns={'Description': 'Text'}, inplace=True)\ndf_ocr_text.rename(columns={'ocr_text': 'Text'}, inplace=True)\n\n# Adding 'Text Type' column\ndf_description['Text Type'] = 'Caption'\ndf_ocr_text['Text Type'] = 'OCR'\n\n# Concatenating the dataframes\nnew_df = pd.concat([df_description, df_ocr_text])\n\n# Dropping any rows where 'Text' is NaN or empty\nnew_df.dropna(subset=['Text'], inplace=True)\nnew_df = new_df[new_df['Text'].str.strip() != '']\n\n# Resetting the index\nnew_df.reset_index(drop=True, inplace=True)\n\n\nnew_df.head()\n\n\n  \n    \n\n\n\n\n\n\nshortcode\nText\nText Type\n\n\n\n\n0\nCyMAe_tufcR\n#Landtagswahl23 ü§©üß°üôè #FREIEW√ÑHLER #Aiwanger #Da...\nCaption\n\n\n1\nCyL975vouHU\nDie Landtagswahl war f√ºr uns als Liberale hart...\nCaption\n\n\n2\nCyL8GWWJmci\nNach einem starken Wahlkampf ein verdientes Er...\nCaption\n\n\n3\nCyL7wyJtTV5\nSo viele Menschen am Odeonsplatz heute mit ein...\nCaption\n\n\n4\nCyLxwHuvR4Y\nHerzlichen Gl√ºckwunsch zu diesem grandiosen Wa...\nCaption\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nBERTopic\nAt this stage, the data is reading for Topic Modeling. We are using the BERTopic package and follow the tutorial notebook provided by the author.\n\n!pip install -q bertopic\n\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 154.1/154.1 kB 3.6 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.2/5.2 MB 25.0 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 90.9/90.9 kB 12.1 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 86.0/86.0 kB 11.0 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.3/1.3 MB 36.3 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 55.8/55.8 kB 7.3 MB/s eta 0:00:00\n  Building wheel for hdbscan (pyproject.toml) ... done\n  Building wheel for sentence-transformers (setup.py) ... done\n  Building wheel for umap-learn (setup.py) ... done\n\n\nIn the following cells we download a stopword dictionary for the German language and applied it according to the documentation\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nnltk.download('stopwords')\n\nSTOPWORDS = stopwords.words('german')\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=STOPWORDS)\n\nNow we‚Äôre ready to create our corpus in docs, a list of text documents to pass to BERTopic.\n\n# We create our corpus\ndocs = new_df['Text']\n\n\nfrom bertopic import BERTopic\n\n# We're dealing with German texts, therefore we choose 'multilingual'. When dealing with English texts exclusively, choose 'english'\ntopic_model = BERTopic(language=\"multilingual\", calculate_probabilities=True, verbose=True, vectorizer_model=vectorizer_model)\ntopics, probs = topic_model.fit_transform(docs)\n\n\n\n\nThe following cells have been copied from the BERTopic Tutorial. Please check the linked notebook for more functions and the documentation for more background information.\n\n\nExtracting Topics\nAfter fitting our model, we can start by looking at the results. Typically, we look at the most frequent topics first as they best represent the collection of documents.\n\nfreq = topic_model.get_topic_info(); freq.head(5)\n\n\n  \n    \n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nRepresentative_Docs\n\n\n\n\n0\n-1\n860\n-1_bayern_csu_uhr_mehr\n[bayern, csu, uhr, mehr, menschen, m√ºnchen, te...\n[Wir gehen mit #herzstatthetze in den Wahlkamp...\n\n\n1\n0\n137\n0_w√§hlen_fdp_hessen_heute\n[w√§hlen, fdp, hessen, heute, stimme, stimmen, ...\n[Unser Ministerpr√§sident @markus.soeder steigt...\n\n\n2\n1\n104\n1_energie_co2_klimaschutz_habeck\n[energie, co2, klimaschutz, habeck, wasserstof...\n[Habeck t√§uscht √ñffentlichkeit mit Zensur: R√ºc...\n\n\n3\n2\n103\n2_zuwanderung_migration_grenzpolizei_migration...\n[zuwanderung, migration, grenzpolizei, migrati...\n[Wir sagen Ja zu #Hilfe und #Arbeitsmigration,...\n\n\n4\n3\n89\n3_uhr_starke mitte_bayerns starke_bayerns\n[uhr, starke mitte, bayerns starke, bayerns, b...\n[\"Deutschland-Pakt\" aus Scholz der Krise komme...\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n-1 refers to all outliers and should typically be ignored. Next, let‚Äôs take a look at a frequent topic that were generated:\n\nlen(freq)\n\n52\n\n\nWe have a total of 52 topics\n\ntopic_model.get_topic(0)  # Select the most frequent topic\n\n[('w√§hlen', 0.01628736425293884),\n ('fdp', 0.01626632927971954),\n ('hessen', 0.013634118460503969),\n ('heute', 0.013441948777152065),\n ('stimme', 0.011907460231710654),\n ('stimmen', 0.011505832701270827),\n ('landtagswahl', 0.011272934711858047),\n ('wahlkampf', 0.01059385752962746),\n ('sonntag', 0.01057520846171656),\n ('bayern', 0.010322807358750668)]\n\n\n\nVisualize Topics\nAfter having trained our BERTopic model, we can iteratively go through perhaps a hundred topic to get a good understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. Instead, we can visualize the topics that were generated in a way very similar to LDAvis:\n\ntopic_model.visualize_topics()\n\n\n\n\nVisualize Terms\nWe can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation. Insights can be gained from the relative c-TF-IDF scores between and within topics. Moreover, you can easily compare topic representations to each other.\n\ntopic_model.visualize_barchart(top_n_topics=15)\n\n\n\n\nTopic Reduction\nWe can also reduce the number of topics after having trained a BERTopic model. The advantage of doing so, is that you can decide the number of topics after knowing how many are actually created. It is difficult to predict before training your model how many topics that are in your documents and how many will be extracted. Instead, we can decide afterwards how many topics seems realistic:\n\ntopic_model.reduce_topics(docs, nr_topics=15)\n\n&lt;bertopic._bertopic.BERTopic at 0x794041658ca0&gt;\n\n\n\n\nVisualize Terms After Reduction\n\ntopic_model.visualize_barchart(top_n_topics=15)\n\n\n\n\nSaving the model\nThe model and its internal settings can easily be saved. Note that the documents and embeddings will not be saved. However, UMAP and HDBSCAN will be saved.\n\n# Save model\ntopic_model.save(\"/content/drive/MyDrive/2023-12-01-LTW23-CrowdTangle-Posts-model\")\n\n\n\nSource: Topic Modeling Using BERTopic"
  },
  {
    "objectID": "processing/exploration.html#extracting-topics",
    "href": "processing/exploration.html#extracting-topics",
    "title": "Data Import",
    "section": "Extracting Topics",
    "text": "Extracting Topics\nAfter fitting our model, we can start by looking at the results. Typically, we look at the most frequent topics first as they best represent the collection of documents.\n\nfreq = topic_model.get_topic_info(); freq.head(5)\n\n\n  \n    \n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nRepresentative_Docs\n\n\n\n\n0\n-1\n860\n-1_bayern_csu_uhr_mehr\n[bayern, csu, uhr, mehr, menschen, m√ºnchen, te...\n[Wir gehen mit #herzstatthetze in den Wahlkamp...\n\n\n1\n0\n137\n0_w√§hlen_fdp_hessen_heute\n[w√§hlen, fdp, hessen, heute, stimme, stimmen, ...\n[Unser Ministerpr√§sident @markus.soeder steigt...\n\n\n2\n1\n104\n1_energie_co2_klimaschutz_habeck\n[energie, co2, klimaschutz, habeck, wasserstof...\n[Habeck t√§uscht √ñffentlichkeit mit Zensur: R√ºc...\n\n\n3\n2\n103\n2_zuwanderung_migration_grenzpolizei_migration...\n[zuwanderung, migration, grenzpolizei, migrati...\n[Wir sagen Ja zu #Hilfe und #Arbeitsmigration,...\n\n\n4\n3\n89\n3_uhr_starke mitte_bayerns starke_bayerns\n[uhr, starke mitte, bayerns starke, bayerns, b...\n[\"Deutschland-Pakt\" aus Scholz der Krise komme...\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n-1 refers to all outliers and should typically be ignored. Next, let‚Äôs take a look at a frequent topic that were generated:\n\nlen(freq)\n\n52\n\n\nWe have a total of 52 topics\n\ntopic_model.get_topic(0)  # Select the most frequent topic\n\n[('w√§hlen', 0.01628736425293884),\n ('fdp', 0.01626632927971954),\n ('hessen', 0.013634118460503969),\n ('heute', 0.013441948777152065),\n ('stimme', 0.011907460231710654),\n ('stimmen', 0.011505832701270827),\n ('landtagswahl', 0.011272934711858047),\n ('wahlkampf', 0.01059385752962746),\n ('sonntag', 0.01057520846171656),\n ('bayern', 0.010322807358750668)]\n\n\n\nVisualize Topics\nAfter having trained our BERTopic model, we can iteratively go through perhaps a hundred topic to get a good understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. Instead, we can visualize the topics that were generated in a way very similar to LDAvis:\n\ntopic_model.visualize_topics()\n\n\n\n\nVisualize Terms\nWe can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation. Insights can be gained from the relative c-TF-IDF scores between and within topics. Moreover, you can easily compare topic representations to each other.\n\ntopic_model.visualize_barchart(top_n_topics=15)\n\n\n\n\nTopic Reduction\nWe can also reduce the number of topics after having trained a BERTopic model. The advantage of doing so, is that you can decide the number of topics after knowing how many are actually created. It is difficult to predict before training your model how many topics that are in your documents and how many will be extracted. Instead, we can decide afterwards how many topics seems realistic:\n\ntopic_model.reduce_topics(docs, nr_topics=15)\n\n&lt;bertopic._bertopic.BERTopic at 0x794041658ca0&gt;\n\n\n\n\nVisualize Terms After Reduction\n\ntopic_model.visualize_barchart(top_n_topics=15)\n\n\n\n\nSaving the model\nThe model and its internal settings can easily be saved. Note that the documents and embeddings will not be saved. However, UMAP and HDBSCAN will be saved.\n\n# Save model\ntopic_model.save(\"/content/drive/MyDrive/2023-12-01-LTW23-CrowdTangle-Posts-model\")"
  },
  {
    "objectID": "processing/exploration.html#exploration-through-prompting",
    "href": "processing/exploration.html#exploration-through-prompting",
    "title": "Data Import",
    "section": "Exploration Through Prompting",
    "text": "Exploration Through Prompting\n\n\nUsing GPT for Information Extraction\nThe focus of this chapter lies in demonstrating how GPT can be employed in a loop to analyze text documents. This methodology aligns with the principles of topic modeling but extends further by leveraging the advanced capabilities of the language model. Our approach involves the iterative processing of text, where GPT aids in identifying, categorizing, and interpreting the underlying themes and sentiments expressed in social media texts.\nThe GPT application presents a significant difference compared to traditional topic modeling. While topic modeling often aims to automatically uncover hidden thematic structures within a text corpus, our approach with GPT is based on a different assumption: We presuppose that there is already a specific theme or a particular question in mind according to which we want to organize and analyze the documents. This approach allows us to navigate through the vast amounts of text in social media in a targeted and efficient manner, identifying specific insights and patterns that are directly related to our predefined areas of interest.\nThe following workflow outlines how we could use this information extraction process to create a topic list. Using the list we can classify each document.\n\n\n\nAn example for a GPT based ‚ÄúTopic Modeling‚Äù approach. I have used this approach in a current research project, the process is not perfect yet.\n\n\n\n!pip install -q openai backoff gpt-cost-estimator\n\n\n\nSetup for the OpenAI API\nWe‚Äôre using the new Colab Feature to store keys safely within the Colab Environment. Click on the key on the left to add your API key and enable it for this notebook. Enter the name fpr your API-Key in the api_key_name variable below.\n\nimport openai\nfrom openai import OpenAI\nfrom google.colab import userdata\nimport backoff\nfrom gpt_cost_estimator import CostEstimator\n\napi_key_name = \"openai-lehrstuhl-api\"\napi_key = userdata.get(api_key_name)\n\n\n# Initialize OpenAI using the key\nclient = OpenAI(\n    api_key=api_key\n)\n\n\n\n@CostEstimator()\ndef query_openai(model, temperature, messages, mock=True, completion_tokens=10):\n    return client.chat.completions.create(\n                      model=model,\n                      temperature=temperature,\n                      messages=messages,\n                      max_tokens=600)\n\n# We define the run_request method to wrap it with the @backoff decorator\n@backoff.on_exception(backoff.expo, (openai.RateLimitError, openai.APIError))\ndef run_request(system_prompt, user_prompt, mock):\n  messages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": user_prompt}\n  ]\n\n  return query_openai(\n          model=\"gpt-3.5-turbo-0613\",\n          temperature=0.0,\n          messages=messages,\n          mock=mock\n        )\n\nNext, we create a system prompt describing what we want to extract. For further examples of prompts and advice on prompt engineering see e.g.¬†the prompting guide and further resources linked at the bottom of the page.\nFor the initial example we use social media content shared by politicans and parties. We know, that some of these texts mention policy issues, let‚Äôs try to extract these issues across all documents.\nNote: The extracted issues are not going to be consistent, because each document is sent as a singular request to the API, thus the previous issues are not going to be used as context.\nModify the following system prompt to extract other types of information. What else could you extract?\n\nLocations (based on names)\nNames (of persons or places)\nMentions of Companies\n‚Ä¶\n\nDo not forget the Prompt Archive when experimenting. Share your successfull prompt with us!\n\nsystem_prompt = \"\"\"\nYou are a helpful assistant, an expert for German politics.\n**Objective:** Extract policy issues from German language social media texts. Policy issues refer to specific topics or subjects that are the focus of public or governmental debate, analysis, and decision-making. Elections themselves and party slogans or their performance are no policy issues.\n**Instructions:** Return each policy issues referenced in user message as a comma-seperated list. Return 'None' if no policy issues are referenced.\n**Formatting:** Return a comma-seperated list.\n\"\"\"\n\n\n\nRunning the request.\nThe following code snippet uses my gpt-cost-estimator package to simulate API requests and calculate a cost estimate. Please run the estimation whne possible to asses the price-tag before sending requests to OpenAI! Make sure ‚Äòrun_request‚Äô and ‚Äòsystem_prompt‚Äô are defined before this block by running the two blocks above!\nSet the following variables:\n\nMOCK: Do you want to mock the OpenAI request (dry run) to calculate the estimated price?\nRESET_COST: Do you want to reset the cost estimation when running the query?\nCOLUMN: What‚Äôs the column name to save the results of the data extraction task to?\nSAMPLE_SIZE: Do you want to run the request on a smaller sample of the whole data? (Useful for testing). Enter 0 to run on the whole dataset.\n\n\nfrom tqdm.auto import tqdm\n\nMOCK = True\nRESET_COST = True \nCOLUMN = 'Policy Issues'\nSAMPLE_SIZE = 0 \n\n# Initializing the empty column\nif COLUMN not in new_df.columns:\n  new_df[COLUMN] = None\n\nif RESET_COST:\n  # Reset Estimates\n  CostEstimator.reset()\n  print(\"Reset Cost Estimation\")\n\nfiltered_df = new_df.copy()\n\n# Skip previously annotated rows\nfiltered_df = filtered_df[pd.isna(filtered_df['Policy Issues'])]\n\nif SAMPLE_SIZE &gt; 0:\n  filtered_df = filtered_df.sample(SAMPLE_SIZE)\n\nfor index, row in tqdm(filtered_df.iterrows(), total=len(filtered_df)):\n    try:\n        response = run_request(system_prompt, row['Text'], MOCK)\n\n        if not MOCK:\n          # Extract the response content\n          # Adjust the following line according to the structure of the response\n          r = response.choices[0].message.content\n\n          # Convert the string 'r' to a list if it's not 'None', otherwise keep it as None\n          if r != 'None':\n              r = r.split(', ')\n          else:\n              r = None\n\n          # Update the 'new_df' DataFrame\n          new_df.at[index, 'Policy Issues'] = r\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        # Optionally, handle the error (e.g., by logging or by setting a default value)\n\nprint()\n\nReset Cost Estimation\n\n\n\n\n\n\n# Save Results\nnew_df.to_csv('/content/drive/MyDrive/2023-12-01-Export-Posts-Text-Master.csv')\n\nNext we create a set of Policy Issues. Sets are similar to lists in that they are used to store multiple items, but each unique item in a set appears only once, regardless of how many times it is added, as sets inherently enforce uniqueness and do not allow duplicates. Unlike lists, sets are unordered, meaning they do not record element position or order of insertion. This property makes sets highly efficient for checking membership and eliminating repeated entries. We create the list policy_issues to generate a word cloud.\n\nunique_policy_issues = set()\npolicy_issues = []\n\nfor issues in new_df['Policy Issues']:\n    if issues is not None:\n        unique_policy_issues.update(issues)\n\n        for issue in issues:\n            policy_issues.append(issue)\n\nprint(unique_policy_issues)\n\n{'faires Verfahren', 'Stra√üen', 'EU-Kommission', 'Zusammenhalt unserer Gesellschaft', 'migrationswende', 'bayernliebe', 'Thema Migration nicht unkontrolliert weiterlaufen lassen', 'Sprache', 'FREIHEITf√ºrBayern', 'Freistaat Bayern', 'rezession', 'Hilfe', 'Mehrwertsteuer', 'tv', 'L√ºgen', 'Bayern stark und stabil bleibt', 'Baumfreifl√§chen', 'BAf√∂G-H√∂chstsatz', 'Online Antrag', 'Zur√ºckweisungen an Binnengrenzen', 'Autoverbot', 'wohnungskonzerne enteignen', 'Infrastrukturausbau', 'Reformierbarkeit des √∂ffentlich-rechtlichen Rundfunks', 'bayerische Antidiskriminierungsstelle', 'Baden-W√ºrttemberg', 'Strompreiszonen', 'Streit und Chaos', 'linke Inhalte', 'Soldatenberuf', 'bezahlbare Energie', 'Verkaufszahlen bei Heizungssystemen', 'Forschung und Entwicklung', '√ñlheizungen', 'Situation an den Aussengrenzen', 'Leitungen', 'Verbotsorgien', 'Kriminalit√§t', 'CDU/CSU', 'Sprachen und Kultur', 'deutsche Universit√§t', 'bezahlbares Bayern f√ºr alle', 'Souver√§nit√§t', 'wiedervereinigung', 'Landwirte', 'Rechtsstaat', 'moderne Lernumgebung', 'Iran', 'Hebammen', 'Tierschutz', 'Baumwipfelpfad', 'politische Einfl√ºsse', 'bayrisches Brauchtum', 'Mitbestimmung', 'Linksextremisten', 'landtagbayern', 'BayernsOpposition', 'ddr', 'ltw', 'Parteidisziplin', 'Planungs- und Genehmigungsprozesse', 'Wirtschaft f√∂rdern', 'Energiepreis', 'Personalschl√ºssel', 'Bestandsgeb√§ude', 'gleichwertige Lebensverh√§ltnisse', 'Energiepreise', 'Sparen', 'bezahlbarer Wohnraum f√ºr alle', 'Jugendzentren', 'Wassersparen', 'niedrigere Standards', 'Sorgen und N√∂te', 'Migrationswende', 'junge M√§nner', 'Haus- und Fach√§rzte', 'Landwirtschaftspolitik', 'unserezukunft', 'Faschist*innen', 'wegenmorgen', 'konservative Werte', 'innenminister', 'Politik', 'Bundeswehr', 'Startchancen f√ºr Kinder', 'Kulturlandschaft', 'umsteuern', 'Klimaschutzziele erreichen', 'F√ºhrerscheinrichtlinie', 'Kita-Betreuungspl√§tze', 'politischer Wechsel', 'Kaufnebenkosten', 'Obergrenze f√ºr Bargeldzahlungen', 'heimatliebe', 'Zivilist*innen', 'Glaubw√ºrdigkeit', 'Mehrwertsteuer f√ºr die Gastronomie', 'unkontrollierte Zuwanderung', 'LOSvonBerlin', 'Anti-Atom-Ideologie', 'Ausbau der Erneuerbaren Energien', 'Mehr Platz f√ºr das Rad und die √ñffis', 'Biodiversit√§t', 'linke Medien', 'afdw√§hlen', 'Sozialwohnungen', 'bayerischerlandtag', 'Grenzen', 'Migrationskatastrophe', 'rechtsradikale Parteien', 'Ego-Show', 'Bildungswende', 'Fach- und Erg√§nzungskr√§fte', 'Terroristen der Hamas', 'anpackenstattankleben', 'Nachtfahrverbot', 'unsereverantwortung', 'illegale Einwanderung', 'Hotel', 'erfolg', 'Deutschlandticket', 'Unterschlei√üheim', 'Wirtschafts- und Standortpolitik', 'Startchancen-Programm', 'Personal- und Sachmittel', 'Landtag Bayern', 'Grenzschutz', 'Dampfheizkraftwerk', 'Gefl√ºchtete', 'Bayernpartei', 'Exportwirtschaft', 'Rechte der B√ºrger', 'Zivilgesellschaft', 'Ernsthaftigkeit', 'B√ºrgerlicheMitte', 'pseudowissenschaftliche Gender-Ideologie', 'sichere Herkunftsstaaten', 'haust√ºrwahlkampf', 'Bayern lebt es sich einfach besser', 'Wahlklatsche', 'Mittelstand', 'Diesel', 'F√∂rderk√ºrzungen', 'Deutschlandpakt', 'Stigmatisierung', 'Befreiung land- und forstwirtschaftlicher Fahrzeuge von der KfZ-Steuer', 'Mobilit√§tsmix', 'Innere & transnationale Sicherheit', 'LOSvonRom', 'Hetze', '8Oktober', 'MINT-F√§cher', 'Stillstand', 'Beteiligung', 'Bevormundung aus Berlin', 'Finanzierungen', 'Technologieoffenheit bewahren', 'illegale Migration', 'islamistischer Terror', 'Auto-Verbot', 'oberfranken', 'Ampel-Regierung', 'GesunderMenschenverstand', 'Perspektiven', 'Arztsitze', 'Terrorangriffe der Hamas auf Israel', 'SPD', 'Bau einer weiteren Betonpiste', 'Bezirksvorsitzender f√ºr Unterfranken', 'sommertour', 'Sach- statt Geldleistungen', 'deutschegeschichte', 'ehrenamt', 'Sprachfeststellungstests', 'Klarheit bei Heizungsgesetz', 'opposition', 'bayerischen Steuerzahler entlasten', 'Kinderbetreuung', 'station√§re Versorgung', '√úberforderung', 'LTWBayern', 'Selbstbestimmungsgesetz', 'Interessen', 'landespolitische Fragen', 'Ausl√§nder', 'Unternehmenssteuern', 'CO2-Maut f√ºr LKW', '√ñffentlich-rechtlicher Rundfunk', 'Bezirkstage', 'missionunion', 'Abgabe', 'Weltoffenheit', 'Waldbauern', 'Elster Zertifikat', 'Hohenwart', 'Meinungen aus', 'Terrororganisation', 'Aufbaugeneration', 'rechtskonform', 'Meisterausbildung', 'digitale Strategie', 'wichtige Themen', 'energiewende', 'Briefwahl', 'Umweltschutz', 'christlich', 'Kaputt gesparte Kommunen', 'Lebensmittel- und Energiepreise', 'kommende Wahlen', 'Chancengerechtigkeit', 'Entlastung', 'Geldleistungen', 'Bezahlbare Wohnungen und faire Mieten f√ºr 7 Mio. Mieter', 'Unterst√ºtzung', 'Wirtschaftswachstum', 'Heizungsverbote', 'Haushaltspolitik', 'j√ºdischer Staat', 'Wasserstoff-Region', 'Flurneuordnung', 'eigene Zukunft', 'Fachkr√§fte', 'Kontrolle', 'Biomassedeckel', 'Verkehr', 'Mehrwertsteuer auf Grundnahrungsmittel', 'Zuwanderung', 'Zahnsanierungen', 'soziale Leistungen', 'Entwicklungshilfe', 'Benachteiligungen des l√§ndlichen Raums', 'Digitale Bildung', 'Chancenbereitung', 'Abschiebung', 'Einfluss der ideologisierten und politisierten ‚ÄûKlimaforschung‚Äú', 'LosvonBerlin', 'Tanz', 'Unterst√ºtzung f√ºr Familien', 'Weltsicht', 'Kahlschlag in der Krankenhauslandschaft', 'Heizgesetz', 'Klimasparbuch', 'Zahnarzt', 'grundsatzprogramm', 'St√§dten', 'Steuergeschenke f√ºr Konzerne', 'Gesundheitspolitik', 'steuerliche Entlastungen', 'Anpacken', 'Wiedereinzug in den Bayerischen Landtag', 'Teamgeist', 'Numerus Clausus', 'Inklusion', 'Verteidigungspolitik', 'Heimatliebe', 'ausr√ºsten', 'Ehrenamtlicher Einsatz', 'gef√§hrdungslage', 'Gender', 'Beschr√§nkungen der Bargeldnutzung', 'Stabilit√§t', 'M√ºnchen', 'Dr. Markus B√ºchler', 'bevormundung', 'selbstbestimmtes Europa', 'Asylbewerberleistungsgesetz', 'wirtschaftspolitik', 'Waldstilllegungen', 'beschr√§nkt g√ºltiger F√ºhrerschein ab 60', 'Bayerns W√§lder', 'Gr√§uel', 'Abschieben', 'islamistischer Terror der Hamas', 'Arbeit', 'MWST in der Gastronomie', 'bezahlbar', '8. Oktober', 'Katastrophenschutz', 'Nahverkehrsangebote', 'Mitmach-Aktionen', 'zivilgesellschaftliche Kr√§fte', 'Fernsehbosse', 'Brenner-Nordzulauf', 'Einigkeit', 'Gewalt', 'Parteiarbeit', 'Abtreibungen', 'Tanken', 'Privateigentumsschutz', 'politische und gesellschaftliche (Fehl-)Entwicklungen', 'Expertenkommission', 'Bayern selbst entscheiden k√∂nnen', 'Ideen', 'aktueller Rechtsruck', 'Asyl-Lobbyisten', 'Biotechnologie', 'starke Stimme', 'Klimaschutz', 'Kontoverbindung', 'bezahlbares Wohnen', 'b√ºrgerliche Koalition', 'Klimawandel', 'Migranten', 'ungewollt schwangere Frauen', 'NGO', 'zeitgem√§√üe F√ºhrung', 'gew√∂hnlicher Aufenthalt', 'Tourismus', 'B√ºrger', 'Interessen Italiens', 'Terror', 'Mietpreissteigerungen', 'Impfzwang', 'Batteriespeicher', 'Kahlschlag', 'L√§rm', 'fr√ºhzeitiges Erlernen der deutschen Sprache', 'Bayern liebt', 'internationale Gemeinschaft', 'Maximilianeum', 'EUCH bewegen', 'Ausbildungsreform f√ºr die Kinderpflege', 'innovative Bauverfahren', 'Ehrenamt', 'Bildungsoffensive f√ºr Deutschland', 'antisemitische Sachverhalte', 'wirtschaft', 'Zusammenarbeit mit dem Regime', 'Pharmazie', 'mittelfranken', 'Abschiebungen', 'Ampel-Chaos', 'gesellschaftliche Unterst√ºtzung', 'Frieden in Europa', '√úbernahme nach dem Studium', 'Stimmung gegen√ºber Gefl√ºchteten', 'Miete', 'Natur erhalten', 'Familien mit Kindern', 'BayernZuerst', 'Bezahlbare Wohnungen und faire Mieten', 'Mitarbeitenden', 'digitale Beantragung', 'Opfer rechter Gewalt', 'Wasserversorgung', 'Pflegegeld', 'Wohnungsoffensive', 'F√ºrDieZukunft', 'Gesetz', 'praktische √úbungsstunde', 'Verkehrstr√§ger', 'Ideologiefreiheit an den Hochschulen', 'Jugendparlament', 'neonazistische Gruppen', 'Hessenweiterf√ºhren', 'Fahrverbote', 'Sachleistungen', 'Rechtsrutsch', 'Schutz vor Gefahren', 'Landwirtinnen', 'CO2-Bepreisung', 'Modlareuth', 'Bayernwahl', 'Weniger B√ºrokratie und eine digitale Verwaltung', 'Petition', 'Herausforderungen unserer Zeit', 'deutsche Richterbund', 'ltwby23', 'Klimafreundliches Heizen', 'Kunst', 'zweite Legislatur', 'Gesundheitswesen', 'H2-Heizung', 'K√ºnstliche Intelligenz', 'Universit√§t', 'Migrationsabkommen', 'E-Fuels', 'Politiker', 'bayerische St√§dtebauf√∂rderung', 'Rassismus', 'w√§hlherzstatthetze', 'L√§nderfinanzausgleich reformieren', 'Medizin', 'Krimbr√ºcke', 'landtagswahl', 'Wasserstoff massiv ausbauen', 'F√ºhrerschein f√ºr schwere PKWs', 'demokratische Werte', 'politisch Verfolgte', 'Verantwortung', 'Grenzpolizei', 'Leistung', 'Gute Pflege', 'recht', 'Leistungen', 'gr√ºne Klimakonto', '√Ñrmsten der Bev√∂lkerung', 'gute Schulen', 'Gefl√ºchtetenpolitik', 'Soldatischer Dienst', 'Stolz', 'Elterngeld', 'regionale Spezialit√§ten', 'soziale Sicherheit', 'klare Positionen', 'LKW', 'eigenst√§ndiges Fahren ab 16 Jahren', 'Bayerns Opposition', 'Klimakollaps', 'Tempolimit', 'einheit', 'CO2-Bilanz', 'heimat', 'friedliches Europa', 'EU-Au√üengrenzen', 'Gr√ºnen', 'Glasindustrie', 'ErbschaftsteuerAbschaffen', 'pro-pal√§stinensische Terrororganisationen', 'Finanzierung', 'Umwelt', 'Schutz von J√ºdinnen und Juden', 'Krankenhausreform', 'EU-Eliten', 'Windkraftausbau', 'DIELINKE', 'Bildungs- und Betreuungseinrichtungen', 'Einheimische Bev√∂lkerung', 'steigende Mieten', 'Kostenlosen und ticketfreien √ñPNV', 'kleidung', 'Wochenmarkt', 'Verantwortungsvolle Regierung', 'unabh√§ngige Justiz', 'Solarenergie-Anlage f√ºr Balkon oder Dach', 'Pflege', 'H√ºrden', 'Mehrwertsteuer auf Speisen in der Gastronomie', 'kathaunterwegs', 'Motor f√ºr Deutschland und Europa', 'Integration', 'Lehrst√ºhle', 'Wiedervereinigung', 'M√§nnern und Kindern in Israel', 'Asylrecht aush√∂hlen', 'Bildungsangebote von Verb√§nden', 'kostenloser Nahverkehr', 'H√ºrden f√ºr den F√ºhrerscheinerwerb', 'individuelle Mobilit√§t', 'Halbzeitbilanz der Ampel', 'Gewerbe', 'Planungshoheit der L√§nder', 'Riedenburg', 'Geb√§udeenergiegesetz', '√∂konomie', 'Diskriminierung bei Nichtverwendung der ‚ÄûGendersprache‚Äú', 'Aiwanger', 'Sonderaufnahmeprogramme', 'Landtagswahlen', 'Taurusmarschflugk√∂rper', 'Partei der Mitte', 'fr√ºhkindliche Bildung', 'Geiseln', 'Mobilit√§t f√ºr alle', 'Vergesellschaftung gro√üer profitorientierter Wohnungskonzerne', 'Apotheke', 'politische Bildung', 'Landr√§te', 'Wirtschaftlichkeit', 'Absenkung der Mehrwertsteuer', '√ñkomodellregionen', 'Leichenmisshandlung', 'R√ºckf√ºhrungsabkommen', 'Unsicherheiten f√ºr Studierende', 'Sympathiekundgebungen f√ºr den Terror in Israel', 'bayerischerrundfunk', 'Corona-Bu√ügelder', 'Parteien', 'Dorferneuerung', 'Kernfusion', 'Werte', 'Krankenhausversorgung auf dem Land', 'Sauerlach', 'B√ºrgerinteresse', 'Bayern wird Wasserstoffland Nummer 1', 'Krankenh√§user', 'mauerfall', 'Genehmigungsprozesse', 'Schwarz-Gr√ºn', 'Atomkraftwerken', 'Hofsterben', 'Ausl√§nder-Gewalt', 'Versorgung', 'Drittstaatsangeh√∂rige', 'Festung Europa', 'Unterfranken', 'Heimat', 'nat√ºrlicher Rohstoff Holz', 'L√∂sungen', 'Abgabelast pro gefahrenen km', 'gegen gr√ºne Ideologie', 'Gegenpositionen zum herrschenden Zeitgeist', 'Hilfe und Arbeitsmigration', 'Sitzen bleiben', 'Familienbetriebe st√§rken', 'Landtagskandidat*innen', 'fossile Energietr√§ger', 'europ√§ische Regelung', 'CSU-Versprechen im Wahlkampf', 'pal√§stinensische Terroristen', 'Gr√ºne', 'Klimakonto', 'Menschenrechtsverletzungen', 'Verkehrsbelastung', 'autorit√§re Gesundheitspolitik', 'Israel', 'wirtschaftliche Entwicklung des Freistaates', 'staatliche Verwaltung', 'tradition', 'gestiegene Kosten f√ºr Heizung', 'Zukunftsvertrag zur Landwirtschaft in Bayern', 'K√ºrzungshammer', 'Soziale Politik', 'gr√ºne', 'Maghreb-Staaten', 'F√∂rderrunde', 'gesellschaftliches Wohlergehen', 'Bewusstsein', 'Gewinnung von Arbeits- und Fachkr√§ften', 'Bandenkriminalit√§t', 'Stellenabbau', 'F√∂rdergelder', 'Pflegeversorgung in der Heimat sicherstellen', 'Richtungsentscheidung', 'bezahlbare Wohnungen', 'Gelder f√ºr Freiwilligendienste', 'bayerische Volkspartei', 'Geburtsstationen', 'landtag', 'Bayerntour', 'konsequente R√ºckf√ºhrung krimineller Straft√§ter', 'Gesundheitsversorgung', 'Bus und Bahn', 'Schweden', 'Regeln', 'kostenlose Kitas', 'Wahl am 8.10.', 'bayerische Grenzpolizei', 'Populistische Politik', 'Klimakleber', 'Leichensch√§ndigung', 'vereint', 'EU-Sanktionen', 'Innere Sicherheit', 'konservative Opposition', 'Breitbandversorgung', 'Lebensqualit√§t', 'Ausbildungskosten', 'bezahlbarkeit', 'Vertrauen in demokratische Institutionen', 'Minderheit im eigenen Land', 'Wasserstoffdrehkreuz', 'CO2-Preis', 'Erdgasheizungen', 'Erbschaftsteuer', 'Abkommen', 'landtagswahlen', 'Mobilfunk', 'Fakten', 'Lebensverh√§ltnisse', 'Bayernliebe', 'DRG-Fallpauschal-Finanzierung abschaffen', 'Parteivorsitzende', 'Demokrat*innen', 'Demonstranten', 'Investition in Ausstattung der Schulen', 'EU stoppen', 'russischer Angriffskrieg gegen die Ukraine', 'politische Gefangene', 'FREIHEITf√ºrBAYERN', 'Wirtschaftspolitik', 'Forschung an KI', 'geb√ºhrenfreie Kitas', 'Gesundheit', 'Familienpolitik', 'Ideologie', 'Handeln', 'Jugend', 'Rechtsstaatlichkeit', 'Startchancen', 'sinkender Strompreis', 'kostenfreie Bildung', 'Forschung', 'Gefahrenstellen', 'Schule und Berufsleben', 'Kommunen', 'Haft', 'Schlaganfall-Versorgung', 'straff√§llig', 'Heizungsgesetz', 'Grenze', 'ltwby', 'Selbstbestimmung', 'Geld', 'tagderdeutscheneinheit', 'l√§ndliche R√§ume', 'Respekt', 'B√ºrgerenergie-Genossenschaften', 'Antisemitismus', 'EU-Asylkompromiss', 'Automobilindustrie', 'Markus S√∂der und die CSU', 'Immobilienhaie', 'Kandidierenden', 'Sanierungsbedarf', 'Kurs', 'Vernichtung', 'Wissenschaft', 'Kitapl√§tze', 'Wirtschafts- und Sozialpolitik', 'Rente', 'EWERG eG', 'Gr√ºne raus aus der Regierung', 'Strompreise von Umlagen und Steuern', 'Arbeitsvertrag', 'Politik f√ºr die eigenen Leute', 'PolitikF√ºrUnsereZukunft', 'Beseitigung von Weltraumschrott', 'Rundfunkrat', 'k√ºnftige Generationen', 'GEAS', 'Bezahlung in der Pflege', 'Spaltung', 'Verbeamtung', 'international', 'n√ºrnberg', 'Strompreise', 'faire Bezahlung von Pflegekr√§ften', 'rechte Politik', 'Wirtschaftszweig', 'Kandidatinnen und Kandidaten', 'Satellitendaten', 'Verkehrsentlastung', 'Nationaler Sicherheitsrat', 'begleitetes Fahren', 'starkes und bezahlbares Bayern', 'Migrationspolitik', 'NGOs', 'Umwidmung von Parkpl√§tzen', 'heimische obst- und nahrungsmittel', 'Seenotrettung', 'Betonfundamente', 'CSU', 'Technologie-Offenheit', 'Datenschutz', '√∂ffentlich-rechtlichen Medien', 'Erhalt der heimischen Lebensmittelproduktion', 'politikmitverstand', 'Landwirtschaft', 'Nationale Raumfahrtstrategie', 'Hightech', 'Arbeitslosenquote', 'Artenvielfalt', 'Kassenleistungen', 'Wirtschaft', 'Wohlstand', 'PIN', 'Zukunftskurs', 'einheitliche Regelungen', 'EU', 'Wohnung', 'staatliche Betriebskostenf√∂rderung', 'neoliberale Wirtschafts- und Finanzpolitik', 'antisemitische Einstellungen', 'Lebensmittel', 'Dorfentwicklung', 'Wasserstoff', 'CO2-Bindung', 'Biomasse', 'handlungsf√§higer Staat', 'Verg√ºtung', 'starke Bildungspolitik', 'Judenhass', 'Privatsph√§re', 'Wasserstoff-Tankstelle', 'Schienen', 'sauberes und bezahlbares Zuhause', 'Geschichte', 'bayerisches Familiengeld', 'Betriebskostenf√∂rderung', 'gemeinn√ºtzige Arbeit', 'LTWBy', 'L√§rmschutz', 'F√∂rderung von Ideen zur Verbesserung von Unterricht und Schule', 'Elektrolyseur', 'Kostenlose Kitas', 'versorgungsrelevant', 'WHO', 'HolDirDeineZukunftZur√ºck', 'Rundfunkbeitrag', 'Photovoltaik', 'Gemeinschaftsschule', 'Mieten', 'Bau- und Wohnwirtschaft', 'Energie', 'Abgase', 'Wissenschaftsfreiheit', 'Terror der Hamas', 'BAf√∂G Reform', 'Gedichte', 'Machen statt Niedermachen', 'H√ºterin der B√ºrgerrechte', 'Chancen', 'Tariftreue-Paket', 'fl√§chendeckende Notfallversorgung', 'Umweltfreundliche Mobilit√§tsformen st√§rken', 'Kliniken', 'Bildungssystem', 'Feindbilder', 'Landes-Antidiskriminierungsgesetz', 'Radio', 'Abrechnung der Arztkosten', 'Anma√üungen des EuGH', 'Eigentum', 'Eltern- und Sch√ºler*innenvertretungen', 'Windkraft', 'Senioren', 'Ungleichheit', 'Liberalismus in Europa', 'Ausbau des mobilen Internets', 'Lauterbach', 'Abwehr dieses Terrorangriffs', 'Politik f√ºr Leistung und Eigentum', 'teambayern', 'Verbindungsachsen', 'exzellente Ausbildung', 'Demokratie-Dialog', 'Wasserstoffnetz', 'krise', 'Wohnraum', 'politik', 'Energieversorgung', 'Eigentum in Familienbesitz sch√ºtzen', 'FreistaatBayern', 'praktische Berufe', 'Werkswohnungen', 'Europawahl 2024', 'Quiz-Spiel', 'Nancy Faeser', 'GemeinsamStark', 'Haft f√ºr Schutzsuchende', 'Ordnung und Sicherheit', 'Lieferung von schweren Waffen', 'Biotech-Standort', 'Grenzschutzoffensive Bayern', 'sittenwidrig', 'Energiewende', 'Abh√§ngigkeit vom Ausland', 'Arbeit im Rentenalter', 'Engagement f√ºr Demokratie', 'queere Menschen', 'Holzwachstum', 'Durchforstungsholz', 'Ingolstadt', 'Pflegekrise', 'Grundnahrungsmittel', 'Versorgungssicherheit', 'attraktive Bedingungen f√ºr deutsche Weltraumunternehmen', 'Bildungspolitik', 'Effizienz', 'Stra√üengro√üprojekt', 'Demoskopen', 'Mutter', 'LTW', 'linke', 'Pflegegesellschaft', 'Rechtsterrorismus', 'Hass und Hetze', 'Kartellamt', 'Zwang', 'Rundfunkgeb√ºhren abschaffen', 'Arbeitsmarktpolitik', 'Merz', 'Patient', 'Partei', 'Krisen', 'GR√úN', 'heimatmitherz', 'Frieden', 'b√ºrokratischer Mehraufwand', 'Netto', 'bezahlbare Lebensmittel', 'Desinformation', 'Schulen', 'bildungsgerechtigkeit', 'Freilassung aller Geiseln', 'Oberbayern', 'MedizinischeVersorgung', 'Energie-Mix', 'Lebenshaltungskosten', 'Postleitzahl', 'Geothermie', 'Bayern', 'Wirtschaftsfreundlichere Rahmenbedingungen', 'starke Wirtschaft und bezahlbare Energie', 'regierung', 'geschichte', 'Wille', 'Europawahlen', 'ltw2023', 'Anstalten', '2023', 'Lieferkettengesetz', 'Verwaltung', 'Bildung und Forschung', 'sexuelle Gewalt', 'HerzStattHetze', 'Manifest f√ºr Freiheit in Europa', 'Fremdbestimmung', 'Steuersenkungen', 'Menschen in Israel', 'Schwaben', 'rechte Gewalt', 'Mietpreisbremse', 'w√ºrdevoll', 'EnergiewendeMitVerstand', 'L√§ndlichen Raum st√§rken', 'CDUParteitag', 'Zukunftsvertrag f√ºr die Landwirtschaft', 'DeineStimmeZ√§hlt', 'Gegenwehr gegen√ºber einer √ºbergriffigen EU-B√ºrokratie', 'Bruder', 'freistaatbayern', 'B√ºrgergeldreform', 'Familie', 'st√§ndige Hetze von S√∂der', 'F√∂rderung innovativer Start-ups', 'demokratie', 'W√§rmewende', 'rechte Szene', 'gute Pflege', 'Friedensbewegung', 'Windr√§der', 'Prinzipien', 'T√§tern', 'Verbindungen', 'Sturm', 'Tauruslieferung', 'Kernkraft', 'Kleinkraftr√§der', 'soziale Gerechtigkeit', 'Hass und Antisemitismus', 'Drogenlegalisierung', 'einigkeit', 'klimaschutz', 'radikale Bewegungen in √ñsterreich und Deutschland', 'wiederverwenden', 'Familien', 'Mangel an Kita- und Pflegepl√§tzen', 'Kraft der Vernunft', 'faire Mieten', 'Land Israel', 'Region', 'Rundfunk', 'Heizungspolitik', 'Migrationskrise', 'DIE LINKE', 'Sozialepolitik', 'Wohnungsbau', 'Fl√ºchtende', 'Bewirtschaftete W√§lder', 'gute L√∂hne', 'Ganztagspl√§tze', 'sicherer Strom', 'Grenzen kontrollieren', 'Gegenrechts', 'Chatkontrolle', 'Technologieoffenheit', 'Menschen in Armut', 'Weiden', 'F√∂rderung des √ñkolandbaus und der Biologischen Vielfalt', 'Europ√§ische Staaten', 'Innenministerin Faeser', 'Zugangscode', 'Strompreis', 'Forschungspolitik', 'Mullah Regime', 'Verteidigung Israels', 'Situation in den Aufnahmekommunen', 'Klimakrise', 'Opfer', 'Ausbildungsst√§tte', 'Rechtsmittel', 'Streit', 'Bildungsprotest2023', 'heimatbayern', 'katrinebnersteiner', 'Kraft', 'Forschungsbedingungen', 'Verwaltungsrat', 'EU-G√§ngelung', 'Impuls', 'Produktionsverlagerung', 'Industrie', 'kriminelle Ausl√§nder', 'Staatswald', 'Nazis', 'sachsenanhalt', 'Babys in Bayern', 'Spitzenkandidat', '√úberl√§nge von 16,50 m auf 17,40 m zulassen', 'Einwanderungs- und Asylpolitik', 'Spritpreisbremse', 'F√∂rdermittelk√ºrzungen', 'gemeinsame Sprache', 'Immatrikulation', 'U18-Wahl', 'mobilit√§t', 'None', 'Kinder- und Jugendplan', 'Krisenverordnung', 'Pflegekonzepte', 'B√ºrokratieabbau', 'BayernSPD', 'bildung', 'Tradition', 'oberbayern', 'w√§hlen', 'Zugang zur Justiz', 'W√§lder', 'Gericht', 'Beste Bildung und weniger Unterrichtsausfall', 'Visionen', 'Terrorism', 'Asylrecht', 'Lernmittelfreiheit', 'Pr√§ventionsangebote', 'schlechter √ñPNV', 'Studis', 'humanit√§re Verantwortung', 'Ampelpl√§ne', 'Asylpolitik', 'br', 'Mopedf√ºhrerschein', 'Abschaffung der ungerechten Erbschaftssteuer', 'Demokratief√∂rderung', 'Land√§rzte', 'traditionelle Studieng√§nge', 'Enteignung', 'AfD-Wahlergebnisse', 'Holz√∂fen', 'Nationale Sicherheit', 'bayerische Bezirkstage', 'Bildungswende jetzt', 'Erbschaftssteuer', 'Volksentscheid', 'Landtags- und Bezirkstagswahl', 'Intoleranz', 'Abschaffung des Asylrechts', 'deutschland', 'ErbschaftssteuerAbschaffen', 'Gerechtigkeit', 'Gr√∂√üenwahn', 'Gewalt und Terror in Israel', 'Familienkasse', 'Atomwaffen', 'Wahl', 'GegenRechts', 'Wochen', 'R√ºcktritt', 'bayerische Staatsangeh√∂rigkeit', 'ehemalige SED-Partei', 'Wohnen', 'Mitgliedsstaaten', 'Kultursommer mit Links', 'arbeitspl√§tze', 'Kernkraftwerke', 'CSU Parteivorstand', 'Terrorismus', 'Anti-Demokrat*innen', 'Sorgen der B√ºrger ernst nehmen', 'bayerische Interessen im Bund und in Europa', 'die das Klima sch√ºtzt', 'soziale Politik f√ºr Bayern', 'Tarifbindung', 'B√ºrger*innen-Energiegenossenschaft', 'Abschaffung der CO2-Steuer', 'linksextremistischen Gruppen', 'bayerische Staatlichkeit', 'EEG-F√∂rderung', 'extremisten', 'Leistung und Eigentum', 'Durchhalteverm√∂gen', 'oberpfalz', 'gerechter Freistaat', 'Verbrechen an unschuldigen Frauen', 'Grundsicherung', 'Verl√§sslichkeit und Kompetenz statt Beliebigkeit und Populismus', 'Los von Berlin', 'ambulante Anlaufstellen', 'AKW-Verteufelung', 'rechnen', 'Herangehensweisen', 'Bezahlbares Wohnen f√ºr 7 Mio. Mieter', 'Existenzrecht des j√ºdischen Staates Israel', 'staatliche Grundfinanzierung von Universit√§ten und Hochschulen ohne ideologische Vorgaben', 'St√§rke', 'Kraftstoff', 'Wasserstoffinfrastruktur', 'vern√ºnftige Mitte', 'Vertr√§ge mit Staaten in Nordafrika und T√ºrkei', 'Landtagswahlen in Hessen und Bayern', 'Tempolimit auf Autobahnen', 'p√§dagogische Qualit√§t von Kitas', 'Gewerkschaften', 'Polizei', 'Russland', 'FlurNatur-Struktur und Landschaftselemente', 'Arbeitsmigration', 'Verbrennungsmotoren', 'Arbeits- und Fachkr√§fte', 'B√ºrokratie', 'Kostenlose Kitas f√ºr 780 000 Kinder', 'schlechte Bildung', 'Enteignen', 'Steuers√§tze', 'soziale Probleme', 'Einreisekontrolle an den EU-Au√üengrenzen', 'israelische St√§dte und D√∂rfer', 'FDP', 'Rechtspopulismus', 'Krieg', 'Steuermodelle', 'Schule', 'alleinerziehend', 'buntes Kinderprogramm', 'VPN', 'Anstand', 'Staatsanwaltschaften', 'Arbeitskr√§ftemangel', 'LandtagBayern', 'Verteilungsfragen', 'Souver√§nit√§tsverlust', 'Menschenrechte', 'Oberpfalz', 'Einkommensteuer', 'kostenfreie Kitas', 'lebenswertes Bayern', 'Investitionen in die Zukunft', 'sichere Stromversorgung', 'Staatsr√§son', 'sozialepolitikf√ºrdich', 'Gute Pflege f√ºr 2,7 Mio. Senioren', 'Anerkennung', 'Kinder', 'ingolstadt', 'schlanker und effizienter Staat', 'F√∂rderung', 'ausbau', 'Anpacken f√ºr Bayern', 'Pers√∂nlichkeitsrechte', 'Zuwanderungspolitik', 'Umweltsch√ºtzer', 'erneuerbare Energien', 'Krankenhaus', 'Deutsche Stromkunden', 'Lehrer', 'Antragsprozess', 'Italianisierung', 'Gegenrassismus', 'inklusives Bildungssystem', 'Stromversorgung', 'Bundesregierung', 'Krankenhaus-Milliarde', 'MitDir', 'heimatschutz', 'Die Konfrontation', 'Toleranz', 'Freibetr√§ge', 'fernsehen', 'freiberufliche Apotheken', 'Leerstandsabgabe', 'Stromleitungen', 'Zahlen', 'Freiheitf√ºrS√ºdTirol', 'lpt2023', 'Deutschland-Pakt gegen unkontrollierte Zuwanderung', 'Lehrerinnen', 'Mehrsprachigkeit', 'Digitalministerin', 'Vereine und das Ehrenamt st√§rken', 'anpacken', 'Bevormundung', 'Identit√§t und Nation', 'Kostenlose Bildung', 'Biotopen', 'Sprit sparen', 'Regierungsform', 'Katharina und Ludwig', 'Versorgungsstra√üen', '√ñPNV', 'Blockabfertigungen', 'Lohnersatzleistungen f√ºr pflegende Angeh√∂rige', 'Ladenschlussgesetz', 'Einb√ºrgerung', 'sozialpolitik', 'Rathausplatz', 'Belebung von Ortszentren und Dorferneuerung', 'Mietendeckel', 'FREIE W√ÑHLER', 'Privatversicherte', 'Standort', 'Wiedervereinigung Deutschlands', 'Mindestlohn', 'R√ºckf√ºhrung von kriminellen Straft√§tern', 'Insolvenzen', 'Mehr Personal und bessere Zusammenarbeit und Vernetzung', 'Einstellung von Richtern und Staatsanw√§lten', 'vereinbarkeit', 'Gendern', 'Erhaltung von D√∂rfern', 'Kinderzukunftsprogramm', 'Pelletheizung', 'Sicherheitsvorkehrungen', 'gute Bildung', 'Gesellschaft', 'landwirtschaft', 'Einzelleistungsverg√ºtung', 'Verdoppelung Kapazit√§t', 'Habeck', 'Naturpark', '√úbergriffigkeiten der EU-Eliten', 'Druck', 'Hausbesitzer', 'Verkehrspolitik', 'Gastronomie', 'starke Wirtschaft', 'Agieren und Finanzierung pal√§stinensischer und propal√§stinensischer Terrororganisationen', 'kostenlosem √ñPNV f√ºr Kinder und Jugendliche', 'Ganzjahrestourismus', 'Schule f√ºr alle', 'Erdbeobachtungen', 'Vergesslichkeit', 'Pessimismus', 'Online Ausweis', 'klimaneutraler Wohnraum', 'Kinder und Jugendliche', 'innovativ', 'Ausbau der Windkraft', 'Landespflegegeld', 'besseres Europa', 'Sozialpolitik', 'Asyl', 'ampel', 'individuelle F√∂rderung', 'BP', 'wahlprogramm', 'Menschen mit Behinderung', 'LINKE', 'internationale Wettbewerbsf√§higkeit', 'mangelndem Wohnraum', 'Kinder und Jugendliche mit Migrationshintergrund', 'Wasserstoff-Gipfel', 'fl√§chendeckend', 'junge Gr√ºne Abgeordnete', 'Herausforderungen in der Migrationspolitik', 'Zwangsimpfungen', 'Freiheit f√ºr Bayern', 'F√∂rderungen f√ºr Holz- und Pellets-Heizungen', 'Kindergrundsicherung', 'bayerische Arbeitspl√§tze', 'Innovation', 'Naturschutz', 'sozialer Aufstieg', 'aktuelle Lage', 'Mittelmeer', 'Gelder', 'innenministerherrmann', 'CO2-Einsparung im deutschen Strommix', 'gr√ºne AKW-Heuchelei', 'Mobilit√§t egal wo du hin willst', 'Italien', 'Deutschland', 'bayern', 'Berliner Senat', 'Gesundheitsreform', 'kostenloses Mittagessen', 'sauberer Strom', 'progressive', 'Mehrwertsteuer in der Gastronomie', 'Steuererh√∂hung', 'Finanzierung islamistischer Organisationen', 'Staat Israel', 'Begleitetes Fahren ab 15 Jahren', 'Ignoranz', 'Musik', 'kostenfreier Schulweg', 'Windrad', 'Kapazit√§ten', 'energie', 'konfrontation', 'Stromerzeugung', 'Studieren', 'wissenschaft', 'Menschen vor Ort', 'Haushaltsmittel zur Kofinanzierung der Gemeinschaftsaufgabe Agrarstruktur und K√ºstenschutz', 'Jugendhaus', 'Streuobstpakt', 'freiheit', 'staatlich subventionierter Industriestrompreis', 'Freie W√§hler', 'Medienbildung', 'Antisemitismus-Beauftragter der Bayerischen Staatsregierung', 'Amberg', 'Bev√∂lkerung', 'Rechtsextremismus', 'Trinkwasserschutz', 'dritter Nationalpark', 'Digitalisierung', 'Fachsch√ºler*innen', 'Asylsystem', 'den Gr√ºnen und den linkslastigen Medien', 'Vielfalt', 'Humanit√§t', 'Augsburg', 'neue Stromleitungen', 'Jom Kippur', 'Prost', 'station√§re Grenzkontrollen', 'gr√ºnklingelt', 'nachhaltigere Raumfahrt', 'Hackschnitzel', 'kostenloser Meister', 'kostenlosen √ñPNV', 'optimistische zukunftsorientierte Politik', 'Sparerpauschbetrag', 'Bargeldnutzung', 'Soziale Gerechtigkeit', 'M√§rkte', 'bayerisches F√∂rderprogramm', 'Umgang mit Unternehmen', 'h√∂herer Mindestlohn', 'Lebensgrundlagen', 'Handwerk', 'Kooperationsvertr√§ge mit der Bundespolizei', 'Fl√§chenverbrauch', 'gleiche Chancen', 'psychische Gesundheit', 'Steuersenkung', 'Ausbildung', 'Grundwasserschutz', 'Institutionen', 'national strukturierteres Abschiebeverfahren', 'angriff', 'Oberfranken', 'Kommunale Krankenh√§user erhalten', 'Anti-Auto-Haltung', 'R√ºckf√ºhrung', 'Wohnungsnot', 'finanzielle F√∂rderung von Grundschulen', 'kostenfreie Meisterausbildung', 'Berlin', 'medizinische Versorgung', '√∂kologie', 'Modernit√§t', 'Innenentwicklung und die Vermeidung von Fl√§chenverbrauch', 'brauchtum', 'Propagandafernsehen', 'Zeltlager', 'jungeunion', 'Mobilit√§tswende', 'Nutzung', 'Sozialdemokratie', 'Bayerischer Landtag', 'Genehmigungen', 'Soziale Politik F√ºr Dich', 'Fachkr√§ftemangel', 'Drogenkonsumr√§ume', 'dezentrale Bevorratung in Bayern und Deutschland', 'unterfranken', 'Geburtshilfe', 'Azubis', 'Fallpauschalensystem', 'gute Pflege f√ºr 2,7 Mio. Senioren', 'Kitas', 'Druck auf die Ampel', 'rechtsrutschstoppen', 'Abdeckungs-Offensive', 'Kommerzialisierung', 'Solaranlage', 'rechte Ausschreitungen', 'bezahlbares Bayern', 'Landschaftswasserhaushalt', 'Bildungsorganisationen', 'echte Beteiligung', 'Mauerfall', 'Kampf gegen Rechts', 'Konkurrenz', 'AnpackenF√ºrBayern', 'moedlareuth', 'klimasch√§dlicher Flugverkehr', 'Finanzen', 'Wahlalter 16', 'wichtige soziale Themen', 'Tempolimit auf deutschen Autobahnen', 'Umweltzerst√∂rungen', 'Vorhaben', 'fachkr√§ftemangel', 'Stellenwert in der Gesellschaft', 'Weltraummanagement', 'Fichtenbestand', 'Ver√§nderungen', 'Dieselfahrverbot', 'Umwelt- und Naturschutz', 'Energie- und Industriepolitik', 'Technik', 'Ticketfreiheit', 'Unterst√ºtzung der Schulen bei der Umsetzung von Programmen', 'Chancengleichheit', 'Erh√∂hung der LKW Maut', 'Europawahlprogramm', 'Schutz der Zivilbev√∂lkerung', 'Schleuserkriminalit√§t', 'selbstverwaltete Justiz', 'EU-Gerichtshof', 'Verbotspolitik', 'LTWby23', 'Antragsberechtigung', 'generationengerechte Politik', 'demokratieverteidigen', 'Todesstrafe', 'J√§hrlichen Stellenaufbau bis 2029 verl√§ngern', 'Holzheizung', 'klarer Kurs', 'Exekutive', 'Asyl-Migration', 'Schutzgrund', 'Recht', 'Nationalismus', 'Wende in der Migrationspolitik', 'Stromnetzausbau', 'Brauchtum', 'lesen', 'Unterdr√ºckung im Iran', 'Waldsch√§dlinge', 'Technologie', 'Ferienangebote', 'Landes- und B√ºndnisverteidigung', 'AUSSENGRENZEN', 'Entf√ºhrungen', 'Tag der Deutschen Einheit', 'Markus S√∂der', 'herrmann', 'Benzin', 'Bayern-Energie', 'Klimaneutralit√§t', 'Politsystem', 'Autonomie f√ºr S√ºd-Tirol', 'Erzeugerpreise', 'deutsche Staatsb√ºrgerschaft', 'Heizen', 'inneresicherheit', 'Brandmauern', 'B√ºrgergeld', 'bezahlbare Mieten', 'Arbeitnehmerrechte', 'B√ºrgerinnen und B√ºrger', 'CO2', 'Gr√ºnen Partei', 'Kinderhaus', 'v√∂lkerrechtswidriger Angriff', 'Student', 'Zinsen', 'bezahlbarer Wohnraum', 'Wohnungsmangel', 'Safe Abortion Day', 'Einkommen', 'Diskutieren wir', 'Europ√§ische Kommission', 'freistaat', 'Frieden und Freiheit', 'Kulturkampf', 'Spoken words', 'Produktion ins Ausland', 'M√∂dlareuth', 'Einheit', 'tvtipp', 'l√§ndlicher Raum', 'angehobene Altersgrenzen', 'Schulsozialarbeit', 'Schutz', 'allestimmengr√ºn', 'zukunft', 'Migration', 'Gr√ºnen wollen das ganze Land bevormunden', 'Grundschule', 'zweitehand', 'Massenmigration', 'Anpacken f√ºr unsere B√ºrger', 'organisiertes', 'B√ºrgerrechte', 'CO2-neutraler Kraftstoff', 'zielgerichtete Leistungen', 'Ampel', 'Life Science Campus', 'Steuerfreibetr√§ge im Monat pro Arbeitnehmer auf 2000 Euro', 'europ√§ische Zukunft', 'schwaben', 'R√ºckf√ºhrungen', 'FSJ-Pl√§tze', 'Wasserkraft', 'Gr√ºne in der Landesregierung', 'Rechtsruck', 'Selbstbewusstsein', 'gemeinsames Lernen', 'D√ºrre', 'Wohnsitz', 'sicherheit', 'Wirtschaftsstrompreis', 'Heimatbewusstsein', 'Bodentruppen', 'DeutscheGeschichte', 'Bus', 'Verbote', 'Abschaffung Erbschaftssteuer', 'F√∂rderung der l√§ndlichen Entwicklung', 'Krankenhausversorgung', 'Legislative', 'Existenzrecht Israels', 'regensburg', 'durchgr√ºntes Berlin', 'Finanzierung des √ñPNVs', 'Heimatvertriebene', 'chrupalla', 'LTW23', 'Landschaftspfleger', 'Sommer', 'illegale Einreisen', 'heimische Energiewelt', 'Neonazi-Strukturen', 'Demokratiebildung', 'bundesweite Grenzpolizei', 'Deregulierung', 'Inflation', 'Zukunftsfinanzierungsgesetz', 'Herz statt Hetze', 'Bezahlbare Energie', 'gesellschaftliche Teilhabe', 'volle Unterst√ºtzung f√ºr die Ukraine', 'bayerische Interessen', 'Elternhaus', 'bessere Taktung', 'Abgaben', 'antisemitische Propaganda', 'l√§ndliche Krankenh√§user', 'Grundversorgung', 'Bayerns erneuerbare Energie', 'Entlastungen', 'Aufl√∂sung des √∂ffentlich-rechtlichen Rundfunks', 'gr√ºne Dogmen', 'effektiver Grenzschutz', 'Erdgas', 'Kriminelle Straftaten', 'Rendite', 'Regensburg', 'Diskriminierung', 'AfD-Erfolgswelle', 'Mobilit√§t', 'Erneuerbare', 'Windenergie', 'Kernwegenetzbau', 'Energieversorgung in Bayern', 'Vitalit√§t', 'staatliche Wohnheime', 'Kinder und Jugendliche in den Fokus', 'Engagement f√ºr die Heimat', 'L√∂hne', 'Hochschule', 'Revolutionsgarden', 'Weltfriedenstag', 'Covid-Ma√ünahmen', 'Bildung f√ºr Bayern', 'Barrierefreiheit', 'Zwangsgeb√ºhren', 'bezahlbare und saubere Energie', 'Baupolitik', 'Wahnsinn des Nationalsozialismus', 'Genuss', 'einseitigen W√§rmepumpen-Tr√§ume der Ampel', 'Selbstregierung', 'kostenlose Meisterausbildung', 'Hisbollah', 'Menschen mit Fluchtgeschichte', 'Zukunftsvertrag zwischen der Staatsregierung und dem Bayerischen Bauernverband', 'Mittelstand sch√ºtzen', 'Kernfusions-Kraftwerk', 'Erhalt aller Schulstandorte', 'Landtagswahl', 'legale Zuwanderung', 'Gerichte', 'L√§nderfinanzausgleich', 'rechts', 'Mut', 'LTWby2023', 'b√ºcher', 'Remigration', 'Bayerische Grenzpolizei', 'Freiheit', 'hohe Energiepreise', 'Klimakatastrophe', 'Existenz- und Altersabsicherung', 'Vernunft statt Ideologie', 'Zusammenleben', 'Asylantr√§ge', 'ortsnahe Versorgung', 'Kontinuit√§t', 'Zahlungen an Pal√§stinenser', 'kostendeckende Schulstarthilfe', 'Aufnahmestopp f√ºr junge M√§nner', 'Inntal', 'Mieterschutz', 'Normalverdiener', 'KEINE dritte Startbahn am Flughafen M√ºnchen', 'gendern', 'bayernsOpposition', '√Ñrztemangel', 'Zeitenwende', 'Haltung', 'LTW2023', 'Holzheizungen', 'Hilfe f√ºr Betroffene von Terrorismus', 'Sicherheit und Ordnung', '√úbergriffe des italienischen Staates', 'Online-Petition', 'Wirtschaftsminister', 'Theoriestunden', 'Schwangerschaftsabbr√ºche', 'Erneuerbare Energien', 'Terrorangriff der Hamas', 'Rechenschaft', 'br24wahl', 'St√§rkung von Landschaften', 'Studierende', 'Artenschutz', 'Grundwasser', 'Wasserschutz', 'Auflagen', 'Zusammenhalt', 'Situation der Studierenden in Bayern', 'LudwigUntrwegs', 'T√ºrkei', 'N√ºrnberg', 'Deutsche Einheit', 'Energiewende vor Ort', 'Hitze', 'Wertsch√§tzung f√ºr √§ltere Menschen', 'Bezahlbare Wohnungen', 'Wahlprogramm', 'Bildung f√ºr alle unsere Kinder', 'nachhaltigkeit', 'Vorschriften', 'starke Infrastruktur', 'Organisationsbereiche', 'Gazastreifen', 'Steigerwaldzentrum', 'Au√üenpolitik', 'saubere Energie', 'Renten', 'Keine Gr√ºnen in der Regierung', 'duales Studium als Bildungsweg st√§rken', 'L√∂hne in Ostdeutschland', 'augsburg', 'Medikamente', 'anf√§ngliche Fehler', 'Bund ID Konto', 'Unternehmen', 'Sicherheit bei Lebensplanung', 'Schnitzel', 'Mitglieder-Anteile', 'Richter', 'Stallbauvorschriften', 'Bildung unabh√§ngig vom Geldbeutel', 'Bayern bleibt', 'Pflegekr√§fte', 'remigration', 'Klima', 'deutschlandfest', 'Aufgaben unserer Zeit', 'fl√§chendeckende Gesundheitsversorgung', 'Chaos', 'Abschiebe-Zahlen', 'Ethos', 'Kampagne finanziert sich', 'Schutzversprechen f√ºr j√ºdisches Leben in Bayern', 'Energie und Treibstoffe', 'Steuerliche F√∂rderungen', 'Bauern', 'Lehrst√ºhle f√ºr ‚ÄûGenderforschung‚Äú', 'zukunftsf√§higes Bildungssystem', 'Wasserstoff-Land Nummer 1 werden', 'Multimillllion√§re', 'sozialpolitische Ma√ünahmen', 'W√§rmepumpe', 'brzahlbares Wohnen', 'Niederbayern', 'Wohnen als Grundrecht', 'B√ºrgermeister', 'Chancengerechtigkeit in Deutschland', 'Schutz des ungeborenen Lebens', 'Ukraine', 'den Automobilstandort Deutschland st√§rken', 'Reform des Gesundheitswesens', 'Energiegewinnung', 'Landkreis M√ºnchen', '√§rztliche Versorgung', 'Akutsprechstunden', 'K√ºrzungspolitik', 'Vertrauen', 'Entlastungen f√ºr Sparerinnen und Sparer', 'Wohnungs- und Mietmarkt', 'Misstrauen', 'Erbschaftssteuer abschaffen', 'Steuerpolitik', 'Volksbegehren', 'EEG-Umlage', 'Ausbildungs- oder Studienstart', 'Zusatzleistungen', 'Bavarian Fusion Cluster', 'Investorenbetriebene Medizinische Versorgungszentren (MVZ)', 'Spitzenmedizin', 'Wahlkampf', 'Kirchen', 'Energiekosten', 'Hessen', 'Verbrennerverbot', 'Marktwirtschaft', 'Landtags- und Kommunalwahlen', 'Stil', 'Verwaltungsdirektor', 'deutsche Staatsr√§son', 'Nationalsozialismus', 'Europa', 'Grundlastf√§higkeit', 'saubere und bezahlbare Energie', 'Bus und Bahn ins ganze Land bringen', 'Asylbewerber*innen', 'Hilfe und F√ºrsorge des Staates und der Gesellschaft', 'Hilfe in sozialen Fragen', 'Elektrolyseur-F√∂rderprogramm', 'Familiengeld', 'Opposition', 'niederbayern', 'Sicherheit des Staates Israel', 'F√ºhrungspositionen', 'Wohnungsgemeinn√ºtzigkeit', 'Medien', 'soziale Herkunft', 'Fernsehen', 'CDU', 'Anpacken statt Ank√ºndigen', 'GEZ abschaffen', 'Arbeitspl√§tze', 'soziale Politik', 'Wasserentnahme', 'Kinderarmut', 'Filz', 'saisonalit√§t', 'Gesellschaften', 'staatliche Energieunternehmen', 'Heimatenergie', 'S√ºden', 'kreislauf', 'Energiepolitik', 'Bildung f√ºr alle', 'Finanzminister', 'steigende Preise', 'LOSvonBERLIN', 'Wahlerfolge f√ºr Boris Rhein und die CDU Hessen', 'mittelschicht', 'Deckelung der Mieten', 'Mitte-rechts', 'volle Unterst√ºtzung f√ºr Israel', 'Benachteiligung von Menschen mit psychischen Problemen', 'TeamBayern', 'beste Bildung', 'Altparteien', 'n√§chste Wahlen', 'g√ºnstiger √ñPNV', 'CO2-freier Strom', 'Bezirkstag', 'Hamas-Terror', 'Schienenverkehr ausbauen', 'Heimische Produktion von Arzneimittel st√§rken', 'Anteile', 'St√§rkung der Demokratie', 'Ruhegehalt', 'Vergabeverfahren von Medizin-Studienpl√§tzen', 'DeshalbAfD', 'Energiesparen', 'Holznutzung', 'dezentrale Unterbringungen', 'BayerischerLandtag', 'Verh√§ltnis zur Partei', 'Identit√§t', 'Abw√§rme', 'Wohnraumf√∂rderung', 'Grenzschutzbayern', 'Bildung', 'Freie und selbstbestimmte Entscheidung der Patienten', 'junge Generation', 'politiker', 'klimaneutrales Wirtschaften', 'Auto', 'Arbeitszeitflexibilisierung', 'H2-Tankstellen', 'AfD verspricht', 'umsetzen', 'Allgemeinbildung', 'ltwbayern', 'Bezahlbare und saubere Energie', 'Einkommensunterschiede', 'infostand', 'forschung', 'unserlandzuerst', 'Migrationsstopp', 'Freiheit und weniger Kontrollen f√ºr bayerische Landwirte', 'Bundestag', 'Sprachkurse', 'Hessenwahl', 'afd', 'gute Ganztagsbetreuung', 'Stromsteuer', 'Hamas', 'primetime', 'Jugendherberge', 'FREIEW√ÑHLER', 'Veranstaltung', 'wehrhafte Demokratie', 'Ursula von der Leyen', 'Mehr Kontrolle und Steuerung bei der Migration', 'Freistaat', 'Angriffe', 'Mieterinnen und Mieter', 'Abschottungswahnsinn', 'Medizinische Versorgung', 'Solidarit√§t', 'Borkenk√§fer', 'Hightech Agenda Bayern', 'Erh√∂hung der Steuerfreibetr√§ge im Monat pro Arbeitnehmer auf 2000 Euro', 'Kommunen bei der Aufnahme von Gefl√ºchteten unterst√ºtzen', 'Gesetzesvolksentscheid', 'Selbstverteidigung', 'Chancenbudget', 'Transportwege', 'familienfreundlicher', 'Wahlniederlage', 'Bayerischer H√§rtefallfonds', 'm√ºnchen', 'Tempo 90 f√ºr Fahranf√§nger', 'Belastungs-Stopp', 'Stromspeicher', 'Seniorinnen und Senioren', 'Artenschwund', 'AfD', 'strukturschwachen Kommunen', 'Vereine', 'Festanstellung', 'Krankenhausinvestitionen', 'Zukunft', 'Gewalttaten', 'Hausbau', 'schlafen', 'Notendruck', 'afdbayern', 'Energieversorgung der Zukunft', 'Kraftstoffpreis', 'Solidarit√§t mit Israel', 'BAf√∂G', 'Strom und Lebensmittel', 'Cannabis-Legalisierung', 'n√§chste Landtagswahl', 'H√§user', 'Profite mit unserer Miete', 'Fr√ºhe Hilfen', 'Regierungsbildung', 'individuelle Mobilit√§t erm√∂glichen', 'LandtagswahlBayern', 'Bezahlbare und saubere Energie f√ºr Bayern', 'leistungsorientierte Bezahlung von Lehrkr√§ften', 'Kinder eine Zukunft', 'Investitionen', 'Durchforstung', 'Binnengrenzen', 'Radwege', 'Vernichtung von Kultur', 'Verkehrssicherheit', 'Unterschriften Sammeln', 'W√§rmenetz', 'Steuererleichterung f√ºr Agrardiesel', 'gr√ºne Politik', 'Wartezeiten f√ºr Therapiepl√§tze', 'Nahversorgung im l√§ndlichen Raum st√§rken', 'kompetente Politik', 'Urteile', 'Waffenrecht', 'illegale Zuwanderung', 'mangelnde Mobilit√§t', 'online', 'St√§rkung von Regionen', 'Spitzenkandidaten', 'hightech', 'Steuern', 'Au√üenwirtschaft', 'startups', 'Benutzername & Passwort', 'Mobilit√§t junger Menschen', 'illegaleMigration', 'gute gesundheitliche Versorgung', 'Direktkandidatin f√ºr den Bayerischen Landtag', 'm√∂rderischer Angriff auf unseren Bundesvorsitzenden', 'Bauvorschriften', 'Grenzkontrollen', 'Alternative zu Deutschland', 'Bescheid', 'Heizungs-Lotsen', 'Massenzuwanderung', 'Asylbewerber', 'steigende Mietpreise', 'Grenzsicherung', 'Rechtsruck in Bayern', 'kostenlose Kitapl√§tze', 'Claudia K√∂hler', 'gesellschaftlicher Zusammenhalt', 'Integrationsf√§higkeit', 'Milit√§r', 'selbstbestimmt', 'Demokratie', 'juxcsu', 'bayernwahl', 'Staatssekret√§r f√ºr Inneres', 'S√ºd-Tirol-Autonomie', 'Landtagswahl in Bayern', 'Verweisung nichtdeutscher Staatsb√ºrger', 'Armut', 'Mittelfranken', 'Studium', 'Hightech Agenda', 'Sicherheitsbeh√∂rden', 'politikmitherz', 'Windpark', 'Gesundheitsschutz', 'Wiedereinzug', 'Aktivrente', 'geringes Einkommen', 'ausl√§ndische Fachkr√§fte', 'Landtag', 'Terror stoppen', 'gerechte Politik', 'Erosionsschutz', 'Speicher', 'regionale Wertsch√∂pfung', 'Unterbringung', 'marktwirtschaftliches Gewissen', 'Austausch mit den B√ºrgerinnen und B√ºrgern', 'wohnen', 'Umbau zu stabileren W√§ldern', 'innenministerium', 'Bahn', 'Gesundheitsvorsorge', 'Entwicklung in Deutschland', 'Gemeinschaftsaufgabe ‚ÄûVerbesserung der Agrarstruktur und des K√ºstenschutzes‚Äú', 'Sicherheit', 'duales Studium verbessern', 'Integrationsgrenze', 'Ganztag', 'S√ºd-Tiroler Freiheit', 'CO2-Steuer abschaffen', 'Steuerverschwendung', 'Teambayern', 'Einwanderung', 'Kostenlose Kitas f√ºr 780.000 Kinder', 'interessierte B√ºrger', 'terroristischer √úberfall', 'Klimapolitik', 'israelisches Volk', 'Dreigliedriges Schulsystem'}\n\n\nLet‚Äôs quickly generate a wordcloud to check for patterns. See the Simple Corpus Analysis Notebook for more information.\n\n!pip install -q wordcloud\n\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport requests\n\n# Retrieve Stopwords from Github\nr = requests.get('https://github.com/stopwords-iso/stopwords-de/raw/master/stopwords-de.json')\nstop_words = r.json()\n\n# Stopw√∂rter in die WordCloud laden\nSTOPWORDS.update(stop_words)\n\ndef generate_wordcloud(text):\n    text = ' '.join(list(text))\n\n    # Generate a word cloud image\n    wordcloud = WordCloud(background_color=\"white\",width=1920, height=1080).generate(text)\n\n    # Dazugeh√∂rige Grafik erstellen\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.figtext(0.5, 0.1, \"Policy Issues\", wrap=True, horizontalalignment='center', fontsize=12)\n    plt.show()\n\ngenerate_wordcloud(policy_issues)\n\n\n\n\nNow we‚Äôre ready to pass the list to GPT to extract a manageable amount of topics. Note the list might be too long to fit into the GPT context window. In this case we have to split the list into several shorter lists and iterate over them.\nThis time we are not interested in a specific formatting for the response. We want to print the result for human interpretation.\n\nsystem_prompt = \"\"\"\nYou are a helpful assistant, an expert for German politics. Derive 15 topics of policy issues from this list of keywords provided by the user. Concentrate on overarching topics and avoid overlapping topics. Provide a set of 10 keywords per topic.\n\"\"\"\n\n\nkeyword_string = \", \".join(unique_policy_issues)\nresponse = run_request(system_prompt, row['Text'], False)\n\nCost: $0.0010 | Total: $0.3431\n\n\n\nprint(response.choices[0].message.content)\n\n1. Sicherheit\n- Polizei\n- Kriminalit√§t\n- Terrorismus\n- √úberwachung\n- Grenzkontrollen\n\n2. Wirtschaftswachstum\n- Industrie\n- Arbeitspl√§tze\n- Investitionen\n- Innovation\n- Export\n\n3. Arbeitslosenquote\n- Arbeitsmarkt\n- Arbeitslosengeld\n- Arbeitsvermittlung\n- Qualifikationen\n- Arbeitslosenversicherung\n\n4. Regierung\n- Politik\n- Parteien\n- Regierungsbildung\n- Koalitionen\n- Opposition\n\n5. Bayern-Power\n- Regionalpolitik\n- Infrastruktur\n- Bildung\n- Kultur\n- Tourismus\n\n6. Ampel-Frust\n- Politikverdrossenheit\n- Koalitionsstreitigkeiten\n- Stillstand\n- Kompromisse\n- Unzufriedenheit\n\n7. Familiengeld\n- Familienpolitik\n- Kinderbetreuung\n- Elternzeit\n- Kindergeld\n- Unterst√ºtzung\n\n8. Pflegegeld\n- Pflegepolitik\n- Altenpflege\n- Pflegeversicherung\n- Pflegeheim\n- Angeh√∂rigenpflege\n\n9. Meisterausbildung\n- Berufsausbildung\n- Fachkr√§ftemangel\n- Handwerk\n- Aufstiegschancen\n- Weiterbildung\n\n10. Briefwahl\n- Wahlrecht\n- Wahlbeteiligung\n- Demokratie\n- Wahlkampf\n- Stimmabgabe\n\n\n\nSource: Text Exploration Using GPT"
  },
  {
    "objectID": "processing/exploration.html#conclusion",
    "href": "processing/exploration.html#conclusion",
    "title": "Data Import",
    "section": "Conclusion",
    "text": "Conclusion\nWe have explored two transformer based approaches for text exploration. BERTopic is an easy to use tool for topic modeling. Using this approach we can quickly explore patterns in the content of (textual) social media content ‚Äì as long as there is a GPU available (e.g.¬†on Colab). We will come back to this tool in the future, when dealing with images, as we might be able to harness its abilities for visual media.\nThe text exploration using GPT, on the other hand, does not rely on special hardware, as we query the API and OpenAI is taking care of the heavy computing. Prompting offers the possibilities to explore our data according to endless questions, yet we need some form of question to get started. We have explored policy issues using the gpt-3.5-turbo model, the results are mixed. Looking through the wordcloud we see issues that might have been at the centre of attention, like Education, Climate Protection, and Security. At this point, however, we should be cautious to generalize, other issues might have been named differently between requests, thus disappearing within the wordcloud. Looking at the BERTopic results, we can spot similar topics, like Security and Migration (Topic 2), Climate Protection (Topic 1), and Education (Topic 12).\nToday‚Äôs prompting marks the tip of the iceberg, over the course of the next weeks we will use more and more prompts, moving from exploration, to classification. One prompting technique which we will not discuss this semester is Retrieval Augmented Generation (RAG), which might also be useful for Text Exploration. This technique combines information retrieval with text generation. LlamaIndex and LangChain are python package that may help to build RAG applications. How to integrate them into our research workflow will be a future project (or topic for a future BA or MA thesis)."
  },
  {
    "objectID": "processing/exploration.html#more-resources",
    "href": "processing/exploration.html#more-resources",
    "title": "Data Import",
    "section": "More Resources",
    "text": "More Resources\n\nPrompting Guide\nBrown et al. (2020): Language Models are Few-Shot Learners\nLiu et al. (2023): Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\nM√∏ller et al. (2023): Is a prompt and a few samples all you need? Using GPT-4 for data augmentation in low-resource classification tasks.\nPromptCompass Prompts (Borra, n.d.)\nBERTopic for Topic Modeling - Maarten Grootendorst - Talking Language AI (YouTube)"
  },
  {
    "objectID": "data-collection/ig-stories.html",
    "href": "data-collection/ig-stories.html",
    "title": "Project Creation",
    "section": "",
    "text": "Instagram stories, characterized by their ephemeral nature, expire after 24 hours. Therefore, it‚Äôs crucial to collect them in a timely manner, as retrospective data collection is not an option with this format. There are two feasible methods: Instaloader and Zeeschuimer-F. Additionally, commercial tools such as 4k Stogram are also available.\nOverall, the ephemeral nature of stories necessitates our continuous monitoring and data collection from our targeted profiles. To ensure that we capture every story item, I recommend collecting stories twice daily, approximately 12 hours apart. This method accounts for potential inaccuracies in timing, as the intervals overlap. Data can be gathered manually or through computational means. The manual approach, especially in conjunction with Zeeschuimer-F, is preferable as it does not violate the Terms of Service (TOS). For this method, we would install the plugin and view all stories in our browser twice daily. Alternatively, using Instaloader involves simply initiating the command and waiting for the software to gather all the data. Optimally, we could utilize tools like Cron to automate this process."
  },
  {
    "objectID": "data-collection/ig-stories.html#instaloader",
    "href": "data-collection/ig-stories.html#instaloader",
    "title": "Project Creation",
    "section": "Instaloader",
    "text": "Instaloader\nInstaloader for Stories operates in a manner akin to collecting Posts. Initially, the package must be installed:\n!pip -q install instaloader\nUnlike the method outlined in the previous tutorial, I advise employing the command-line interface of Instaloader for collecting stories. To do this, open a terminal and execute the command below:\ninstaloader --login your.profile.name --dirname-pattern ig_stories/{profile} :stories --no-compress-json\nExecuting this command generates a dedicated subfolder within ig_stories for each user followed by your profile. It downloads the metadata, images, and videos of each story. The metadata is saved in a JSON file. While these files are typically xz-compressed by default, using the --no-compress-json option prevents this compression. Subsequently, the JSON files can be imported into a pandas DataFrame in Python.\nThis process can be automated, for example, by utilizing a bash script in conjunction with cron:\n#!/bin/bash\n\n# Generate a random number of seconds between 0 and 3600 (1 hours)\nsleep_duration=$(( RANDOM % 3600 ))\n\n# Print the sleep duration\necho \"Sleeping for $sleep_duration seconds...\"\n\n# Sleep for the random duration\nsleep $sleep_duration\n\n# Run Instaloader command to download the latest Instagram stories\ninstaloader --login your.profile.name --dirname-pattern ~/ig_stories/{profile} :stories  --no-compress-json\n\n# Add more script to check for success and send alerts in case of error\nStart cron by entering crontab -e on your terminal and add a line pointing to the bash script, e.g.:\n* 8,20 * * * /path/to/your/script.sh &gt;/dev/null 2&gt;&1\n\n\nPros:\n\n\nVery easy to automate\n\n\nCollects all data: metadata, images, videos\n\n\n\n\nCons:\n\n\nPossibly against the TOS\n\n\nRate Limits\n\n\nBlocked Accounts (very quickly)"
  },
  {
    "objectID": "data-collection/ig-stories.html#zeeschuimer-f",
    "href": "data-collection/ig-stories.html#zeeschuimer-f",
    "title": "Project Creation",
    "section": "Zeeschuimer-F",
    "text": "Zeeschuimer-F\n\nThis method is based on the Zeeschuimer Firefox Plugin. I have adapted the original plugin to create Zeeschuimer-F, which is specifically tailored for collecting Instagram stories and interfacing with the Zeeschuimer-Firebase-Backend for real-time data collection. You can find Zeeschuimer-F on GitHub. To use it, download the latest version via Firefox and install the plugin. For our seminar, I‚Äôll provide a backend instance; refer to the README.md on GitHub for guidance on setting up your own instance on Firebase. Credentials for our seminar will be distributed through GRIPS. Follow these steps to download stories using Zeeschuimer-F:\n\nDownload and install the plugin.\nCreate a project on the backend (via Notebook).\nConfigure the plugin.\nRegularly view stories in Firefox to collect them.\nDownload the collected data (via Notebook).\n\n\nPlugin Installation\nTo install the plugin, download the latest release .xpi file from GitHub using Firefox. After downloading, click on the file in Firefox and confirm the installation of the extension.\n\n\n\nScreenshot of Firefox with the open extensions menu\n\n\nVerify the extension‚Äôs installation by checking the right-hand menu in Firefox. We will return to the browser shortly.\n\n\n\nThe Firebase Interface Notebook\n\nThe following lines of code assume that the firebase Credential File has been downloaded from GRIPS and uploaded to Colab / your Jupyter project path. First of all install the necessary packages:\n\n!pip -q install firebase-admin\n\nNext, we connect to our firebase project. Please update the credentials_path variable with the path to your credentials file (see above).\n\nimport firebase_admin\nfrom firebase_admin import credentials, firestore\n\ncredentials_path = '/content/XXXX-adminsdk-YYYYYY.json' \n\ncred = credentials.Certificate(credentials_path)\nfirebase_admin.initialize_app(cred)\ndb = firestore.client()\n\n\nProject Creation\nPlease provide an alert_email and project_name to create a new project on the backend. The backend checks hourly when the last stories have been uploaded to a project. If no story has been uploaded for more than 12 hours, an email alert will be triggered.\nRun the cell to create the new project on the backend. When successfull, the project id and api key will be displayed.\n\nfrom IPython.display import display, Markdown\nimport pandas as pd\n\nalert_email = 'michael@achmann.me'\nproject_name = 'Forschungsseminar23 Test'\n\n# Create Project\nimport uuid\n\n# Generate a UUID for the document\nproject_id = str(uuid.uuid4())\napi_key = str(uuid.uuid4())\n\n# Your data\ndata = {\n    \"api_key\": api_key,\n    \"email\": alert_email,\n    \"name\": project_name\n}\n\n# Add a new document with a UUID as the document name (ID)\ndoc_ref = db.collection('projects').document(project_id)\ndoc_ref.set(data)\n\ndisplay(Markdown(\"### Project Created:\"))\ndisplay(Markdown(f\"**Project Name:** {project_name}\"))\ndisplay(Markdown(f\"**Alert Email:** {alert_email}\"))\ndisplay(Markdown(f\"**Project ID:** {project_id}\"))\ndisplay(Markdown(f\"**API-Key:** {api_key}\"))\n\n\nProject Created:\nProject Name: Forschungsseminar23 Test\nAlert Email: michael@achmann.me\nProject ID: 959466fe-4088-4099-a6b2-3cbe058889d3\nAPI-Key: 554fbce8-fb15-44f1-bb4d-54cdc57554f2\n\n\n\nConfigure the Plugin\nConfigure Zeeschuimer-F using the above information after creating a project. In order to access the settings of Firefox plugins click on the puzzle tile on the top right of the browser. Click on Zeeschuimer F and the settings open.\n\n\n\nScreenshot of Firefox with open extensions menu\n\n\nFill in the Firebase Project field with the project id and aFirebase API Key with the api key provided after running the Project Creation. The Firebase Endopint URL will be provided via GRIPS (unless you‚Äôve installed your own instance).\n\n\n\nScreenshot of the Settings for Zeeschuimer-F\n\n\n1) Turn the IG Stories Switch on, 2) restart your browser for the values to be loaded correctly. Once the browser has started again, you‚Äôre ready to collect you first stories! Open the Instagram website and open any story.\n\n\n\nScreenshot of the switch\n\n\nCheck the extension settings page to see whether it is collecting stories while browsing. The counter should increase with each story visit. The remote collection process can currently only be checked through the Firebase Interface notebook. Follow the next steps to download the collected data.\n\n\nProject Export\nThe following code downloads all stories in JSON format and saves it locally (i.e.¬†on your colab instance). Provide the PROJECT_ID variable and an export_path to download all stories.\n\nfrom tqdm.auto import tqdm\nimport os\nimport json\n\nPROJECT_ID = '959466fe-4088-4099-a6b2-3cbe058889d3'\nexport_path = '/content/export' \n\n\ndef fetch_stories(project_id):\n    stories_ref = db.collection('projects').document(project_id).collection('stories')\n    docs = stories_ref.stream()\n\n    stories = []\n    for doc in docs:\n        stories.append(doc.to_dict())\n\n    return stories\n\ndb = fetch_stories(PROJECT_ID)\n\nif not os.path.exists('export'):\n    os.makedirs('export')\n\n# Iterate over each element in the database\nfor element in tqdm(db, desc='Exporting elements'):\n    # Serialize the element to JSON\n    element_json = json.dumps(element, indent=4)\n\n    # Write to a file named {id}.json\n    with open(os.path.join('export', f\"{element['id']}.json\"), 'w') as f:\n        f.write(element_json)\n\n\n\n\n\n\nConvert to DataFrame\nNext, we convert the exported JSON files to a pandas DataFrame and save the table as CSV. Provide the df_export_path variable for the location where to save the exported CSV file.\n\n\n\n\n\n\nWork-In-Progress\n\n\n\nThe DataFrame in the current version has a different structure than the one we created when downloading Instagram Posts.. In order to compare stories with posts we will might want to use the same data structure.\n\n\n\n\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n\ndf_export_path = '/content/2022-11-09-Stories-Exported.csv' \n\ndef process_instagram_story(data):\n\n    # Extract relevant information\n    story_info = {\n        'ID': data.get(\"id\"),\n        'Time of Posting': datetime.utcfromtimestamp(data['taken_at']).strftime('%Y-%m-%d %H:%M:%S'),\n        'Type of Content': 'Video' if 'video_duration' in data else 'Image',\n        'video_url': None,\n        'image_url': None,\n        'Username': data['user']['username'],\n        'Video Length (s)': data.get('video_duration', None),\n        'Expiration': (datetime.utcfromtimestamp(data['taken_at']) + timedelta(hours=24)).strftime('%Y-%m-%d %H:%M:%S'),\n        'Caption': data.get('caption', None),\n        'Is Verified': data['user']['is_verified'],\n        'Stickers': data.get('story_bloks_stickers', []),\n        'Accessibility Caption': data.get('accessibility_caption', ''),\n        'Attribution URL': data.get('attribution_content_url', '')\n    }\n\n    return story_info\n\nrows = []\nfor element in db:\n  rows.append(process_instagram_story(element))\n\ndf = pd.DataFrame(rows)\ndf.to_csv(df_export_path)\nprint(f\"Successfully exported {len(df)} rows as CSV.\")\n\nSuccessfully exported 22 rows as CSV.\n\n\nNow let‚Äôs take a look at the structure of the exported data:\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nID\nTime of Posting\nType of Content\nvideo_url\nimage_url\nUsername\nVideo Length (s)\nExpiration\nCaption\nIs Verified\nStickers\nAccessibility Caption\nAttribution URL\n\n\n\n\n0\n3231585718932790545_1483455177\n2023-11-08 14:50:59\nImage\n&lt;NA&gt;\nhttps://storage.googleapis.com/zeeschuimer-fb-...\nrmf24.pl\nNaN\n2023-11-09 14:50:59\nNone\nFalse\n[]\nPhoto by Fakty RMF FM | Rozmowy | Podcasty on ...\n\n\n\n1\n3231585778860997221_1483455177\n2023-11-08 14:51:06\nImage\n&lt;NA&gt;\nhttps://storage.googleapis.com/zeeschuimer-fb-...\nrmf24.pl\nNaN\n2023-11-09 14:51:06\nNone\nFalse\n[]\nPhoto by Fakty RMF FM | Rozmowy | Podcasty on ...\n\n\n\n2\n3231750838597692854_1349651722\n2023-11-08 20:19:00\nVideo\nhttps://storage.googleapis.com/zeeschuimer-fb-...\n&lt;NA&gt;\ntagesschau\n13.300\n2023-11-09 20:19:00\nNone\nTrue\n[]\n\n\n\n\n3\n3231750989408058657_1349651722\n2023-11-08 20:19:18\nVideo\nhttps://storage.googleapis.com/zeeschuimer-fb-...\n&lt;NA&gt;\ntagesschau\n15.267\n2023-11-09 20:19:18\nNone\nTrue\n[]\n\n\n\n\n4\n3231751135118088390_1349651722\n2023-11-08 20:19:35\nVideo\nhttps://storage.googleapis.com/zeeschuimer-fb-...\n&lt;NA&gt;\ntagesschau\n17.000\n2023-11-09 20:19:35\nNone\nTrue\n[]\n\n\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nDownload Images and Videos\nAll videos and images for our Instagram stories have been downloaded by our firebase backend. They are saved in a Cloud Bucket. The following cell helps with these two steps:\n\nCreate a signed link to each video and image\nDownload each file and saves it in the following structure: {media_export_path}/{image|video}/{username}/{ID.jpg|mp4}. It is important to keep a unique identifier (here ID) to map metadata and images for future data analysis.\n\nPlease provide a storage_bucket and media_export_path.\n\nstorage_bucket = \"XXXX.appspot.com\"  \nmedia_export_path =  '/content/media/'\n\nfrom firebase_admin import storage\nimport os\nimport requests\n\nbucket = storage.bucket(storage_bucket)\n\ndef generate_signed_url(username, content_id, file_type):\n    if file_type not in ['images', 'videos']:\n        raise ValueError(\"Invalid file type specified\")\n\n    ext = 'jpeg' if file_type == 'images' else 'mp4'\n    blob_path = f\"projects/{PROJECT_ID}/stories/{file_type}/{username}/{content_id}.{ext}\"\n    blob = bucket.blob(blob_path)\n    # Set the expiration of the link. Here, it's set to 24 hours.\n    return blob.generate_signed_url(expiration=timedelta(hours=24), method='GET')\n\n# Create a function to be applied across DataFrame rows\ndef apply_generate_signed_url(row):\n    image_url = generate_signed_url(row['Username'], row['ID'], 'images')\n    video_url = generate_signed_url(row['Username'], row['ID'], 'videos') if row['Type of Content'] == 'Video' else pd.NA\n    return pd.Series({'image_url': image_url, 'video_url': video_url})\n\n# Apply the function along the axis=1 (row-wise)\ndf[['image_url', 'video_url']] = df.apply(apply_generate_signed_url, axis=1)\n\n# Now, creating the lists for images and videos can be done more efficiently\ndata_images = df.loc[df['image_url'].notna(), ['ID', 'image_url', 'Username', 'Time of Posting']] \\\n               .rename(columns={'image_url': 'url', 'Time of Posting': 'datetime'}) \\\n               .to_dict('records')\n\ndata_videos = df.loc[df['video_url'].notna(), ['ID', 'video_url', 'Username', 'Time of Posting']] \\\n               .rename(columns={'video_url': 'url', 'Time of Posting': 'datetime'}) \\\n               .to_dict('records')\n\n\ndef create_directories(base_path, entries, subdir):\n    usernames = set(entry['Username'] for entry in entries)\n    for username in usernames:\n        os.makedirs(os.path.join(base_path, subdir, username), exist_ok=True)\n\ndef download_file(entry, media_type, media_export_path, session):\n    directory = os.path.join(media_export_path, media_type, entry['Username'])\n    ext = 'jpg' if media_type == 'images' else 'mp4'\n    filename = os.path.join(directory, f\"{entry['ID']}.{ext}\")\n\n    with session.get(entry['url'], stream=True) as response:\n        if response.status_code == 200:\n            with open(filename, 'wb') as file:\n                for chunk in response.iter_content(8192):\n                    file.write(chunk)\n        else:\n            print(f\"Failed to download {entry['url']}. Status code: {response.status_code}\")\n\nsession = requests.Session()\n# Pre-create directories\ncreate_directories(media_export_path, data_images, 'images')\ncreate_directories(media_export_path, data_videos, 'videos')\n\n# Download images\nfor entry in tqdm(data_images, desc=\"Downloading Images\", unit=\"file\"):\n    download_file(entry, 'images', media_export_path, session)\n\n# Download videos\nfor entry in tqdm(data_videos, desc=\"Downloading Videos\", unit=\"file\"):\n    download_file(entry, 'videos', media_export_path, session)\n\nprint(\"Download complete!\")\n\n\n\n\n\n\n\nDownload complete!\n\n\n\n\nPrepare Downloadable ZIP\nRun the following to ZIP all files. Optionally copy them to Google Drive.\n\n!zip -r 2023-11-09-Story-Media-Export.zip media/*\n\n\n!cp 2023-11-09-Story-Media-Export.zip /content/drive/MyDrive/\n\n\nSource: Firebase Interface Notebook\n\n\n\nPros:\n\n\nWe do not infringe the TOS\n\n\nCollects all data: metadata, images, videos\n\n\nThe firebase backend handles alert emails\n\n\n\n\nCons:\n\n\nCurrent solution relies on the firebase backend\n\n\nWe need to manually browse the stories twice a day (can be automated using Selenium)\n\n\nData is collected on firebase storage, we need to export to use it"
  },
  {
    "objectID": "data-collection/ig-stories.html#conclusion",
    "href": "data-collection/ig-stories.html#conclusion",
    "title": "Project Creation",
    "section": "Conclusion",
    "text": "Conclusion\nThis page offers an overview of two methods for collecting ephemeral Instagram stories, which are crucial to capture in real time due to their 24-hour expiration period. The first method, instaloader, is theoretically effective. However, similar to the case with posts, Instagram accounts utilizing Instaloader face a high risk of being banned swiftly.\nThe second approach adopts a less invasive strategy. It involves capturing the data transmitted to the browser while viewing stories on Instagram, and then transferring the metadata to our Firebase project. Upon the addition of a new story to the database, the backend initiates the download of videos and images for that story.\nTo facilitate this process, I have provided a notebook for project creation, a manual for configuring the plugin, and additional code to export the captured stories via a Jupyter notebook."
  },
  {
    "objectID": "data-collection/ig-posts.html",
    "href": "data-collection/ig-posts.html",
    "title": "Instagram Posts",
    "section": "",
    "text": "Instagram offers two ways of image sharing: permanent posts and ephemeral stories. In this chapter I will offer three approaches for collecting posts: Instaloader, CrowdTangle, and Zeeschuimer.\nPosts are shaped by several affordances and contain different type of media: least one image or video, often paired with text (captions). Posts may also contain an album consisting of more than one image or video. Captions may contain hashtags and / or mentions. Hashtags are used to self-organize posts on the platform, users can subscribe to hashtags and search for them. Mentions are used to link a post to another profile. Moreover, users can like, share and comment posts. Some data-collection approaches, like CrowdTangle, offer access to one image and post metrics, like the comment and like count. Instaloader, offer access to all images / videos, while being the legally most questionable approach. And then there‚Äôs the middle ground: Zeeschuimer (optionally in connection with 4CAT).\nThrough the following subchapters I will try to illuminate the advantages of each collection methods. For each method I will provide a manual to follow in order to collect metadata and the actual media for Instagram posts."
  },
  {
    "objectID": "data-collection/ig-posts.html#instaloader",
    "href": "data-collection/ig-posts.html#instaloader",
    "title": "Instagram Posts",
    "section": "Instaloader",
    "text": "Instaloader\nInstaloader is a python package for downloading instagram pictures and videos along with their metadata. I have written a getting started tutorial on Medium. It is, together with the provided notebook, the basis for this chapter.\n\n\n\n\n\n\nNote\n\n\n\nInstaloader is a stand-alone piece of software: It offers options to download most Instagram content, like posts and stories, through different strategies, e.g.¬†lists of profiles or by hashtag. For complex tasks I recommend to call instaloader from terminal, see the documentation for more information.\n\n\n\nIn order to download posts and stories from Instagram, we use the package instaloader. You can install package for python using pip install &lt;package&gt;, the command -q minimizes the output.\n\n!pip -q install instaloader\n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60 kB 3.0 MB/s eta 0:00:011\n  Building wheel for instaloader (setup.py) ... done\n\n\nOnce you install instaloader we log in using your username and password. Session information (not your credentials!) is stored in Google Drive to minimize the need for signing in.\nIn order to minimize the risk for your account to be disabled we suggest creating a new account on your phone before proceeding!\n\nusername = 'your.username'\n\n# We save the sessionfile to the following directory. Default is the new folder `.instaloader` in your google drive. (This is optional)\nsession_directory = '/content/drive/MyDrive/.instaloader/'\n\nimport instaloader\nfrom os.path import exists\nfrom pathlib import Path\n\n# Creating session directory, if it does not exists yet\nPath(session_directory).mkdir(parents=True, exist_ok=True)\n\nfilename = \"{}session-{}\".format(session_directory, username)\nsessionfile = Path(filename)\n\n\n# Get instance\nL = instaloader.Instaloader(compress_json=False)\n\n# Check if sessionfile exists. If so load session,\n# else login interactively\nif exists(sessionfile):\n  L.load_session_from_file(username, sessionfile)\n\nelse:\n  L.interactive_login(username)\n  L.save_session_to_file(sessionfile)\n\nLoaded session from /content/drive/MyDrive/.instaloader/session-mi_sm_lab05.\n\n\n\nDownloading first Posts\nNext, we try to download all posts of a profile. Provide a username and folder:\n\ndest_username = 'some.profile' \ndest_dir = '/content/drive/MyDrive/insta-posts/' # Once more we save the files to Google Drive. Replace this with a local directory if necessary.\n\nt = Path(\"{}{}\".format(dest_dir, dest_username))\nt.mkdir(parents=True, exist_ok=True)\n\nprofile = instaloader.Profile.from_username(L.context, dest_username)\nfor post in profile.get_posts():\n    L.download_post(post, target=t)\n\nWell, you just downloaded your first posts! Open Google Drive and check the folder insta-posts/ (or whatever folder you chose above)! There should be three files for each post, the image, a .json file and a .txt file. The .txt includes the image caption, the .json lots of metadata about the post.\n\nDiving into the metadata\nThe next cell reads all .json files of the downloaded posts. Then we browse through some interesting data.\n\n# Reading the paths of all JSON files from dest_dir\nimport os\n\njson_files = []\n\nfor subdir, dirs, files in os.walk(t):\n    for file in files:\n        fullpath = os.path.join(subdir, file)\n        filename, file_extension = os.path.splitext(fullpath)\n        if file_extension == \".json\":\n          json_files.append(fullpath)\n\n\n# Reading all JSON files\nfrom tqdm.notebook import tqdm\nimport json\n\njson_data = []\n\nfor file in tqdm(json_files):\n  with open(file, 'r') as f:\n    data = json.load(f)\n    json_data.append(data)\n\n\n\n\nOk, now all metadata for all posts is saved to the variable json_data. Run the next line and copy its output to http://jsonviewer.stack.hu/. Your output should look similar, go ahead and play around to explore your data! What information can you extract?\n\nprint(json.dumps(json_data[0]))\n\n\n\nMetadata Preprocessing\nPosts contain plenty of data, like time and location of the post, the authoring user, a caption, tagged users and more. The following cells demonstrate how to normalize the data into a table format, which is useful when working with pandas. Nevertheless, this is optional!\n\n# Use booleans (True / False) values to select what type of data you'd like to analyse. \nusername = True #@param {type:\"boolean\"}\ntimestamp = True #@param {type:\"boolean\"}\ncaption = True #@param {type:\"boolean\"}\nlocation = True #@param {type:\"boolean\"}\nshortcode = True #@param {type:\"boolean\"}\nid = True #@param {type:\"boolean\"}\ntagged_users = True #@param {type:\"boolean\"}\n\nNext we loop through the data and create a new pandas DataFrame. The DataFrame will have one column for each variable selected above and one row for each downloaded posts.\nIf you are not yet familiar with the concept of dataframes have a look at YouTube, there‚Äôs plenty of introductory videos available.\n\nimport pandas as pd\n\nposts = []  # Initializing an empty list for all posts\nfor post in tqdm(json_data):\n  row = {} # Initializing an empty row for the post\n\n  node = post.get(\"node\")\n\n  if username:\n    owner = node.get(\"owner\")\n    row['username'] = owner.get(\"username\")\n\n  if timestamp:\n    row['timestamp'] = node.get(\"taken_at_timestamp\")\n\n  if location:\n    l = node.get(\"location\", None)\n    if l:\n      row['location'] = l.get(\"name\")\n\n  if shortcode:\n    row['shortcode'] = node.get(\"shortcode\")\n\n  if id:\n    row['id'] = node.get(\"id\")\n  \n  if tagged_users:\n    pass\n\n  if caption:\n    c = \"\"\n    emtc = node.get(\"edge_media_to_caption\")\n    edges = emtc.get(\"edges\")\n    for element in edges:\n      caption_node = element.get(\"node\")\n      c = c + caption_node.get(\"text\")\n    row['caption'] = c\n\n  # Finally add row to posts\n  posts.append(row)\n\n# After looping through all posts create data frame from list\nposts_df = pd.DataFrame.from_dict(posts)\n\nNow all information selected above is saved to the dataframe posts_df. Run the next cell and it will return a nicely formatted table. If your data is quite long, output will be cropped. Click the wand and after a few seconds you are able to browse through the data or filter by columns\n\nposts_df\n\nIn order to get a first impression of dataframes, the head() method is also useful. Run the next cell to see the result\n\nposts_df.head()\n\nThe dataframe is only saved in memory, thus when disconnecting and deleting the runtime, the dataframe is lost. Running the next cell saves the table to a CSV-file on your drive.\nNow the processed data may be recovered or used in another notebook.\n\nposts_df.to_csv('{}{}.csv'.format(dest_dir, username))\n\n\n\nSource: Collecting Posts with Instaloader\n\n\nPros:\n\n\nMaximum Flexibility\n\n\nCan collect everything out of the box\n\n\nWe can collect content computationally\n\n\n\n\nCons:\n\n\nPossibly against the TOS\n\n\nRate Limits\n\n\nBlocked Accounts"
  },
  {
    "objectID": "data-collection/ig-posts.html#downloading-first-posts",
    "href": "data-collection/ig-posts.html#downloading-first-posts",
    "title": "Instagram Posts",
    "section": "Downloading first Posts",
    "text": "Downloading first Posts\nNext, we try to download all posts of a profile. Provide a username and folder:\n\ndest_username = 'some.profile' \ndest_dir = '/content/drive/MyDrive/insta-posts/' # Once more we save the files to Google Drive. Replace this with a local directory if necessary.\n\nt = Path(\"{}{}\".format(dest_dir, dest_username))\nt.mkdir(parents=True, exist_ok=True)\n\nprofile = instaloader.Profile.from_username(L.context, dest_username)\nfor post in profile.get_posts():\n    L.download_post(post, target=t)\n\nWell, you just downloaded your first posts! Open Google Drive and check the folder insta-posts/ (or whatever folder you chose above)! There should be three files for each post, the image, a .json file and a .txt file. The .txt includes the image caption, the .json lots of metadata about the post.\n\nDiving into the metadata\nThe next cell reads all .json files of the downloaded posts. Then we browse through some interesting data.\n\n# Reading the paths of all JSON files from dest_dir\nimport os\n\njson_files = []\n\nfor subdir, dirs, files in os.walk(t):\n    for file in files:\n        fullpath = os.path.join(subdir, file)\n        filename, file_extension = os.path.splitext(fullpath)\n        if file_extension == \".json\":\n          json_files.append(fullpath)\n\n\n# Reading all JSON files\nfrom tqdm.notebook import tqdm\nimport json\n\njson_data = []\n\nfor file in tqdm(json_files):\n  with open(file, 'r') as f:\n    data = json.load(f)\n    json_data.append(data)\n\n\n\n\nOk, now all metadata for all posts is saved to the variable json_data. Run the next line and copy its output to http://jsonviewer.stack.hu/. Your output should look similar, go ahead and play around to explore your data! What information can you extract?\n\nprint(json.dumps(json_data[0]))\n\n\n\nMetadata Preprocessing\nPosts contain plenty of data, like time and location of the post, the authoring user, a caption, tagged users and more. The following cells demonstrate how to normalize the data into a table format, which is useful when working with pandas. Nevertheless, this is optional!\n\n# Use booleans (True / False) values to select what type of data you'd like to analyse. \nusername = True #@param {type:\"boolean\"}\ntimestamp = True #@param {type:\"boolean\"}\ncaption = True #@param {type:\"boolean\"}\nlocation = True #@param {type:\"boolean\"}\nshortcode = True #@param {type:\"boolean\"}\nid = True #@param {type:\"boolean\"}\ntagged_users = True #@param {type:\"boolean\"}\n\nNext we loop through the data and create a new pandas DataFrame. The DataFrame will have one column for each variable selected above and one row for each downloaded posts.\nIf you are not yet familiar with the concept of dataframes have a look at YouTube, there‚Äôs plenty of introductory videos available.\n\nimport pandas as pd\n\nposts = []  # Initializing an empty list for all posts\nfor post in tqdm(json_data):\n  row = {} # Initializing an empty row for the post\n\n  node = post.get(\"node\")\n\n  if username:\n    owner = node.get(\"owner\")\n    row['username'] = owner.get(\"username\")\n\n  if timestamp:\n    row['timestamp'] = node.get(\"taken_at_timestamp\")\n\n  if location:\n    l = node.get(\"location\", None)\n    if l:\n      row['location'] = l.get(\"name\")\n\n  if shortcode:\n    row['shortcode'] = node.get(\"shortcode\")\n\n  if id:\n    row['id'] = node.get(\"id\")\n  \n  if tagged_users:\n    pass\n\n  if caption:\n    c = \"\"\n    emtc = node.get(\"edge_media_to_caption\")\n    edges = emtc.get(\"edges\")\n    for element in edges:\n      caption_node = element.get(\"node\")\n      c = c + caption_node.get(\"text\")\n    row['caption'] = c\n\n  # Finally add row to posts\n  posts.append(row)\n\n# After looping through all posts create data frame from list\nposts_df = pd.DataFrame.from_dict(posts)\n\nNow all information selected above is saved to the dataframe posts_df. Run the next cell and it will return a nicely formatted table. If your data is quite long, output will be cropped. Click the wand and after a few seconds you are able to browse through the data or filter by columns\n\nposts_df\n\nIn order to get a first impression of dataframes, the head() method is also useful. Run the next cell to see the result\n\nposts_df.head()\n\nThe dataframe is only saved in memory, thus when disconnecting and deleting the runtime, the dataframe is lost. Running the next cell saves the table to a CSV-file on your drive.\nNow the processed data may be recovered or used in another notebook.\n\nposts_df.to_csv('{}{}.csv'.format(dest_dir, username))"
  },
  {
    "objectID": "data-collection/ig-posts.html#crowdtangle",
    "href": "data-collection/ig-posts.html#crowdtangle",
    "title": "Instagram Posts",
    "section": "CrowdTangle",
    "text": "CrowdTangle\n\n\n\nScreenshot of the CrowdTangle interface.\n\n\nCrowdTangle is the best option to collect IG posts ‚Äì in theory. It provides legal access to Instagram data and offers several tools to export large amount of data. For a current project we‚Äôve exported more than 500.000 public posts through a hashtag query. Unfortunately there are several restrictions: CrowdTangle is the best tool to export metadata of public posts, and captions. The abilty to collect images through the platform is limited: Image links expire after a certain amount of time, thus we need to use some makeshift approach to download the images. When we can download the images, it‚Äôs always just one per post, no matter if it‚Äôs a gallery or a single image. And let‚Äôs not talk about videos. I have written another Medium story with a step-by-step guide to CrowdTangle.\n\n\nPros:\n\n\nLegal Access\n\n\nWe can select the time frame for export\n\n\nExport in CSV format\n\n\n\n\nCons:\n\n\nOnly access to one image for album posts\n\n\nLimited access to historical images, the browsing to the bottom strategy is limited\n\n\nNo videos for newer posts"
  },
  {
    "objectID": "data-collection/ig-posts.html#zeeschuimer-4cat",
    "href": "data-collection/ig-posts.html#zeeschuimer-4cat",
    "title": "Instagram Posts",
    "section": "Zeeschuimer & 4CAT",
    "text": "Zeeschuimer & 4CAT\n\n\n\nScreenshot of Zeeschuimer\n\n\nZeeschuimer (Peeters, n.d.) and 4CAT (Peeters, Hagen, and Wahl, n.d.) are two tools developed for the https://wiki.digitalmethods.net/. The first is a firefox plugin that captures traffic when browsing websites likes Instagram or TikTok. The second, 4CAT, is an analysis platform incorporating several steps of preprocessing and further analyses. For post collection we can use the original Zeeschuimer Firefox Plugin, download the latest release from GitHub and install it in Firefox. To download Instagram posts using Zeeschuimer follow these steps (* steps are only necessary when working with 4CAT):\n\nDownload and install Firefox\nDownload and install the Plugin\n*Register a 4CAT Account\nActivate the Instagram (Posts) Switch.\n*Fill out the 4CAT server URL field (https://4cat.digitalhumanities.io/).\nOpen Instagram in a new tab. Browse the profiles you‚Äôre interested in. Keep scrolling to the bottom of the profile until you reach posts at the end of your period of investigation.\nDownload the data from the plugin or export the data to 4CAT.\n\n\n\nPros:\n\n\nWe do not infringe the TOS\n\n\nCan collect data from private profiles\n\n\nWe can collect all media, also albums and videos\n\n\n\n\nCons:\n\n\nWe need to browse through the profiles\n\n\nPractical limitations (e.g.¬†volume, timeframe, # of profiles ‚Ä¶)\n\n\n\n\n\nWorking with 4CAT\n\n\n\nScreenshot of 4CAT\n\n\n4CAT is a tool developed by the Digital Methods Initiative. The collected data can be exported to 4CAT with only the click of a button. After successfully importing the post data, the tools offers several modules. At first, download the images associated with each post with the Download images module at the bottom. Select image_url in the options tab and hit Run.\n\n\n\nAvailable modules for visual analysis using 4CAT\n\n\nOnce the images have been downloaded more analysis options are available when clicking the More button on the right. Further, you may download images as a ZIP file and can export the posts from 4CAT in CSV format. Repeat the process with the Download Video function to access posted videos. We will be able to use the collected data using the CSV export and the media files provided in the ZIP packages. Additionally, each ZIP file contains a .metadata.json file which we may use to map filenames to media files.\nThe authors of Zeeschuimer and 4CAT have published a manual here.\n\n\nWorking with Python\nData collected using Zeeschuimer can also be exported as ndjson files. The Zeeschuimer Import notebook provides a code example for reading the files and converting them to either 4CAT format, or a table format compatible with the above notebooks for CrowdTangle and instaloader.\n\n\n\n\n\n\nWork-In-Progress\n\n\n\nWe could download multiple images / videos for albums with little refactoring. We will work on an update if necessary."
  },
  {
    "objectID": "data-collection/ig-posts.html#references",
    "href": "data-collection/ig-posts.html#references",
    "title": "Instagram Posts",
    "section": "References",
    "text": "References\n\n\nPeeters, Stijn. n.d. ‚ÄúZeeschuimer.‚Äù https://doi.org/10.5281/zenodo.8399900.\n\n\nPeeters, Stijn, Sal Hagen, and Dale Wahl. n.d. ‚Äú4CAT Capture and Analysis Toolkit.‚Äù https://doi.org/10.5281/zenodo.8139174."
  },
  {
    "objectID": "processing/classification.html",
    "href": "processing/classification.html",
    "title": "From Documents to Markdown Tables",
    "section": "",
    "text": "The text as data taught us that text is unstructured data, which needs some processing to convert its content into measurable structured data useful for quantitative analyses. This process is for many analyses the operationalization step, where we translate theoretical concepts into measurable quantities (Nguyen et al. 2020). Content analysis, a research method used in social science and other disciplines, provides a well-established framework for all necessary steps towards operationalization, classification (labelling or coding), and evaluation. Content analysis can be conducted qualitatively and quantitatively. D√∂ring and Bortz (2016) define the two as:\nLast session‚Äôs text exploration approaches might be useful in context of qualitative document analyses. For the quantitative approach, however, we need to operationalize our concept of interest (from the theory, or we use operationalization from the literature), and classify our text according to the operationalization. Additionally, we want to evaluate the computational classification, which will be next session‚Äôs topic. For today‚Äôs session, we work with two operationalizations, or measurements, from the literature: 1) Mobilization (Wurst, Pohl, and Ha√üler 2023; Ha√üler, K√ºmpel, and Keller 2021), and 2) Sentiment (M√∏ller et al. 2023; Schmidt et al. 2022)."
  },
  {
    "objectID": "processing/classification.html#classification-using-gpt",
    "href": "processing/classification.html#classification-using-gpt",
    "title": "From Documents to Markdown Tables",
    "section": "Classification using GPT",
    "text": "Classification using GPT\nWe are going to practice text classification using GPT based on operationalization from the literature. As outlined above, we are going to measure sentiment and mobilization. Each variable has different values and applications:\nSentiment analysis, also known as Opinion Mining, is a field within natural language processing (NLP) and linguistics that focuses on identifying and analyzing people‚Äôs opinions, sentiments, evaluations, appraisals, attitudes, and emotions expressed towards various entities like products, services, organizations, individuals, events, and topics (B. Liu 2022). Generally, we can conduct polarity-based and emotion-based sentiment analyses. In today‚Äôs session we are interested in polarity: Schmidt et al. (2022) distinguish between Positive, Negative, Neutral, and Mixed tweets, M√∏ller et al. (2023) use the categories Positive, Negative, and Neutral.\nMobilization, on the other hand, refers to the efforts made by political parties to encourage and activate citizens to participate in the political process. This can include activities such as voting, supporting a campaign, seeking political information, liking and sharing posts on social media, and other forms of civic engagement (Wurst, Pohl, and Ha√üler 2023). The authors distinguish between three types of calls to participate: calls to inform, calls to interact, and calls to support. They also subcategorized offline and online forms of each type of call.\n\nPrompt Engineering\nPrompt engineering is a new technique in machine learning that has grown alongside the development of large pre-trained models, such as foundation models or large language models (LLMs). This method emerged when it was realized that these models work better with well-designed inputs. Prompt engineering is about creating or changing a question or input so the model can more easily find the right information (Gu et al. 2023). It is based on the understanding that different questions can produce more or less accurate results, so adjusting the format and examples of the prompt is key to getting the best results (Zhao et al. 2021). The field of prompt engineering involves different ways of making these prompts. One can decide to create prompts manually or use automated methods (P. Liu et al. 2023). The growth and use of prompt engineering signify a major change in machine learning, deeply linked to the flexibility and wide range of applications of foundation models (Gu et al. 2023)."
  },
  {
    "objectID": "processing/classification.html#zero-shot-classification",
    "href": "processing/classification.html#zero-shot-classification",
    "title": "From Documents to Markdown Tables",
    "section": "Zero-Shot Classification",
    "text": "Zero-Shot Classification\nZero-shot prompting is a method where a model receives only a natural language instruction to perform a task, without any prior examples or demonstrations, which mirrors the way humans often approach tasks, using only textual instructions. This approach emphasizes convenience and the potential for robustness, minimizing the risk of learning spurious correlations that may be present in the training data. However, this method presents significant challenges, as it can be hard even for humans to understand the task requirements without examples (Brown et al. 2020).\n\n\nDesigning the Prompt\nThe literature provides several prompts for sentiment analysis using GPT-models. Let‚Äôs take this example:\n\nSystem prompt: You are an advanced classifying AI. You are tasked with classifying the sentiment of a text. Sentiment can be either positive , negative or neutral.\nPrompt: Classify the following social media comment into either ‚Äònegative‚Äô, ‚Äòneutral‚Äô or ‚Äòpositive‚Äô. Your answer MUST be either one of [‚Äònegative‚Äô, ‚Äòneutral‚Äô, ‚Äòpositive‚Äô]. Your answer must be lowercase.\n‚Äì M√∏ller et al. (2023) (via Borra (n.d.)).\n\nTesting new prompts within the ChatGPT interface turned out as a good practice through my experiments: Without an additional cost we receive a first understanding of the efficacy of the prompt. The following screenshot shows the sentiment analysis prompt used with some random Amazon reviews:\n\n\n\nThe Sentiment Prompt used with a Positive Review (GPT-4.0)\n\n\n\n\n\nThe Sentiment Prompt used with a Negative Review (GPT-4.0)\n\n\nUsing the ChatGPT interface, we can also interact with the model asking for updates:\n\n\n\nUpdating the Prompt using ChatGPT.\n\n\n\nSystem Prompt: You are an advanced classifying AI. Your task is to classify the sentiment of a text. Sentiment can be either ‚Äòpositive‚Äô, ‚Äònegative‚Äô, or ‚Äòneutral‚Äô.\nFormatting: After processing the text, the response should be formatted in JSON like this:\n{ \n  \"sentiment\": \"positive\" // or \"negative\" or \"neutral\"`\n}\nPlease classify the following social media comment into either ‚Äònegative‚Äô, ‚Äòneutral‚Äô, or ‚Äòpositive‚Äô. Your answer MUST be one of [‚Äònegative‚Äô, ‚Äòneutral‚Äô, ‚Äòpositive‚Äô], and it should be presented in lowercase within a JSON format.\nText: [Insert the text here]\n\nNext, let‚Äôs use our improved prompt in the playground to test the differntiation between system prompt and user prompt:\n\n\n\n\n\n\nTip\n\n\n\nSet the temperature variable to 0 for more consistent model output.\n\n\n\n\n\nTesting the Sentiment Analysis in the Playground\n\n\n\n\nImplementing the Prompt using Python\n\n\nLet‚Äôs read last week‚Äôs Text DataFrame\n\nimport pandas as pd\n\ndf = pd.read_csv('/content/drive/MyDrive/2023-12-01-Export-Posts-Text-Master.csv')\n\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nshortcode\nText\nText Type\nPolicy Issues\n\n\n\n\n0\n0\nCyMAe_tufcR\n#Landtagswahl23 ü§©üß°üôè #FREIEW√ÑHLER #Aiwanger #Da...\nCaption\n['1. Political parties:\\n- FREIEW√ÑHLER\\n- Aiwa...\n\n\n1\n1\nCyL975vouHU\nDie Landtagswahl war f√ºr uns als Liberale hart...\nCaption\n['Landtagswahl']\n\n\n2\n2\nCyL8GWWJmci\nNach einem starken Wahlkampf ein verdientes Er...\nCaption\n['1. Wahlkampf und Wahlergebnis:\\n- Wahlkampf\\...\n\n\n3\n3\nCyL7wyJtTV5\nSo viele Menschen am Odeonsplatz heute mit ein...\nCaption\n['Israel', 'Terrorismus', 'Hamas', 'Entwicklun...\n\n\n4\n4\nCyLxwHuvR4Y\nHerzlichen Gl√ºckwunsch zu diesem grandiosen Wa...\nCaption\n['1. Wahlsieg und Parlamentseinstieg\\n- Wahlsi...\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nSetup for GPT\n\n!pip install -q openai backoff gpt-cost-estimator\n\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 221.4/221.4 kB 3.2 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75.0/75.0 kB 7.9 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.0/2.0 MB 12.1 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 76.9/76.9 kB 7.8 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 58.3/58.3 kB 6.2 MB/s eta 0:00:00\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nllmx 0.0.15a0 requires cohere, which is not installed.\n\n\nWe‚Äôre using the new Colab Feature to store keys safely within the Colab Environment. Click on the key on the left to add your API key and enable it for this notebook. Enter the name of your API-Key in the api_key_name variable.\n\nimport openai\nfrom openai import OpenAI\nfrom google.colab import userdata\nimport backoff\nfrom gpt_cost_estimator import CostEstimator\n\napi_key_name = \"openai-lehrstuhl-api\"\napi_key = userdata.get(api_key_name)\n\n# Initialize OpenAI using the key\nclient = OpenAI(\n    api_key=api_key\n)\n\n@CostEstimator()\ndef query_openai(model, temperature, messages, mock=True, completion_tokens=10):\n    return client.chat.completions.create(\n                      model=model,\n                      temperature=temperature,\n                      messages=messages,\n                      max_tokens=600)\n\n# We define the run_request method to wrap it with the @backoff decorator\n@backoff.on_exception(backoff.expo, (openai.RateLimitError, openai.APIError))\ndef run_request(system_prompt, user_prompt, model, mock):\n  messages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": user_prompt}\n  ]\n\n  return query_openai(\n          model=model,\n          temperature=0.0,\n          messages=messages,\n          mock=mock\n        )\n\nNext, we create a system prompt describing what we want to classify. For further examples of prompts and advice on prompt engineering see e.g.¬†the prompting guide and further resources linked at the bottom of the page.\nFor the moment we are going to use the prompt from the literature.\nDo not forget the Prompt Archive when experimenting. Share your successfull prompt with us!\n\nsystem_prompt = \"\"\"\nYou are an advanced classifying AI. Your task is to classify the sentiment of a text. Sentiment can be either ‚Äòpositive‚Äô, ‚Äònegative‚Äô, or ‚Äòneutral‚Äô.\n\"\"\"\n\n\nprompt = \"\"\"\nPlease classify the following social media comment into either ‚Äònegative‚Äô, ‚Äòneutral‚Äô, or ‚Äòpositive‚Äô. Your answer MUST be one of [‚Äònegative‚Äô, ‚Äòneutral‚Äô, ‚Äòpositive‚Äô], and it should be presented in lowercase.\nText: [TEXT]\n\"\"\"\n\n\n\nRunning the request.\nThe following code snippet uses my gpt-cost-estimator package to simulate API requests and calculate a cost estimate. Please run the estimation whne possible to asses the price-tag before sending requests to OpenAI! Make sure run_request and system_prompt (see Setup for GPT) are defined before this block by running the two blocks above!\nFill in the MOCK, RESET_COST, COLUMN, SAMPLE_SIZE, and MODEL variables as needed (see comments above each variable.)\n\nfrom tqdm.auto import tqdm\n\n#@markdown Do you want to mock the OpenAI request (dry run) to calculate the estimated price?\nMOCK = False # @param {type: \"boolean\"}\n#@markdown Do you want to reset the cost estimation when running the query?\nRESET_COST = True # @param {type: \"boolean\"}\n#@markdown What's the column name to save the results of the data extraction task to?\nCOLUMN = 'Sentiment' # @param {type: \"string\"}\n#@markdown Do you want to run the request on a smaller sample of the whole data? (Useful for testing). Enter 0 to run on the whole dataset.\nSAMPLE_SIZE = 25 # @param {type: \"number\", min: 0}\n\n#@markdown Which model do you want to use?\nMODEL = \"gpt-3.5-turbo-0613\" # @param [\"gpt-3.5-turbo-0613\", \"gpt-4-1106-preview\", \"gpt-4-0613\"] {allow-input: true}\n\n\n# Initializing the empty column\nif COLUMN not in df.columns:\n  df[COLUMN] = None\n\n# Reset Estimates\nCostEstimator.reset()\nprint(\"Reset Cost Estimation\")\n\nfiltered_df = df.copy()\n\n# Skip previously annotated rows\nfiltered_df = filtered_df[pd.isna(filtered_df[COLUMN])]\n\nif SAMPLE_SIZE &gt; 0:\n  filtered_df = filtered_df.sample(SAMPLE_SIZE)\n\nfor index, row in tqdm(filtered_df.iterrows(), total=len(filtered_df)):\n    try:\n        p = prompt.replace('[TEXT]', row['Text'])\n        response = run_request(system_prompt, p, MODEL, MOCK)\n\n        if not MOCK:\n          # Extract the response content\n          # Adjust the following line according to the structure of the response\n          r = response.choices[0].message.content\n\n          # Update the 'new_df' DataFrame\n          df.at[index, COLUMN] = r\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        # Optionally, handle the error (e.g., by logging or by setting a default value)\n\nprint()\n\nReset Cost Estimation\nCost: $0.0002 | Total: $0.0069\n\n\n\n\n\n\ndf[~pd.isna(df['Sentiment'])].head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nshortcode\nText\nText Type\nPolicy Issues\nSentiment\n\n\n\n\n6\n6\nCyLt56wtNgV\nViele gemischte Gef√ºhle waren das gestern Aben...\nCaption\n['Demokratie']\nnegative\n\n\n27\n27\nCyKwo3Ft6tp\nSwipe dich r√ºckw√§rts durch die Kampagne ‚ú®\\n\\nü§Ø...\nCaption\n['Soziale Gerechtigkeit']\npositive\n\n\n29\n29\nCyKwBKcqi31\n#FREIEW√ÑHLER jetzt zweite Kraft in Bayern! Gro...\nCaption\n['St√§rkung der Demokratie', 'Sorgen der B√ºrger...\npositive\n\n\n66\n66\nCyIjC3QogWT\nIn einer gemeinsamen Erkl√§rung der Parteivorsi...\nCaption\n['Israel']\npositive\n\n\n212\n212\nCyAmHU7qlVc\n#FREIEW√ÑHLER #Aiwanger\nCaption\nNaN\nneutral\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# Save Results\ndf.to_csv('/content/drive/MyDrive/2023-12-01-Export-Posts-Text-Master.csv')\n\nLet‚Äôs plot the result for a first big picture\n\n\nimport matplotlib.pyplot as plt\n\n# Count the occurrences of each sentiment\nsentiment_counts = df['Sentiment'].value_counts()\n\n# Create a bar chart\nsentiment_counts.plot(kind='bar')\n\n# Adding labels and title\nplt.xlabel('Sentiment')\nplt.ylabel('Count')\nplt.title('Sentiment Counts')\n\n# Show the plot\nplt.show()\n\n\n\n\n\nSource: GPT Text Classification"
  },
  {
    "objectID": "processing/classification.html#zero-shot-multiclass",
    "href": "processing/classification.html#zero-shot-multiclass",
    "title": "From Documents to Markdown Tables",
    "section": "Zero-Shot Multiclass",
    "text": "Zero-Shot Multiclass\nSo far we have been using one request for exactly one classification. Additionally, our classification has been a categorical variable (sentiment). Since GPT natively speaks JSON as well as other file formats, we can easily request our responses to be formated in JSON. As such, we can request the model to return not just one classification at a time, but multiple classifications simultaneously. Above I introduced two theoretically motivated operationalizations. The second example, mobilization, can be measured e.g.¬†as direct vs.¬†indirect calls to action, or online or offline calls. We could model this question as two categorical classification tasks (direct/indirect/NA, online/offline/NA). My example below makes use of so-called dummy variables, where the presence or absence of each value is coded using 1 or 0 (True or False), as a boolean variable. The dummy variables simplifies the prompt and allow cases, where multiple types of calls to action are used in one text.\nPrompting for multiclass classification works well when defining the output format to adhere strict formatting rules, for more complex use-cases I recommend the guardrails package. The second step is to intpret the GPT response in the right, in our case, to use the json package. This is an error-prone process (image the model to retun None instead of {})! Make use of python errors and exceptions to guard your loop against runtime errors. The example below expects all values in the COLUMNS variable to be part of the JSON object returned from the model and saves the result in df‚Äôs column of the same name. Python‚Äôs dynamic typing usually takes care of casting the model result to boolean, further down the stream we might have to cast the columns manually (i.e.¬†after saving and loading the df from csv.)\n\n\nsystem_prompt = \"\"\"\nYou're an expert in detecting calls-to-action (CTAs) from texts.\n**Objective:**\nDetermine the presence or absence of explicit and implicit CTAs within German-language content sourced from Instagram texts such as posts, stories, video transcriptions, and captions related to political campaigns from the given markdown table.\n**Instructions:**\n1. Examine each user input as follows:\n2. Segment the content into individual sentences.\n3. For each sentence, identify:\n   a. Explicit CTA: Direct requests for an audience to act which are directed at the reader, e.g., \"beide Stimmen CDU!\", \"Am 26. September #FREIEW√ÑHLER in den #Bundestag w√§hlen.\"\n   b. Explicit CTA: A clear direction on where or how to find additional information, e.g. \"Mehr dazu findet ihr im Wahlprogramm auf fdp.de/vielzutun\", \"Besuche unsere Website f√ºr weitere Details.\"\n   c. Implicit CTA: Suggestions or encouragements that subtly propose an action directed at the reader without a direct command, e.g., \"findet ihr unter dem Link in unserer Story.\"\n4. Classify whether an online or offline action is referrenced.\n5. CTAs should be actions that the reader or voter can perform directly, like voting for a party, clicking a link, checking more information, etc. General statements, assertions, or suggestions not directed at the reader should not be classified as CTAs.\n5. Return boolean variables for Implicit CTAs (`Implicit`), Explicit CTAs (`Explicit`), `Online`, and `Offline` as a JSON objet.\n**Formatting:**\nOnly return the JSON object, nothing else. Do not repeat the text input.\n\"\"\"\n\nThe following code snippet uses my gpt-cost-estimator package to simulate API requests and calculate a cost estimate. Please run the estimation whne possible to asses the price-tag before sending requests to OpenAI!\nNote: This code block adds some logic to deal with multiple variables contained in the JSON object: {\"Implicit\": false, \"Explicit\": false, \"Online\": false, \"Offline\": false}. We add the columns Implicit, Explicit, Online, and Offline accordingly. To classify different variables the code need to be modified accordingly. ChatGPT can help with this task!\nFill in the MOCK, RESET_COST, SAMPLE_SIZE, COLUMNS and MODEL variables as needed (see comments above each variable.)\n\nfrom tqdm.auto import tqdm\nimport json\n\n#@markdown Do you want to mock the OpenAI request (dry run) to calculate the estimated price?\nMOCK = False # @param {type: \"boolean\"}\n#@markdown Do you want to reset the cost estimation when running the query?\nRESET_COST = True # @param {type: \"boolean\"}\n#@markdown Do you want to run the request on a smaller sample of the whole data? (Useful for testing). Enter 0 to run on the whole dataset.\nSAMPLE_SIZE = 5 # @param {type: \"number\", min: 0}\n\n#@markdown Which model do you want to use?\nMODEL = \"gpt-3.5-turbo-0613\" # @param [\"gpt-3.5-turbo-0613\", \"gpt-4-1106-preview\", \"gpt-4-0613\"] {allow-input: true}\n\n#@markdown Which variables did you define in your Prompt?\nCOLUMNS = [\"Implicit\", \"Explicit\", \"Online\", \"Offline\"] # @param {type: \"raw\"}\n\n# This method extracts the four variables from the response.\ndef extract_variables(response_str):\n    # Initialize the dictionary\n    extracted = {}\n\n    for column in COLUMNS:\n      extracted[column] = None\n\n    try:\n        # Parse the JSON string\n        data = json.loads(response_str)\n\n        for column in COLUMNS:\n          # Extract variables\n          extracted[column] = data.get(column, None)\n\n        return extracted\n\n    except json.JSONDecodeError:\n        # Handle JSON decoding error (e.g., malformed JSON)\n        print(\"Error: Response is not a valid JSON string.\")\n        return extracted\n    except KeyError:\n        # Handle cases where a key is missing\n        print(\"Error: One or more keys are missing in the JSON object.\")\n        return extracted\n    except Exception as e:\n        # Handle any other exceptions\n        print(f\"An unexpected error occurred: {e}\")\n        return extracted\n\n\n# Initializing the empty column\nif COLUMN not in df.columns:\n  df[COLUMN] = None\n\n# Reset Estimates\nCostEstimator.reset()\nprint(\"Reset Cost Estimation\")\n\nfiltered_df = df.copy()\n\n# Skip previously annotated rows\nfiltered_df = filtered_df[pd.isna(filtered_df[COLUMN])]\n\nif SAMPLE_SIZE &gt; 0:\n  filtered_df = filtered_df.sample(SAMPLE_SIZE)\n\nfor index, row in tqdm(filtered_df.iterrows(), total=len(filtered_df)):\n    try:\n        p = row['Text']\n        response = run_request(system_prompt, p, MODEL, MOCK)\n\n        if not MOCK:\n          # Extract the response content\n          # Adjust the following line according to the structure of the response\n          r = response.choices[0].message.content\n          extracted = extract_variables(r)\n\n          for column in COLUMNS:\n            df.at[index, column] = extracted[column]\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        # Optionally, handle the error (e.g., by logging or by setting a default value)\n\nprint()\n\nReset Cost Estimation\nCost: $0.0191 | Total: $0.0838\n\n\n\n\n\n\ndf[~pd.isna(df['Implicit'])]\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nshortcode\nText\nText Type\nPolicy Issues\nCall\nImplicit\nExplicit\nOnline\nOffline\n\n\n\n\n442\n442\nCxxXJBtAHhv\nFriedrich Merz ist nicht gerade bekannt f√ºr se...\nCaption\n['Asylbewerberleistungsgesetz', 'Zahnsanierung...\nNone\nFalse\nFalse\nFalse\nFalse\n\n\n453\n453\nCxvqTwmtlJK\nDamit es uns nicht so ergeht wie den Indianern...\nCaption\nNaN\nNone\nFalse\nTrue\nFalse\nTrue\n\n\n494\n494\nCxs9ujENMqI\nüîπ#Krankenh√§userüîπ#Geburtsstationen und üîπ#Hebamm...\nCaption\n['Krankenh√§user', 'Geburtsstationen', 'Hebamme...\nNone\nFalse\nTrue\nFalse\nTrue\n\n\n839\n839\nCxWF0mcqrhg\nUnterwegs im oberbayerischen Moosburg: Herzlic...\nCaption\nNaN\nNone\nFalse\nTrue\nFalse\nTrue\n\n\n1818\n1818\nCxvKsBBos0j\n9801 Bayerische Staatsregierung MISSION 7272 9...\nOCR\nNaN\nNone\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nSource: GPT Text Classification"
  },
  {
    "objectID": "processing/classification.html#few-shot-classification",
    "href": "processing/classification.html#few-shot-classification",
    "title": "From Documents to Markdown Tables",
    "section": "Few-Shot Classification",
    "text": "Few-Shot Classification\nFew-shot learning, involves presenting a model with a small number of task demonstrations at inference time. The number of examples is constrained by the model‚Äôs context window capacity. The primary advantage of few-shot learning is the significant reduction in the need for task-specific data, alongside minimizing the risk of learning a narrow distribution from a large, but limited, fine-tuning dataset. However, this method has shown inferior performance compared to state-of-the-art fine-tuned models and still requires a minimal amount of task-specific data (Brown et al. 2020).\n\n\nsystem_prompt = \"\"\"\nYou are an advanced classifying AI. Your task is to classify the sentiment of a text. Sentiment can be either ‚Äòpositive‚Äô, ‚Äònegative‚Äô, or ‚Äòneutral‚Äô.\n**Examples:**\n\"Wir sind EIN Volk! üá©üá™ In Leipzig nahm es den Anfang, breitete sich aus wie ein Lauffeuer und ebnete den Weg f√ºr die deutsche Einheit. Was damals viel Arbeit war, zahlte sich aus. Was heute noch Arbeit ist, wird sich auszahlen. Ein geeintes Deutschland ist keine Selbstverst√§ndlichkeit und wir sind dankbar f√ºr die Demokratie, den Rechtsstaat und unsere freiheitliche Gesellschaft. Und wir arbeiten t√§glich daf√ºr, dass uns diese Werte erhalten bleiben.\": positive\n\"FREIE W√ÑHLER Wir FREIE W√ÑHLER k√§mpfen f√ºr eine fl√§chendeckende Gesundheitsversorgung auch auf dem Land. HUBERT AJUANGER\": neutral\n\"Die #Gr√ºnen sind mit daf√ºr verantwortlich, dass die #Ampel-Regierung in Berlin meilenweit an der Lebenswirklichkeit der Menschen vorbei regiert. Ausgerechnet unter einem gr√ºnen Klimaminister l√§sst die Akzeptanz f√ºr #Klimaschutz in der Gesellschaft nach. Mit uns wird es keine Gr√ºnen in der Bayerischen Staatsregierung geben.\": negative\n\"\"\"\n\n\nprompt = \"\"\"\nPlease classify the following social media comment into either ‚Äònegative‚Äô, ‚Äòneutral‚Äô, or ‚Äòpositive‚Äô. Your answer MUST be one of [‚Äònegative‚Äô, ‚Äòneutral‚Äô, ‚Äòpositive‚Äô], and it should be presented in lowercase.\nText: [TEXT]\n\"\"\"\n\nThe following code snippet uses my gpt-cost-estimator package to simulate API requests and calculate a cost estimate. Please run the estimation whne possible to asses the price-tag before sending requests to OpenAI! Make sure run_request and system_prompt are defined before this block by running the two blocks above (see Setup for GPT)!\nFill in the MOCK, RESET_COST, COLUMN, SAMPLE_SIZE, and MODEL variables as needed (see comments above each variable.)\n\nfrom tqdm.auto import tqdm\n\n#@markdown Do you want to mock the OpenAI request (dry run) to calculate the estimated price?\nMOCK = False # @param {type: \"boolean\"}\n#@markdown Do you want to reset the cost estimation when running the query?\nRESET_COST = True # @param {type: \"boolean\"}\n#@markdown What's the column name to save the results of the data extraction task to?\nCOLUMN = 'Sentiment' # @param {type: \"string\"}\n#@markdown Do you want to run the request on a smaller sample of the whole data? (Useful for testing). Enter 0 to run on the whole dataset.\nSAMPLE_SIZE = 25 # @param {type: \"number\", min: 0}\n\n#@markdown Which model do you want to use?\nMODEL = \"gpt-3.5-turbo-0613\" # @param [\"gpt-3.5-turbo-0613\", \"gpt-4-1106-preview\", \"gpt-4-0613\"] {allow-input: true}\n\n\n# Initializing the empty column\nif COLUMN not in df.columns:\n  df[COLUMN] = None\n\n# Reset Estimates\nCostEstimator.reset()\nprint(\"Reset Cost Estimation\")\n\nfiltered_df = df.copy()\n\n# Skip previously annotated rows\nfiltered_df = filtered_df[pd.isna(filtered_df[COLUMN])]\n\nif SAMPLE_SIZE &gt; 0:\n  filtered_df = filtered_df.sample(SAMPLE_SIZE)\n\nfor index, row in tqdm(filtered_df.iterrows(), total=len(filtered_df)):\n    try:\n        p = prompt.replace('[TEXT]', row['Text'])\n        response = run_request(system_prompt, p, MODEL, MOCK)\n\n        if not MOCK:\n          # Extract the response content\n          # Adjust the following line according to the structure of the response\n          r = response.choices[0].message.content\n\n          # Update the 'new_df' DataFrame\n          df.at[index, COLUMN] = r\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        # Optionally, handle the error (e.g., by logging or by setting a default value)\n\nprint()\n\nReset Cost Estimation\nCost: $0.0010 | Total: $0.0278\n\n\n\n\n\n\ndf[~pd.isna(df['Sentiment'])].sample(5)\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nshortcode\nText\nText Type\nPolicy Issues\nSentiment\n\n\n\n\n1833\n1833\nCxunhdYNvw3\ntanten\nOCR\nNaN\nneutral\n\n\n2299\n2299\nCxJAr3Ht7mh\nEIN JAHR FEMINISTISCHE REVOLUTION IM IRAN LASS...\nOCR\nNaN\nneutral\n\n\n369\n369\nCx2gzYdIv5d\nWir gratulieren Sven Schulze, der gestern in M...\nCaption\nNaN\npositive\n\n\n1886\n1886\nCxqbrYztMdC\nBerliner Senat; nachdem er rausgefunden hat, d...\nOCR\nNaN\nnegative\n\n\n290\n290\nCx7ruIdiOXb\n#TagderdeutschenEinheit \\n\\nUnser #Bayern hat ...\nCaption\n['LosvonBerlin', 'Bayernpartei']\nnegative\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# Save Results\ndf.to_csv('/content/drive/MyDrive/2023-12-01-Export-Posts-Text-Master.csv')\n\n\nimport matplotlib.pyplot as plt\n\n# Count the occurrences of each sentiment\nsentiment_counts = df['Sentiment'].value_counts()\n\n# Create a bar chart\nsentiment_counts.plot(kind='bar')\n\n# Adding labels and title\nplt.xlabel('Sentiment')\nplt.ylabel('Count')\nplt.title('Sentiment Counts')\n\n# Show the plot\nplt.show()\n\n\n\n\nSource: GPT Text Classification"
  },
  {
    "objectID": "processing/classification.html#saving-money-multidocument-classification",
    "href": "processing/classification.html#saving-money-multidocument-classification",
    "title": "From Documents to Markdown Tables",
    "section": "Saving Money ‚Äì Multidocument Classification",
    "text": "Saving Money ‚Äì Multidocument Classification\nWhen using GPT for text classification using the above prompts, we send one request per text document in our df. Each time, we send the system_prompt and prompt, repeating the same text over and over again. With the code below we try another approach: We send a table with multiple documents at once, thus we just need to send the system_prompt and prompt once every n documents, saving tokens and therefore saving money. Classifications using gpt-3.5 are relatively cheap, and the multidocument classification resulted in small quality drops through my experiments, for gpt-4, however, it cut my expenses drastically. gpt-4-turbo lies inbetween the two, it is still 10 times more expansive than gpt-3.5, yet input tokens are 1/3 of gpt-4 prices. See: https://openai.com/pricing\nVerdict: Always run the mock requests first to estimate cost. For gpt-3.5 sending one document per request is often the best option. For gpt-4 the multidocument approach is often the better option: Cheaper than single-document gpt-4, higher quality than gpt-3.5. (According to my experiments, which have limitations!).\n\nNew System Prompt\nLet‚Äôs get started by creating a new system prompt that incoporates command for the new approach. We need to define the prompt, as we need to calculate the tokens before splitting the textdocuments in tables.\n\n\nsystem_prompt = \"\"\"\nYou are an advanced classifying AI. Your task is to classify the sentiment of a text. Sentiment can be either ‚Äòpositive‚Äô, ‚Äònegative‚Äô, or ‚Äòneutral‚Äô.\n**Instructions**\n  1. Examine each row in the table under the 'Text' column.\n  2. For each row consisting of social media comments, classify the content into either ‚Äònegative‚Äô, ‚Äòneutral‚Äô, or ‚Äòpositive‚Äô.\n  3. Fill the 'Classification' column for the corresponding 'Text' row with your answer. Your answer MUST be one of [‚Äònegative‚Äô, ‚Äòneutral‚Äô, ‚Äòpositive‚Äô], and it should be presented in lowercase.\n**Formatting**\nReturn a markdown table with the columns \"shortcode\" and \"Classification\"\n\"\"\"\n\nWe use the tabulate python package to create markdown tables for as many tables as we manage to send within the model‚Äôs context window. Currently, the result_table token length (the mockup response) is calculated using the length of False. Replace the value if you expect longer classifications in this line:\ncurrent_result_table = tabulate(batched_data + [(row[meta], False)], headers=[meta, \"Classification\"], tablefmt=\"pipe\")\n\nfrom tabulate import tabulate\nfrom datetime import datetime\nfrom gpt_cost_estimator import num_tokens_from_messages\n\ndef batch_rows_for_tables(df, system_prompt, column, meta, model=\"gpt-3.5-turbo-0613\", **kwargs):\n    max_rows = kwargs.get(\"max_rows\", 999)\n    if model == \"gpt-4-0613\":\n      max_tokens = 8192\n\n    if model == \"gpt-4-1106-preview\":\n      max_tokens = 128000 # This model has not been tested with the multidocument approach. It is only capable of 4096 tokens output, therefore we might run into trouble\n\n    if model == \"gpt-3.5-turbo-0613\":\n      max_tokens = 4096\n\n    \"\"\"Batch rows from the dataframe to fit within token limits and return as a list of markdown tables.\"\"\"\n    tables = []\n\n    df[column] = df[column].astype(str)\n\n    pbar = tqdm(total=len(df))\n\n\n    while not df.empty:\n        current_tokens = 0\n        batched_data = []\n        batched_results = []\n\n        i = 0\n        for index, row in df.iterrows():\n            # Remove newline characters from the specific column\n            cleaned_data = row[column].replace('\\n', ' ')\n\n            # Construct the table for the current batch\n            current_table = tabulate(batched_data + [(row[meta], cleaned_data)], headers=[meta, \"Text\"], tablefmt=\"pipe\")\n            current_result_table = tabulate(batched_data + [(row[meta], False)], headers=[meta, \"Classification\"], tablefmt=\"pipe\")\n\n            message = [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": current_table},\n                {\"role\": \"assistant\", \"content\": current_result_table}\n                ]\n\n            tokens_needed = num_tokens_from_messages(message, model=model)\n\n            if tokens_needed &lt;= max_tokens and i &lt; max_rows:\n                current_tokens = tokens_needed\n                batched_data.append((row[meta], cleaned_data))\n                batched_results.append((row[meta], False))\n                df.drop(index, inplace=True)\n                i += 1\n            else:\n                # Stop when you've reached close to the max token count\n                pbar.update(len(batched_data))\n                break\n\n        # Convert batched rows to a markdown table and store in tables list\n        markdown_table = tabulate(batched_data, headers=[meta, \"Text\"], tablefmt=\"pipe\")\n        tables.append(markdown_table)\n\n    pbar.close()\n\n    return tables\n\nThe next command uses the above function to generate all necessary markdown tables. The column parameter of batch_rows_for_tables expects the name of the text column, the meta parameter expects the name of the identifier column. Additionally, we pass the dataframe, system_prompt, and MODEL to the function. Fill in the TEXT_COLUMN, IDENTIFIER, MODEL, and MAX_ROWS variables as needed. See the comments above each variable for more information.\n\n#@markdown What's the column name of the text column?\nTEXT_COLUMN = 'Text' # @param {type: \"string\"}\n#@markdown What's the column name of the text column?\nIDENTIFIER = 'shortcode' # @param {type: \"string\"}\n#@markdown Which model do you want to use?\nMODEL = \"gpt-4-0613\" # @param [\"gpt-3.5-turbo-0613\", \"gpt-4-1106-preview\", \"gpt-4-0613\"] {allow-input: true}\n#@markdown Is there a maximum length of rows? (**Set a very high number, like 999, to disable this feature**)\nMAX_ROWS = 999 # @param {type: \"number\", min:0}\n\n# Create a copy of your df. This is important! The batching process removes processed rows from the df.\ndf_batch_copy = df.copy()\n\n# Batching the tables, takes a few seconds (~1 Minute)\ntables = batch_rows_for_tables(df_batch_copy, system_prompt, TEXT_COLUMN, IDENTIFIER, MODEL, max_rows=MAX_ROWS)\n\n\n\n\nLet‚Äôs inspect the table. This is one of many tables that will be sent to the model. (I set the MAX_ROWS to 5 to keep the example short. When working with this approach I usually use MAX_ROWS=999.)\n\nprint(tables[0])\n\n| shortcode   | Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n|:------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| CyMAe_tufcR | #Landtagswahl23 ü§©üß°üôè #FREIEW√ÑHLER #Aiwanger #Danke #Landtagswahl                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| CyL975vouHU | Die Landtagswahl war f√ºr uns als Liberale hart. Wir haben alles gegeben, um die FDP wieder in den Landtag zu bringen, aber leider hat es nicht gereicht. Danke f√ºr euren Einsatz, egal ob beim Plakatieren, Flyern oder am Infostand. üíõ  Wir Julis stehen f√ºr unsere √úberzeugungen ein, auch wenn es gerade nicht gut l√§uft. Das macht uns aus! Das haben wir in diesem Wahlkampf gezeigt und das werden wir auch in der au√üerparlamentarischen Opposition zeigen. üí™  Du bist auch davon √ºberzeugt, dass Freiheit und Eigenverantwortung eine Stimme in der Politik brauchen? Dann steh auch du jetzt f√ºr diese √úberzeugung ein. Unter www.julis.de/mitglied-werden/ kannst du noch heute Mitglied der besten Jugendorganisation der Welt werden. üöÄ  #freistart23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| CyL8GWWJmci | Nach einem starken Wahlkampf ein verdientes Ergebnis! üí™ Herzlichen Gl√ºckwunsch an die CSU und unsere bayrischen JUler, die in der n√§chsten Legislaturperiode f√ºr ein sicheres und stabiles Bayern arbeiten werden. Wir w√ºnschen euch viel Erfolg und alles Gute f√ºr das Landtagsmandat (v.l.n.r.): Manuel Knoll, Konrad Baur, Daniel Artmann, Kristan von Waldenfels.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| CyL7wyJtTV5 | So viele Menschen am Odeonsplatz heute mit einer klaren Botschaft: Wir stehen an der Seite Israels.   Die massiven und brutalen Angriffe der Terrororganisation Hamas sind abscheuliche Verbrechen an unschuldigen M√§nnern, Frauen und Kindern. Die Bilder und Videos der barbarischen Morde zerrei√üen einem das Herz.   Der Terror der Hamas ist durch nichts zu rechtfertigen und muss sofort gestoppt werden. Israel hat ein v√∂lkerrechtlich verbrieftes Recht auf Selbstverteidigung.  Wir Gedenken den Toten. Wir trauern mit den Familien und Angeh√∂rigen. Und wir bangen und hoffen mit den verschleppten Israelis.   Es ist gut, dass die Bundesregierung die Entwicklungshilfe f√ºr die palestinensischen Gebiete eingefroren hat. Das ist richtig.   Nicht richtig ist, dass Menschen in Deutschland die Angriffe der Hamas auf J√ºdinnen und Juden feiern. Das ist mit nichts zu rechtfertigen und wir verurteilen es aufs sch√§rfste.   Wir hier in Deutschland und Bayern haben noch viel zu tun: Antisemitismus und auch israelbezogener Antisemitismus ist in der Mitte unserer Gesellschaft vorhanden. Es ist die Aufgabe des frisch gew√§hlten Bayerischen Landtags noch mehr gegen Judenhass zu tun.   üì∏ @andreasgregor   #standwithisrael #israel #m√ºnchen #bayern |\n| CyLxwHuvR4Y | Herzlichen Gl√ºckwunsch zu diesem grandiosen Wahlsieg!  Mit allen 12 JU-Direktkandidaten seid ihr in den hessischen Landtag gezogen üéâ Wir gratulieren euch und w√ºnschen euch viel Erfolg f√ºr den Start und die n√§chsten f√ºnf Jahre im Parlament (v.l.n.r.): Kim-Sarah Speer, Frederik Bouffier, Sebastian Sommer, Lucas Schmitz, Sebastian M√ºller, Christin Ziegler, Marie-Sophie K√ºnkel, Maximilian Schimmel, Christoph Mikuschek, Patrick Appel, Maximilian Bathon und Dominik Leyh!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n\n\nWe can also inspect them using Markdown formatting in the notebooks:\n\nfrom IPython.display import Markdown, display\n\ndisplay(Markdown(tables[0]))\n\n\n\n\n\n\n\n\nshortcode\nText\n\n\n\n\nCyMAe_tufcR\n#Landtagswahl23 ü§©üß°üôè #FREIEW√ÑHLER #Aiwanger #Danke #Landtagswahl\n\n\nCyL975vouHU\nDie Landtagswahl war f√ºr uns als Liberale hart. Wir haben alles gegeben, um die FDP wieder in den Landtag zu bringen, aber leider hat es nicht gereicht. Danke f√ºr euren Einsatz, egal ob beim Plakatieren, Flyern oder am Infostand. üíõ Wir Julis stehen f√ºr unsere √úberzeugungen ein, auch wenn es gerade nicht gut l√§uft. Das macht uns aus! Das haben wir in diesem Wahlkampf gezeigt und das werden wir auch in der au√üerparlamentarischen Opposition zeigen. üí™ Du bist auch davon √ºberzeugt, dass Freiheit und Eigenverantwortung eine Stimme in der Politik brauchen? Dann steh auch du jetzt f√ºr diese √úberzeugung ein. Unter www.julis.de/mitglied-werden/ kannst du noch heute Mitglied der besten Jugendorganisation der Welt werden. üöÄ #freistart23\n\n\nCyL8GWWJmci\nNach einem starken Wahlkampf ein verdientes Ergebnis! üí™ Herzlichen Gl√ºckwunsch an die CSU und unsere bayrischen JUler, die in der n√§chsten Legislaturperiode f√ºr ein sicheres und stabiles Bayern arbeiten werden. Wir w√ºnschen euch viel Erfolg und alles Gute f√ºr das Landtagsmandat (v.l.n.r.): Manuel Knoll, Konrad Baur, Daniel Artmann, Kristan von Waldenfels.\n\n\nCyL7wyJtTV5\nSo viele Menschen am Odeonsplatz heute mit einer klaren Botschaft: Wir stehen an der Seite Israels. Die massiven und brutalen Angriffe der Terrororganisation Hamas sind abscheuliche Verbrechen an unschuldigen M√§nnern, Frauen und Kindern. Die Bilder und Videos der barbarischen Morde zerrei√üen einem das Herz. Der Terror der Hamas ist durch nichts zu rechtfertigen und muss sofort gestoppt werden. Israel hat ein v√∂lkerrechtlich verbrieftes Recht auf Selbstverteidigung. Wir Gedenken den Toten. Wir trauern mit den Familien und Angeh√∂rigen. Und wir bangen und hoffen mit den verschleppten Israelis. Es ist gut, dass die Bundesregierung die Entwicklungshilfe f√ºr die palestinensischen Gebiete eingefroren hat. Das ist richtig. Nicht richtig ist, dass Menschen in Deutschland die Angriffe der Hamas auf J√ºdinnen und Juden feiern. Das ist mit nichts zu rechtfertigen und wir verurteilen es aufs sch√§rfste. Wir hier in Deutschland und Bayern haben noch viel zu tun: Antisemitismus und auch israelbezogener Antisemitismus ist in der Mitte unserer Gesellschaft vorhanden. Es ist die Aufgabe des frisch gew√§hlten Bayerischen Landtags noch mehr gegen Judenhass zu tun. üì∏ (andreasgregor?) #standwithisrael #israel #m√ºnchen #bayern\n\n\nCyLxwHuvR4Y\nHerzlichen Gl√ºckwunsch zu diesem grandiosen Wahlsieg! Mit allen 12 JU-Direktkandidaten seid ihr in den hessischen Landtag gezogen üéâ Wir gratulieren euch und w√ºnschen euch viel Erfolg f√ºr den Start und die n√§chsten f√ºnf Jahre im Parlament (v.l.n.r.): Kim-Sarah Speer, Frederik Bouffier, Sebastian Sommer, Lucas Schmitz, Sebastian M√ºller, Christin Ziegler, Marie-Sophie K√ºnkel, Maximilian Schimmel, Christoph Mikuschek, Patrick Appel, Maximilian Bathon und Dominik Leyh!\n\n\n\n\n\n\nRun the Multidocument Request\nhe following code snippet uses my gpt-cost-estimator package to simulate API requests and calculate a cost estimate. Please run the estimation whne possible to asses the price-tag before sending requests to OpenAI!\nFill in the MOCK, RESET_COST, SAMPLE_SIZE, CLASS_NAME, and FILE_NAME variables as needed (see comments above each variable.)\n\nfrom tqdm.auto import tqdm\nimport json\nimport ast\nfrom datetime import datetime\nfrom io import StringIO\n\n#@title Run the Multidocument Request\n#@markdown T\n#@markdown Do you want to mock the OpenAI request (dry run) to calculate the estimated price?\nMOCK = False # @param {type: \"boolean\"}\n#@markdown Do you want to reset the cost estimation when running the query?\nRESET_COST = True # @param {type: \"boolean\"}\n\n#@markdown How many **tables** do you want to send? Enter $0$ for all.\nSAMPLE_SIZE = 1 # @param {type: \"number\", min: 0}\n\n#@markdown Filename for the **new** table that only contains sentiments.\nFILE_NAME = '/content/drive/MyDrive/2023-12-08-Posts-LTW-Sentiment' # @param {type: \"string\"}\n\n#@markdown Name for the classification column\nCLASS_NAME = 'Sentiment' # @param {type: \"string\"}\n\n\ndef safe_literal_eval(value):\n    if isinstance(value, (str, bytes)):\n        try:\n            return ast.literal_eval(value)\n        except ValueError:\n            return value  # or handle the error in another way if you want\n    return value\n\ndef parse_response(response):\n    # Determine if the response is a list or markdown table\n    if ':' in response.split('\\n')[0]:\n        # List\n        lines = [line.strip() for line in response.strip().split('\\n')]\n        data = [(int(line.split(': ')[0]), line.split(': ')[1]) for line in lines]\n        # Convert the parsed data into a DataFrame\n        result_df = pd.DataFrame(data, columns=['uuid', 'Positioning'])\n    else:\n        # Markdown Table\n        csv_data = '\\n'.join([','.join(line.split('|')[1:-1]) for line in response.split('\\n') if line.strip() and not line.startswith('|:')])\n        result_df = pd.read_csv(StringIO(csv_data.strip()), sep=\",\", skipinitialspace=True)\n\n\n    # Striping Whitespaces\n    result_df.columns = [col.strip() for col in result_df.columns]\n    if 'Classification' in result_df.columns:\n        # Renaming the column to fit the rest of the project.\n        result_df = result_df.rename(columns={\"Classification\": CLASS_NAME})\n\n    result_df = result_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n\n    return result_df\n\n\ntry:\n    # Attempt to read the CSV file into a DataFrame\n    new_df = pd.read_csv(FILE_NAME)\nexcept FileNotFoundError:\n    # If the file is not found, create an empty DataFrame with the specified columns\n    new_df = pd.DataFrame(columns=[IDENTIFIER, CLASS_NAME])\n\n# Reset Estimates\nCostEstimator.reset()\nprint(\"Reset Cost Estimation\")\n\nif 0 &lt; SAMPLE_SIZE &lt;= len(tables):\n    filtered_tables = tables[:SAMPLE_SIZE]\nelse:\n    filtered_tables = tables\n\nfor table in tqdm(filtered_tables):\n    result = run_request(system_prompt, table, MODEL, MOCK)\n    if result and not MOCK:\n      # Parsing the data\n      result_df = parse_response(result.choices[0].message.content)\n\n      # Append it to master_df\n      new_df = pd.concat([new_df, result_df], ignore_index=True)\n\n      # Save Progress\n      new_df.to_csv(FILE_NAME, index=False)\n\nprint()\n\nif not MOCK:\n  print(f\"Saved {FILE_NAME}.\")\n\n  new_df = new_df.dropna(subset=[IDENTIFIER])\n  new_df[CLASS_NAME] = new_df[CLASS_NAME].apply(safe_literal_eval)\n  uuid_to_classification = new_df.set_index(IDENTIFIER)[CLASS_NAME].to_dict()\n  mask = df[IDENTIFIER].isin(uuid_to_classification.keys())\n  df.loc[mask, CLASS_NAME] = df.loc[mask, IDENTIFIER].replace(uuid_to_classification)\n\nprint()\n\nReset Cost Estimation\nCost: $0.1408 | Total: $0.1408\nSaved /content/drive/MyDrive/2023-12-08-Posts-LTW-Sentiment.\n\n\n\n\n\n\n\nnew_df.head()\n\n\n  \n    \n\n\n\n\n\n\nshortcode\nSentiment\n\n\n\n\n0\nCyMAe_tufcR\npositive\n\n\n1\nCyL975vouHU\nneutral\n\n\n2\nCyL8GWWJmci\npositive\n\n\n3\nCyL7wyJtTV5\nnegative\n\n\n4\nCyLxwHuvR4Y\npositive\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nThe code above expects the GPT-API to return results in a markdown formatted table (see above). We keep appending the API responses to a new_df where we temporarily store the classifications. For each loop (i.e.¬†each time received a classification), we store the results on Google Drive as a backup, since each result has a price tag. In case of error we can resume the operation later without the need to start all over again. The code above does not provide the necessary logic for that, but you should be able to quickly add it.\nOnce the loop finished, we use the shortcode column from the API response and join the classification data with df:\n\nAnd finally our df looks as follows. As outlined at the start of the text exploration chapter, we want to fill one dataframe piece by piece with more and more classifications.\n\ndf[mask][['shortcode', 'Text', 'Text Type', 'Sentiment']].head()\n\n\n  \n    \n\n\n\n\n\n\nshortcode\nText\nText Type\nSentiment\n\n\n\n\n0\nCyMAe_tufcR\n#Landtagswahl23 ü§©üß°üôè #FREIEW√ÑHLER #Aiwanger #Da...\nCaption\npositive\n\n\n1\nCyL975vouHU\nDie Landtagswahl war f√ºr uns als Liberale hart...\nCaption\nneutral\n\n\n2\nCyL8GWWJmci\nNach einem starken Wahlkampf ein verdientes Er...\nCaption\npositive\n\n\n3\nCyL7wyJtTV5\nSo viele Menschen am Odeonsplatz heute mit ein...\nCaption\nnegative\n\n\n4\nCyLxwHuvR4Y\nHerzlichen Gl√ºckwunsch zu diesem grandiosen Wa...\nCaption\npositive\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nSource: GPT Text Classification"
  },
  {
    "objectID": "processing/classification.html#conclusion",
    "href": "processing/classification.html#conclusion",
    "title": "From Documents to Markdown Tables",
    "section": "Conclusion",
    "text": "Conclusion\nWe have scratched the surface of (textual) content analysis as a foundation for our text classification tasks. Starting our journey with the idea of text as data and following the exploration of textual content, we just added a new instrument to our toolbox for computational social media analysis: text classification. We focused solely on prompting and GPT for the classification tasks. There exist several other approaches (e.g.¬†using BERT and other trasnformer models), and several providers offer cloud services and APIs for classification tasks (e.g.¬†in the Google Cloud). For sentiment analysis there are dedicated models (see Schmidt et al. (2022) for the application of such a model), and even more services and APIs (e.g.¬†on Microsoft Azure).\nAt the same time, the first papers show interesting results when using GPT for text classification (e.g. Brown et al. 2020), with prompt design being accessible for researcher with zero to few experience with machine learning. There is currently a lot of opportunity to experiment with prompts, and to test and evaluate Large Language Models and prompts against fine-tuned and existing models. We are currently missing one last step to setup a complete experiment: The evaluation, which is the next topic of our seminar. While there exists literature about prompting and prompt engineering (see top and further reading), some of the literature has a more technical motivation and is short of practical advice. Through this session I have presented the practical knowledge that I gathered through my last research project (currently under review), which still is experimental. I presented the Zero-Shot and Few-Shot approach, as well as a Zero-Shot Multiclass approach and a Multidocument approach to save money / requests while working with expensive models."
  },
  {
    "objectID": "processing/classification.html#further-reading",
    "href": "processing/classification.html#further-reading",
    "title": "From Documents to Markdown Tables",
    "section": "Further Reading",
    "text": "Further Reading\n\nWhite et al. (2023): A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT\nZamfirescu-Pereira et al. (2023): Why Johnny Can‚Äôt Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts\nCollection of Projects and (Preprint) Papers on Prompt Engineering\nGPT3 Subreddit"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Starting in 2022, several student projects and bachelor and master theses have been exploring Social Media Analysis under my supervision. Each project approached the field with a unique research interest and questions, influenced by interdisciplinary questions and perspectives. On this page I keep track of past and current projects."
  },
  {
    "objectID": "projects/index.html#semester-projects",
    "href": "projects/index.html#semester-projects",
    "title": "Projects",
    "section": "Semester Projects",
    "text": "Semester Projects\n\nDigital Humanities ‚Äì Winter 2022/2023\n\n‚ÄúTrends in Visual Features of NFT Art from the Most Economically Successful Artists on OpenSea‚Äù (Ferah Noor, Mari McCarville)\n‚ÄúBody Positivity auf Instagram ‚Äì Eine qualitative Inhaltsanalyse‚Äù (Anna Ignjatovic, Adela Myslikova, Ronny Retschmeier)\n‚ÄúAktivismus oder Klimaterror? Die Kommentierung der Klimabewegung auf Twitter in der Analyse‚Äù (Marie Ederer, Sebastian Daniel)\n‚ÄúDie Relevanz des Ukraine-Kriegs im nationalen Kontext BILD, SZ und Tagesschau im Vergleich‚Äù (Milena Bach, Tobias Ederer, Sebastian Mi√üler)\n‚ÄúDie Verwendung naturverbundener Farben anhand ausgew√§hlter Food-Influencerinnen‚Äù (Anna Zagel, Mona Meier-to-Krax, Chiara Rahe)\n‚ÄúInstagram Beitr√§ge zum Thema Ukraine von Nachrichtenkan√§len aus verschiedenen L√§ndern (vor und w√§hrend des Krieges im Vergleich)‚Äù (Philipp Pielmeier, Katharina Kampa, Johanna Gr√ºnler)\n‚ÄúDie Vermarktung von ESN-Fitnessprodukten in den Stories auf der Plattform Instagram‚Äù (Kessler Julia, Nett Ellena, Ousseni Oc√©ane, Tra√ül-Wilterius Annika, Umbreit Janosch)\n‚ÄúPolitisches Posten im Rahmen des Krieges in der Ukraine‚Äù (Jakob Berg)\n\n\n\nComputational Analysis of Visual Social Media ‚Äì Winter 2023/2024\nWork in Progress: We will form groups with a final set of topics on October, 30th. We organize our groups and topics on mural."
  },
  {
    "objectID": "projects/index.html#theses",
    "href": "projects/index.html#theses",
    "title": "Projects",
    "section": "Theses",
    "text": "Theses\n\nBachelor\n\n\n\n\n\n\n\n\n\nTitle\nAuthor\nStatus\nYear\n\n\n\n\nSocial Media Analyse von Instagram Stories am Beispiel politischer Akteure w√§hrend der Bundestagswahl 2021\nLisa Hampel\nCompleted\n2021\n\n\nSammlung und Auswertung eines Social-Media-Korpus durch Entwicklung eines Browser-Plugins zur Annotation von Instagram Stories\nRuslan Asabidi\nCompleted\n2021\n\n\nClassification of Multimodal Social Media Crisis Data ‚Äì Evaluation and Comparison of two Multimodal Machine Learning Models\nMarkus Weinberger\nCompleted\n2023\n\n\nReaktionen politischer Akteure auf den russischen Angriffskrieg in Instagram Stories & Videos\nFranka Heinlein\nCompleted\n2023\n\n\nPolitical stories ‚Äì improving face recognition performance for political Instagram story analysis\nPhilip Pirkl\nCompleted\n2023\n\n\nData Donations and Ephemeral Content: Obtaining Instagram-Stories\nTobias Lanzl\nWork in Progress\n2023\n\n\nXu H∆∞·ªõng: An Analysis of Trending TikTok Videos in Vietnam\nThuy-Linh Nguyen\nWork in Progress\n2023\n\n\nEntwicklung eines interaktiven Dashboards zur Echtzeitauswertung der politischen Kommunikation auf Instagram im Landtagswahlkampf 2023\nJonas Ernst\nWork in Progress\n2023\n\n\n\n\n\nMaster\n\n\n\nTitle\nAuthor\nStatus\nYear\n\n\n\n\nUntersuchung der Telegram-Kan√§le der ‚ÄúQuerdenker‚Äù-Bewegung\nTheresa Strohmeier\nCompleted\n2022\n\n\nInvestigating African American Writing and Thought by comparison of two corpora via Distant Reading\nAenne Knierim\nCompleted\n2022\n\n\nSelbstoptimierung vs.¬†Selbstliebe? Eine vergleichende Inhaltsanalyse von Fitspiration- und Bodypositivity-Bildern auf Instagram mit Methoden der automatischen Bildklassifikation\nJulia Glas\nCompleted\n2022\n\n\nAnalyse visueller und textueller Kommunikationsaspekte von deutschen Lifestyle-Influencern auf Instagram und deren Einfluss auf das User Engagement\nNina Dillinger\nCompleted\n2023"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Michael Achmann  Universit√§tsstr. 31  93053 Regensburg  Email: michael.achmann@informatik.uni-regensburg.de  Phone: 0941 / 943 5098"
  },
  {
    "objectID": "about.html#website-owner",
    "href": "about.html#website-owner",
    "title": "About",
    "section": "",
    "text": "Michael Achmann  Universit√§tsstr. 31  93053 Regensburg  Email: michael.achmann@informatik.uni-regensburg.de  Phone: 0941 / 943 5098"
  },
  {
    "objectID": "about.html#copyright-information",
    "href": "about.html#copyright-information",
    "title": "About",
    "section": "Copyright Information",
    "text": "Copyright Information\nCopyright ¬© Michael Achmann 2023. The text-content and supplement materials are licensed unter GNU GPL 3.0.\n\nCitation\nCiting information for the website will follow soon."
  },
  {
    "objectID": "about.html#disclaimer",
    "href": "about.html#disclaimer",
    "title": "About",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe information contained on this website is for general informational purposes only. While we make every effort to keep the information up to date and accurate, we make no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability, or availability concerning the website or the information, products, services, or related graphics contained on the website for any purpose. Any reliance you place on such information is strictly at your own risk."
  },
  {
    "objectID": "about.html#links-to-third-party-websites",
    "href": "about.html#links-to-third-party-websites",
    "title": "About",
    "section": "Links to Third-Party Websites",
    "text": "Links to Third-Party Websites\nThis website may contain links to third-party websites. These links are provided solely for your convenience and do not imply any endorsement, sponsorship, or recommendation by us. We have no control over the content of these websites and assume no responsibility for their accuracy, legality, or content."
  },
  {
    "objectID": "about.html#data-protection",
    "href": "about.html#data-protection",
    "title": "About",
    "section": "Data Protection",
    "text": "Data Protection\nThis Quarto website is hosted on GitHub. While this website does not use any analytics software, GitHub may store cookies on your device. You can change the cookie preferences at any time."
  },
  {
    "objectID": "about.html#trademark-notice",
    "href": "about.html#trademark-notice",
    "title": "About",
    "section": "Trademark Notice",
    "text": "Trademark Notice\nAll trademarks and logos used on this website are the property of their respective owners."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\nIf you have any questions or concerns regarding this impressum or our website, please contact us using the contact information provided above."
  },
  {
    "objectID": "getting-started/tools.html",
    "href": "getting-started/tools.html",
    "title": "Tools and Software",
    "section": "",
    "text": "This page provides an overview of useful tools ‚Äì not just for the visual social media analysis. In our tools session (October 30th) we will take a look at Colab and git / GitHub. As the semester progresses we will learn more tools, which gradually will be added to this page. A first list of software for the future sessions is linked at the bottom of the page."
  },
  {
    "objectID": "getting-started/tools.html#colab",
    "href": "getting-started/tools.html#colab",
    "title": "Tools and Software",
    "section": "Colab",
    "text": "Colab\nColab is a platform created by Google for collaborative work and research. It offers a preconfigured Python development environment with access to popular libraries and tools. It is based on Jupyter notebooks, which allows users to create and share documents that contain live code, equations, visualizations, and narrative text. Throughout the semester I am going to provide code for different applications as Jupyter notebooks, which can easily be accessed and run on Colab.\n\n\n\nA screenshot of Colab\n\n\nColab can be used for free, but it also offers a paid subscription plan called Colab Pro. The pro version offers, among other features, access to GPUs, which are often used for machine learning. We are probably going to use APIs and GPT throughout the semester, if we need to access GPUs we may use schlaubox."
  },
  {
    "objectID": "getting-started/tools.html#obsidian",
    "href": "getting-started/tools.html#obsidian",
    "title": "Tools and Software",
    "section": "Obsidian",
    "text": "Obsidian\nObsidian and Notion are excellent tools for note-taking. My personal recommendation is Obsidian, as it is free for personal use and notes are saved in markdown format on your harddrive. Thus, the software does not require any subscriptions. Use Dropbox, iCloud or Nextcloud to backup your files! Obsidian is a note-taking app based on the concept of interconnected notes.\n\n\n\nA screenshot of Obsidian\n\n\nThe app allows to easily link between notes, it is a flexible and powerful tool with a wide range of plugins available. Thanks to this large amount of plugins, it is also possible to extend its use. I recommend the dataloom plugin to organize excel-like lists, the textgenerator plugin to use GPT within Obsidian, and the Kanban plugin to organize your tasks. Additionally, use Day Planner to create daily todo lists, the citations plugin to organize your literature notes, and the admonition plugin to add visually outstanding text blocks. Use obsidian-git to collaborate using GitHub."
  },
  {
    "objectID": "getting-started/tools.html#open-research-quarto-for-project-documentation",
    "href": "getting-started/tools.html#open-research-quarto-for-project-documentation",
    "title": "Tools and Software",
    "section": "Open Research: Quarto for Project Documentation",
    "text": "Open Research: Quarto for Project Documentation\nQuarto is an innovative open-source scientific and technical publishing system. We can draft our research and projects using Jupyter notebooks or with plain text markdown in our chosen editors. What‚Äôs more, we‚Äôre able to craft dynamic content using Python, and other languages. When it comes to publishing our findings, we can produce reproducible, top-quality articles, presentations, websites, blogs, and books in various formats, including HTML, PDF, MS Word, and ePub. Writing is made easy with Pandoc markdown, letting us include equations, citations, cross-references, figure panels, callouts, and advanced layouts. The source for this website is available on GitHub. When working on your projects you will be able to share milestones using Quarto, telling a story with your data.\n\n\n\nA screenshot of this website opened in Visual Studio Code with quarto running in the terminal\n\n\nYou may clone the repository with git clone git@github.com:michaelachmann/social-media-lab-quarto.git. Add a folder for your projects in the projects folder and create a cover page called index.qmd. See the README for more information on how to commit and push your changes for publication. Everything will be reviewed before publication!"
  },
  {
    "objectID": "getting-started/tools.html#git-github",
    "href": "getting-started/tools.html#git-github",
    "title": "Tools and Software",
    "section": "Git & GitHub",
    "text": "Git & GitHub\nGit is a distributed version control system that enables us to track changes in our codebase, allowing multiple team members to work simultaneously without overwriting each other‚Äôs contributions. By creating and switching between different branches, we can experiment with new features or bug fixes without disturbing the main code. When we‚Äôre ready, merging these changes back into the main branch is straightforward. Moreover, Git‚Äôs history tracking feature ensures that we can always trace back our steps, understand the evolution of our code, and even revert to previous versions if necessary.\n\n\n\nA screenshot of commiting and pushing changes to the repository of this website.\n\n\n\nGitHub, Inc.¬†(/Àà…°…™th åb/[a]) is a platform and cloud-based service for software development and version control using Git, allowing developers to store and manage their code. It provides the distributed version control of Git plus access control, bug tracking, software feature requests, task management, continuous integration, and wikis for every project.[6] Headquartered in California, it has been a subsidiary of Microsoft since 2018. Wikipedia\n\nUsing GitHub, we can manage our projects, collaborate on coding tasks, and track changes seamlessly using Git. The platform can also be used to host research data and can connected to OSF, to provide code and data anonymously to reviewers. Using Zenodo we can create DOIs and provide citable software packages."
  },
  {
    "objectID": "getting-started/tools.html#future-sessions-outlook",
    "href": "getting-started/tools.html#future-sessions-outlook",
    "title": "Tools and Software",
    "section": "Future Sessions ‚Äì Outlook",
    "text": "Future Sessions ‚Äì Outlook\nVisual Exploration\n\nImageJ\nPixPlot\n\nImage Feature Extraction using APIs\n\nMemespector\n\nData Visualization\n\nFigma\nRawgraphs"
  },
  {
    "objectID": "getting-started/index.html",
    "href": "getting-started/index.html",
    "title": "About the Seminar",
    "section": "",
    "text": "The research seminar Computational Analysis of Visual Social Media consists of project-centred work in groups, lectures on theory and practical sessions. Each group will follow their own research interests and datasets. Groups will be formed in the third session, together with preliminary topics. We have participants from different fields, the topics will mirror this interdisciplinarity, roughly drawn from the interesctions of media studies, political science, and communication science. The seminar aims at master students with first knowledge of at least one programming language.\n\n\nBy the end of the semester you should know more about:\n\nThe state of Social Media Research,\ninteresting questions to answer with social media data,\nethical and legal restrictions,\ndevelop operationalizations for visual and textual data,\n\n\n\n\nBy the end of the semester you will be able to:\n\nCollect Instagram stories, posts and TikTok videos,\napply OCR and automatically transcribe videos,\ncomputationally classify text and images using GPT and CLIP,\nevaluate and optimize your classifications using human annotations,\npresent your results.\n\n\n\n\nThe following expectations and criteria must be met to pass the course:\n\nIndependent familiarization with your own scientific topic: Literature research, formulation of research questions, and operationalization.\nWillingness to master new tools (supported by practical units and some provided Jupyter Notebooks).\nActive and regular team collaboration on the individual project.\nContinuous documentation of current progress through a project wiki.\nWriting a project report at the end of the semester.\n\n\n\n\n\nWe will openly document our project progress, incorporating a strong Open Science stance.\nThe project report template is a good starting point for your report and continuous documentation.\nThrough the semester we will come back to the draft and extend it towards the final report.\nThe goal is to publish the report on social-media-lab.net.\n\n\n\n\n\nThe project report will be handed in collaboratively,\nconsists of app. 20 pages,\nfollows the IMRAD1 principle,\nuses APA citation style,\nneeds to be handed in no later than 31.03.2024.\n\n\n\n\n\nSuggest your own project\nPossible Projects:\n\nLandtagswahl BY 2023\n\nIG Stories & Posts\nTikTok\nJugendorganisationen\n\nPolitische Influencer auf TikTok\nWar in Sozialen Medien:\n\nUkraine Invasion\nHamas Angriff auf Israel\n\nFalschinformationen & KI-Generierte Inhalte\n\n\n\n\n\n\nMost course material will be available on social-media-lab.net.\nAdditional material will be provided via GRIPS.\nWe will work collaboratively on the website through the semester.\nThe content is edited using Quarto and Markdown, you will need a GitHub Account.\nPlease provide your GitHub username to get access to the repository.\nAll content will be published under GPL-3."
  },
  {
    "objectID": "getting-started/index.html#what-to-expect-theoretical-skills",
    "href": "getting-started/index.html#what-to-expect-theoretical-skills",
    "title": "About the Seminar",
    "section": "",
    "text": "By the end of the semester you should know more about:\n\nThe state of Social Media Research,\ninteresting questions to answer with social media data,\nethical and legal restrictions,\ndevelop operationalizations for visual and textual data,"
  },
  {
    "objectID": "getting-started/index.html#what-to-expect-practical-skills",
    "href": "getting-started/index.html#what-to-expect-practical-skills",
    "title": "About the Seminar",
    "section": "",
    "text": "By the end of the semester you will be able to:\n\nCollect Instagram stories, posts and TikTok videos,\napply OCR and automatically transcribe videos,\ncomputationally classify text and images using GPT and CLIP,\nevaluate and optimize your classifications using human annotations,\npresent your results."
  },
  {
    "objectID": "getting-started/index.html#class-requirements",
    "href": "getting-started/index.html#class-requirements",
    "title": "About the Seminar",
    "section": "",
    "text": "The following expectations and criteria must be met to pass the course:\n\nIndependent familiarization with your own scientific topic: Literature research, formulation of research questions, and operationalization.\nWillingness to master new tools (supported by practical units and some provided Jupyter Notebooks).\nActive and regular team collaboration on the individual project.\nContinuous documentation of current progress through a project wiki.\nWriting a project report at the end of the semester."
  },
  {
    "objectID": "getting-started/index.html#project-documentation",
    "href": "getting-started/index.html#project-documentation",
    "title": "About the Seminar",
    "section": "",
    "text": "We will openly document our project progress, incorporating a strong Open Science stance.\nThe project report template is a good starting point for your report and continuous documentation.\nThrough the semester we will come back to the draft and extend it towards the final report.\nThe goal is to publish the report on social-media-lab.net."
  },
  {
    "objectID": "getting-started/index.html#project-report",
    "href": "getting-started/index.html#project-report",
    "title": "About the Seminar",
    "section": "",
    "text": "The project report will be handed in collaboratively,\nconsists of app. 20 pages,\nfollows the IMRAD1 principle,\nuses APA citation style,\nneeds to be handed in no later than 31.03.2024."
  },
  {
    "objectID": "getting-started/index.html#project-ideas",
    "href": "getting-started/index.html#project-ideas",
    "title": "About the Seminar",
    "section": "",
    "text": "Suggest your own project\nPossible Projects:\n\nLandtagswahl BY 2023\n\nIG Stories & Posts\nTikTok\nJugendorganisationen\n\nPolitische Influencer auf TikTok\nWar in Sozialen Medien:\n\nUkraine Invasion\nHamas Angriff auf Israel\n\nFalschinformationen & KI-Generierte Inhalte"
  },
  {
    "objectID": "getting-started/index.html#social-media-lab",
    "href": "getting-started/index.html#social-media-lab",
    "title": "About the Seminar",
    "section": "",
    "text": "Most course material will be available on social-media-lab.net.\nAdditional material will be provided via GRIPS.\nWe will work collaboratively on the website through the semester.\nThe content is edited using Quarto and Markdown, you will need a GitHub Account.\nPlease provide your GitHub username to get access to the repository.\nAll content will be published under GPL-3."
  },
  {
    "objectID": "getting-started/index.html#introduction-to-social-media-analysis",
    "href": "getting-started/index.html#introduction-to-social-media-analysis",
    "title": "About the Seminar",
    "section": "Introduction to Social Media Analysis",
    "text": "Introduction to Social Media Analysis\n\nOverview of social media studies\n\nWhich academic disciplines are interested in plattforms like Instagram?\nWhat is their interest, how do they study the user generated content?\nSpecial focus: Political Communication on Instagram\n\nHow to conduct your own literature review\nTheory: Digital Methods & Cultural Analytics\nA short word about ethics & laws"
  },
  {
    "objectID": "getting-started/index.html#getting-started-tools",
    "href": "getting-started/index.html#getting-started-tools",
    "title": "About the Seminar",
    "section": "Getting Started: Tools",
    "text": "Getting Started: Tools\n\nInstallation & Configuration of different tools.\n\nGoogle Colab / Jupyter Notebooks\nQuarto & Markdown for project documentation\nGit & GitHub\nFirefox Plugins\nFigma\nand more"
  },
  {
    "objectID": "getting-started/index.html#data-collection-ig-posts-stories",
    "href": "getting-started/index.html#data-collection-ig-posts-stories",
    "title": "About the Seminar",
    "section": "Data Collection: IG Posts & Stories",
    "text": "Data Collection: IG Posts & Stories\n\nPost types and platform affordances of Instagram\nHow to use Instaloader\nHow to use CrowdTangle2\nCollecting Stories using Zeeschuimer-F and the firebase backend.\nCollecting Posts using Zeeschuimer and 4CAT\n\n\n\n\nScreenshot of the CrowdTangle interface."
  },
  {
    "objectID": "getting-started/index.html#data-collection-tiktok",
    "href": "getting-started/index.html#data-collection-tiktok",
    "title": "About the Seminar",
    "section": "Data Collection: TikTok",
    "text": "Data Collection: TikTok\n\nPost types and platform affordances of TikTok\nCollecting TikToks using Zeeschuimer and 4CAT\n\n\n\n\nScreenshot of the Firebase Backend and Zeeschuimer-F."
  },
  {
    "objectID": "getting-started/index.html#data-preprocessing",
    "href": "getting-started/index.html#data-preprocessing",
    "title": "About the Seminar",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nOCR\nWe are going to use easyocr to detect and recognize text embedded in images, such as posts and stories.\nWe will export the first frame of videos for OCR and further analyses.\nAutomated Transcription\n\nWe will extract any audio from collected videos.\nWe will use whisper to transcribe the audio content of videos"
  },
  {
    "objectID": "getting-started/index.html#textual-exploration",
    "href": "getting-started/index.html#textual-exploration",
    "title": "About the Seminar",
    "section": "Textual Exploration",
    "text": "Textual Exploration\n\nWe will first take a look at the textual data using simple frequency analyses and wordclouds.\nWe will use the GPT-API to explore the textual content of our data.\nOptional we might use BERTopic to explore our textual data."
  },
  {
    "objectID": "getting-started/index.html#operationalization-i",
    "href": "getting-started/index.html#operationalization-i",
    "title": "About the Seminar",
    "section": "Operationalization I",
    "text": "Operationalization I\n\nThis session depends on your own research: By december you should have developed an initial research request and explored related work in order to develop the first operationalization for content analysis.\nWe will learn more about content analysis in this session.\nBased on your research, and the explorations of the previous sesssion, we will develop the first annotation guide.\nThrough the session we will explore how to (efficiently) use GPT for text data annotation."
  },
  {
    "objectID": "getting-started/index.html#data-annotation",
    "href": "getting-started/index.html#data-annotation",
    "title": "About the Seminar",
    "section": "Data Annotation",
    "text": "Data Annotation\n\nIn this session we will import our data into LabelStudio and develop a final annotation manual.\nUsing the manuals and LabelStudio projects we will annotate the data.\nWe will shuffle annotators: Everyone will annotate for another group."
  },
  {
    "objectID": "getting-started/index.html#evaluation-i",
    "href": "getting-started/index.html#evaluation-i",
    "title": "About the Seminar",
    "section": "Evaluation I",
    "text": "Evaluation I\n\nUsing the human annotations we will evaluate the performance of our computational text annotations / information extractions.\nWe can fine-tune our prompts using the annotation data to improve the annotation quality.\nWe will learn how to present and visualize the quality of the model."
  },
  {
    "objectID": "getting-started/index.html#exploration-of-visual-data",
    "href": "getting-started/index.html#exploration-of-visual-data",
    "title": "About the Seminar",
    "section": "Exploration of Visual Data",
    "text": "Exploration of Visual Data\n\nWe will explore different tools to visualize images:\nImageJ\nPixPlot\nMemespector and Gephi3\nThe visualization forms the basis for image classification: In this stage we want to find similarities and differences.\n\n\n\n\nExample of image exploration using PixPlot."
  },
  {
    "objectID": "getting-started/index.html#operationalization-ii",
    "href": "getting-started/index.html#operationalization-ii",
    "title": "About the Seminar",
    "section": "Operationalization II",
    "text": "Operationalization II\n\nOnce more a dive in the literature: This time on visual content analysis.\nCombining the results of our exploration, reserach interest and related work with content analysis, we will develop an annotation manual for the images.\nWe will learn how to use CLIP for image classification\nBased on your previous experience you will create human annotations.\nWe will shuffle annotators: Everyone will annotate for another group.\n\n\n\n\nDecomposition of different layers in a Story by @gruenebayern during the 2023 Bavarian state elections."
  },
  {
    "objectID": "getting-started/index.html#evaluation-ii",
    "href": "getting-started/index.html#evaluation-ii",
    "title": "About the Seminar",
    "section": "Evaluation II",
    "text": "Evaluation II\n\nOnce more we will evaluate the quality of our model,\nand fine-tune our prompts.\nWork in Progress: We might organize this session differently on short notice, depending on the outcomes of my current research project.\nWaiting in Progress: In case of visual GPT being published we might have to adapt.\n\n\n\n\nExample of a visual inspection of classification results: Intermediary results of image types classifications using CLIP for the 2021 federal election. Two out of five stories posted by differnt parties have been misclassified."
  },
  {
    "objectID": "getting-started/index.html#data-wrangling-as-a-conversation",
    "href": "getting-started/index.html#data-wrangling-as-a-conversation",
    "title": "About the Seminar",
    "section": "Data Wrangling as a Conversation",
    "text": "Data Wrangling as a Conversation\n\nThe Advanced Data Analysis mode of ChatGPT is a powerful tool to (quickly) analyze metadata (and more) of social media data.\nWe will give it a shot with some simple analyses, like trends over time.\nExperimental in case we have enough time left, we might try to create a workflow with LangChain and LlamaIndex to chat with our data."
  },
  {
    "objectID": "getting-started/index.html#visual-presentation-of-your-data",
    "href": "getting-started/index.html#visual-presentation-of-your-data",
    "title": "About the Seminar",
    "section": "Visual Presentation of your Data",
    "text": "Visual Presentation of your Data\n\nOur last session of the semester will be all about telling a story with your data.\nWe will use Python (Jupyter Notebooks) to transform our data in CSV files.\nWe will import the data into RAWGraphs to create convincing plots\nWe will use Figma to collaboratively sketch the layout of your project report website."
  },
  {
    "objectID": "getting-started/index.html#footnotes",
    "href": "getting-started/index.html#footnotes",
    "title": "About the Seminar",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIntroduction, Method, Result, Analaysis, Discussion‚Ü©Ô∏é\nI will not be able to provide access to the tool. We can, however, export data for our projects from the platform and you will learn how to use the exported data.‚Ü©Ô∏é\nfollowing Omena‚Äôs concept for cloud vision labels, see related work.‚Ü©Ô∏é"
  },
  {
    "objectID": "evaluation/index.html",
    "href": "evaluation/index.html",
    "title": "Gold Standard Validation",
    "section": "",
    "text": "Warning\n\n\n\nThis chapter is work in progress. It will be available on December 18th 2023.\nIn computational social media analysis, validation is crucial for ensuring the accuracy and reliability of text analysis methods. As highlighted by Birkenmaier, Lechner, and Wagner (2023), validation entails both internal and external processes. Internal validation assess a model‚Äôs plausibility and quality within the data context and tends to rely researchers‚Äô judgment, whereas external validation compares model outputs with external benchmarks, such as human-annotated labels. Baden et al. (2022) further emphasize the significance of continuous evaluation and transparent reporting of validation steps and results. Additionally, they criticize the the frequent oversight in evaluating the validity of the actual measures. This problem arises when researchers focus more on the technical performance of their models, neglecting to verify whether these models accurately represent the intended social phenomena. This gap can lead to results that are statistically sound but lack real-world relevance.\nIn context of our research projects we‚Äôll focus on external validation through non-expert annotations using LabelStudio, a practical aspect of the validation approach. Our focus will be on generating gold standard data for an external validation. This is crucial because external validation, through methods like crowd annotation, directly assesses how well computational models perform against real-world data. The setup of LabelStudio projects and the creation of annotation manuals are key steps in this process, ensuring that the data used for validation is accurately and consistently labeled, providing a solid foundation for assessing model performance. Although the evaluation of the actual measures is important (Baden et al. 2022), our discussion will concentrate on these practical aspects of external validation.\nHaving humans coders annotate your social media content is the first part of generating a gold standard dataset. The second step will the the evaluation of the annotations, to validate their quality. We will use the interrater agreement as measurement for the coherence of our annotations. We will focus on this topic in the next session."
  },
  {
    "objectID": "evaluation/index.html#creating-an-annotation-manual",
    "href": "evaluation/index.html#creating-an-annotation-manual",
    "title": "Gold Standard Validation",
    "section": "Creating an Annotation Manual",
    "text": "Creating an Annotation Manual\nDeveloping an annotation manual for social media text data is an iterative process. We start with a theoretical understanding of the phenomenon to be annotated, and describe it for easy application by annotators, minimizing ambiguity. The process optimally involves multiple rounds of annotation, each refining the guidelines through discussions of disagreements and revisions. Pilot annotations should be done by those familiar with the theory, focusing on major disagreements to refine categories and examples. As guidelines evolve, both the guidelines and annotators improve, while we need to make sure that the guidelines that are understandable even to less trained individuals (Reiter, Willand, and Gius 2019; Reiter, n.d.). In this section I will provide some examples for annotation manuals and some practical adivce to create effective annotation guidelines for social media analysis.\n\n\n\nExample of an annotation workflow with multiple iterations Source\n\n\nPractically speaking, get started by creating a document which can be shared online, e.g.¬†on Google Docs or CodiMD. I suggest to structure you document as follows:\n.\n‚îú‚îÄ‚îÄ Introduction\n‚îÇ   ‚îú‚îÄ‚îÄ Outline the research project:\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ What is your goal?\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Why do you need the help of annotators?\n‚îÇ   ‚îú‚îÄ‚îÄ What can annotators expect?\n‚îÇ   ‚îî‚îÄ‚îÄ How are they being reimbursed? \n‚îú‚îÄ‚îÄ Overview of all steps, e.g.\n‚îÇ   ‚îú‚îÄ‚îÄ How and where to register\n‚îÇ   ‚îî‚îÄ‚îÄ When to act\n‚îú‚îÄ‚îÄ Software Manual (LabelStudio HowTo)\n‚îÇ   ‚îú‚îÄ‚îÄ Annotation Interface\n‚îÇ   ‚îî‚îÄ‚îÄ Keyboard Shortcuts\n‚îú‚îÄ‚îÄ Introduction to Variables\n‚îÇ   ‚îú‚îÄ‚îÄ Definition for each variable / Values\n‚îÇ   ‚îî‚îÄ‚îÄ Instructions for each variable / Values\n‚îî‚îÄ‚îÄ Examples for each Variable\n    ‚îú‚îÄ‚îÄ Positive Examples\n    ‚îî‚îÄ‚îÄ Negative Examples\nMake use of tables, images and formatting to guide the attention of the readers to the right places. Put emphasize on the most important parts of the annotation manual to gain good quality annotations. I have created several annotation projects in the past. The quality of the manuals started evolving as well. Take a look at the examples for a better understanding of good formatting and how to present examples to your annotators:\n\nCoding categories for images (German)\nCoding policy issues for text (German)\nCoding multiple content variables for text (German)\n\nBased on my personal experience I would recommend to:\n\nFocus on few variables per annotation project, due to two reasons: On the one hand it is easiert to read a short annotation manual for one or few variables and then keep annotating. We do not need to keep switching between tasks and thus to look up the definition of the one or the other variable again and again. On the other hand the software which we are going to use (LabelStudio) has a neat keyboard shortcut feature: This enables annotators to quickle select values by pressing buttons on their keyboard. The less options, the better shortcuts can be used (and remembered).\nGenerally speaking: Less is more. Keep the amount of variables low. Stick to one modality at a time. Keep the total amount of annotations per annotator at a manageable level (e.g.¬†2-3 hours of work) and ask the participants to take breaks when coding!\nThrough my annotations B.A. Students reached consistently lower annotation quality than M.A.¬†/ M.Sc. students.\nTo improve the quality I have experimented with Google Forms and generated a quiz. We can provide the correct solutions to questions, ask future annotators to take the quiz and they will receive some feedback on a test round of coding before starting the actual project.\nIn another approach to improve the quality I asked to participants first code a small subsets and take a qualitative look at the results. I gave feedback and resolved conflicts before adding the annotators to my actual project.\n\n\n\n\n\nA negative example: This annotation interface, based on Ha√üler, K√ºmpel, and Keller (2021), slows the annotation process down: Keyboard shortcuts cannot be used efficiently."
  },
  {
    "objectID": "notebooks/ig-instaloader.html",
    "href": "notebooks/ig-instaloader.html",
    "title": "Instagram Posts",
    "section": "",
    "text": "In order to download posts and stories from Instagram, we use the package instaloader. You can install package for python using pip install &lt;package&gt;, the command -q minimizes the output.\n!pip -q install instaloader\n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60 kB 3.0 MB/s eta 0:00:011\n  Building wheel for instaloader (setup.py) ... done\nOnce you install instaloader we log in using your username and password. Session information (not your credentials!) is stored in Google Drive to minimize the need for signing in.\nIn order to minimize the risk for your account to be disabled we suggest creating a new account on your phone before proceeding!\nusername = 'your.username'\n\n# We save the sessionfile to the following directory. Default is the new folder `.instaloader` in your google drive. (This is optional)\nsession_directory = '/content/drive/MyDrive/.instaloader/'\n\nimport instaloader\nfrom os.path import exists\nfrom pathlib import Path\n\n# Creating session directory, if it does not exists yet\nPath(session_directory).mkdir(parents=True, exist_ok=True)\n\nfilename = \"{}session-{}\".format(session_directory, username)\nsessionfile = Path(filename)\n\n\n# Get instance\nL = instaloader.Instaloader(compress_json=False)\n\n# Check if sessionfile exists. If so load session,\n# else login interactively\nif exists(sessionfile):\n  L.load_session_from_file(username, sessionfile)\n\nelse:\n  L.interactive_login(username)\n  L.save_session_to_file(sessionfile)\n\nLoaded session from /content/drive/MyDrive/.instaloader/session-mi_sm_lab05."
  },
  {
    "objectID": "notebooks/ig-instaloader.html#downloading-first-posts",
    "href": "notebooks/ig-instaloader.html#downloading-first-posts",
    "title": "Instagram Posts",
    "section": "Downloading first Posts",
    "text": "Downloading first Posts\nNext, we try to download all posts of a profile. Provide a username and folder:\n\ndest_username = 'some.profile' \ndest_dir = '/content/drive/MyDrive/insta-posts/' # Once more we save the files to Google Drive. Replace this with a local directory if necessary.\n\nt = Path(\"{}{}\".format(dest_dir, dest_username))\nt.mkdir(parents=True, exist_ok=True)\n\nprofile = instaloader.Profile.from_username(L.context, dest_username)\nfor post in profile.get_posts():\n    L.download_post(post, target=t)\n\nWell, you just downloaded your first posts! Open Google Drive and check the folder insta-posts/ (or whatever folder you chose above)! There should be three files for each post, the image, a .json file and a .txt file. The .txt includes the image caption, the .json lots of metadata about the post.\n\nDiving into the metadata\nThe next cell reads all .json files of the downloaded posts. Then we browse through some interesting data.\n\n# Reading the paths of all JSON files from dest_dir\nimport os\n\njson_files = []\n\nfor subdir, dirs, files in os.walk(t):\n    for file in files:\n        fullpath = os.path.join(subdir, file)\n        filename, file_extension = os.path.splitext(fullpath)\n        if file_extension == \".json\":\n          json_files.append(fullpath)\n\n\n# Reading all JSON files\nfrom tqdm.notebook import tqdm\nimport json\n\njson_data = []\n\nfor file in tqdm(json_files):\n  with open(file, 'r') as f:\n    data = json.load(f)\n    json_data.append(data)\n\n\n\n\nOk, now all metadata for all posts is saved to the variable json_data. Run the next line and copy its output to http://jsonviewer.stack.hu/. Your output should look similar, go ahead and play around to explore your data! What information can you extract?\n\nprint(json.dumps(json_data[0]))\n\n\n\nMetadata Preprocessing\nPosts contain plenty of data, like time and location of the post, the authoring user, a caption, tagged users and more. The following cells demonstrate how to normalize the data into a table format, which is useful when working with pandas. Nevertheless, this is optional!\n\n# Use booleans (True / False) values to select what type of data you'd like to analyse. \nusername = True #@param {type:\"boolean\"}\ntimestamp = True #@param {type:\"boolean\"}\ncaption = True #@param {type:\"boolean\"}\nlocation = True #@param {type:\"boolean\"}\nshortcode = True #@param {type:\"boolean\"}\nid = True #@param {type:\"boolean\"}\ntagged_users = True #@param {type:\"boolean\"}\n\nNext we loop through the data and create a new pandas DataFrame. The DataFrame will have one column for each variable selected above and one row for each downloaded posts.\nIf you are not yet familiar with the concept of dataframes have a look at YouTube, there‚Äôs plenty of introductory videos available.\n\nimport pandas as pd\n\nposts = []  # Initializing an empty list for all posts\nfor post in tqdm(json_data):\n  row = {} # Initializing an empty row for the post\n\n  node = post.get(\"node\")\n\n  if username:\n    owner = node.get(\"owner\")\n    row['username'] = owner.get(\"username\")\n\n  if timestamp:\n    row['timestamp'] = node.get(\"taken_at_timestamp\")\n\n  if location:\n    l = node.get(\"location\", None)\n    if l:\n      row['location'] = l.get(\"name\")\n\n  if shortcode:\n    row['shortcode'] = node.get(\"shortcode\")\n\n  if id:\n    row['id'] = node.get(\"id\")\n  \n  if tagged_users:\n    pass\n\n  if caption:\n    c = \"\"\n    emtc = node.get(\"edge_media_to_caption\")\n    edges = emtc.get(\"edges\")\n    for element in edges:\n      caption_node = element.get(\"node\")\n      c = c + caption_node.get(\"text\")\n    row['caption'] = c\n\n  # Finally add row to posts\n  posts.append(row)\n\n# After looping through all posts create data frame from list\nposts_df = pd.DataFrame.from_dict(posts)\n\nNow all information selected above is saved to the dataframe posts_df. Run the next cell and it will return a nicely formatted table. If your data is quite long, output will be cropped. Click the wand and after a few seconds you are able to browse through the data or filter by columns\n\nposts_df\n\nIn order to get a first impression of dataframes, the head() method is also useful. Run the next cell to see the result\n\nposts_df.head()\n\nThe dataframe is only saved in memory, thus when disconnecting and deleting the runtime, the dataframe is lost. Running the next cell saves the table to a CSV-file on your drive.\nNow the processed data may be recovered or used in another notebook.\n\nposts_df.to_csv('{}{}.csv'.format(dest_dir, username))"
  },
  {
    "objectID": "notebooks/whisper-notebook.html",
    "href": "notebooks/whisper-notebook.html",
    "title": "Social Media Lab",
    "section": "",
    "text": "Extract Audio from Video File\nAfter loading the metadta and media files from the Google Drive, we extract the audio from each video file to prepare the automated transcription.\n\n!pip install -q moviepy\n\n\nimport os\n\n# Set audio directory path\naudio_path = \"media/audio/\"\n\n# Check if the directory exists\nif not os.path.exists(audio_path):\n    # Create the directory if it does not exist\n    os.makedirs(audio_path)\n\n\nfrom moviepy.editor import *\n\nfor index, row in df.iterrows():\n    if row['video_file'] != \"\":\n        # Load the video file\n        video = VideoFileClip(row['video_file'])\n        filename = row['video_file'].split('/')[-1]\n\n        # Extract the audio from the video file\n        audio = video.audio\n\n        if audio is not None:\n            sampling_rate = audio.fps\n            current_suffix = filename.split(\".\")[-1]\n            new_filename = filename.replace(current_suffix, \"mp3\")\n\n            # Save the audio to a file\n            audio.write_audiofile(\"{}{}\".format(audio_path, new_filename))\n        else:\n            new_filename = \"No Audio\"\n            sampling_rate = -1\n\n        # Update DataFrame inplace\n        df.at[index, 'audio_file'] = new_filename\n        df.at[index, 'duration'] = video.duration\n        df.at[index, 'sampling_rate'] = sampling_rate\n\n        df.at[index, 'video_file'] = row['video_file'].split('/')[-1]\n\n        # Close the video file\n        video.close()\n\nMoviePy - Writing audio in media/audio/CzD93SEIi-E.mp3\nMoviePy - Done.\n\n\n                                                                      \n\n\nWe‚Äôve extracted the audio content of each video file to a mp3 file in the media/audio folder. The files keep the name of the video file. We added new columns to the metadata for audio duration and sampling_rate. In case the video did not include an audio file, smapling_rateis set to -1, which we use to filter the df when transcribing the files.\n\ndf[df['video_file'] != \"\"].head()\n\n\n  \n    \n\n\n\n\n\n\nid\nthread_id\nparent_id\nbody\nauthor\nauthor_fullname\nauthor_avatar_url\ntimestamp\ntype\nurl\n...\nnum_comments\nnum_media\nlocation_name\nlocation_latlong\nlocation_city\nunix_timestamp\nvideo_file\naudio_file\nduration\nsampling_rate\n\n\n\n\n4\nCzD93SEIi-E\nCzD93SEIi-E\nCzD93SEIi-E\nMitzuarbeiten f√ºr unser Land, Bayern zu entwic...\nmarkus.soeder\nMarkus SoÃàder\nhttps://scontent-fra3-1.cdninstagram.com/v/t51...\n2023-10-31 12:06:23\nvideo\nhttps://www.instagram.com/p/CzD93SEIi-E\n...\n227\n1\nNaN\nNaN\nNaN\n1698753983\nCzD93SEIi-E.mp4\nCzD93SEIi-E.mp3\n67.89\n44100.0\n\n\n\n\n\n1 rows √ó 24 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\nLet‚Äôs update the ZIPed folder to include the audio files.\n\n!zip -r /content/drive/MyDrive/2023-11-24-4CAT-Images-Clean.zip media\n\nupdating: media/ (stored 0%)\nupdating: media/videos/ (stored 0%)\nupdating: media/videos/CzD93SEIi-E.mp4 (deflated 0%)\n  adding: media/audio/ (stored 0%)\n  adding: media/audio/CzD93SEIi-E.mp3 (deflated 1%)\n\n\nAnd save the updated metadata file. Change filename when importing stories here!\n\ndf.to_csv(four_cat_file_path)\n\nTranscriptions using Whisper\n\nThe Whisper model was proposed in Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.\n\n\nThe abstract from the paper is the following:\n\n\n\nWe study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.\n\n\n‚Äì https://huggingface.co/docs/transformers/model_doc/whisper\n\n!pip install -q transformers\n\nThe next code snippet initializes the Whisper model. The transcribe_aduio method is applied to each row of the dataframe where sampling_rate &gt; 0, thus only to those lines with referencees to audio files. Each audio file is transcribed using Whisper, the result, one text string, is saved to the transcript column.\nAdjust the language variable according to your needs! The model is also capable of automated translation, e.g.¬†setting language to english when processing German content results in an English translation of the speech. (Additionally, the task variable accepts translate).\n\nimport torch\nfrom transformers import pipeline, WhisperProcessor, WhisperForConditionalGeneration\nimport librosa\n\n# Set device to GPU if available, else use CPU\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize the Whisper model pipeline for automatic speech recognition\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=\"openai/whisper-large\",\n    chunk_length_s=30,\n    device=device,\n)\n\n# Load model and processor for multilingual support\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n\n# Function to read, transcribe, and handle longer audio files in different languages\ndef transcribe_audio(filename, language='german'):\n    try:\n        # Load and resample audio file\n        audio_path = f\"{audio_folder}/{filename}\"\n        waveform, original_sample_rate = librosa.load(audio_path, sr=None, mono=True)\n        waveform_resampled = librosa.resample(waveform, orig_sr=original_sample_rate, target_sr=16000)\n\n        # Get forced decoder IDs for the specified language\n        forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=\"transcribe\")\n\n        # Process the audio file in chunks and transcribe\n        transcription = \"\"\n        for i in range(0, len(waveform_resampled), 16000 * 30):  # 30 seconds chunks\n            chunk = waveform_resampled[i:i + 16000 * 30]\n            input_features = processor(chunk, sampling_rate=16000, return_tensors=\"pt\").input_features\n            predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n            chunk_transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n            transcription += \" \" + chunk_transcription\n\n        return transcription.strip()\n    except Exception as e:\n        print(f\"Error processing file {filename}: {e}\")\n        return \"\"\n\n\n# Filter the DataFrame (sampling_rates &lt; 0 identify items without audio)\nfiltered_index = df['sampling_rate'] &gt; 0\n\n# Apply the transcription function to each row in the filtered DataFrame\ndf.loc[filtered_index, 'transcript'] = df.loc[filtered_index, 'audio_file'].apply(transcribe_audio)\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\ndf[df['video_file'] != \"\"].head()\n\n\n  \n    \n\n\n\n\n\n\nid\nthread_id\nparent_id\nbody\nauthor\nauthor_fullname\nauthor_avatar_url\ntimestamp\ntype\nurl\n...\nnum_media\nlocation_name\nlocation_latlong\nlocation_city\nunix_timestamp\nvideo_file\naudio_file\nduration\nsampling_rate\ntranscript\n\n\n\n\n4\nCzD93SEIi-E\nCzD93SEIi-E\nCzD93SEIi-E\nMitzuarbeiten f√ºr unser Land, Bayern zu entwic...\nmarkus.soeder\nMarkus SoÃàder\nhttps://scontent-fra3-1.cdninstagram.com/v/t51...\n2023-10-31 12:06:23\nvideo\nhttps://www.instagram.com/p/CzD93SEIi-E\n...\n1\nNaN\nNaN\nNaN\n1698753983\nCzD93SEIi-E.mp4\nCzD93SEIi-E.mp3\n67.89\n44100.0\nIch bitte auf den abgelagerten Vortrag der Maa...\n\n\n\n\n\n1 rows √ó 25 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\n\ndf.loc[4, 'transcript']\n\n'Ich bitte auf den abgelagerten Vortrag der Maa√üen-S√∂der-Entf√ºhlen ein.  Erf√ºllung meiner Amtspflichten, so wahr mir Gott helfe. Ich schw√∂re Treue der Verfassung des Freistaates Bayern, Gehorsam den Gesetzen und gewissenhafte Erf√ºllung meiner Amtspflichten, so wahr mir Gott helfe. Herr Ministerpr√§sident, ich darf Ihnen im Namen des ganzen Hauses ganz pers√∂nlich die herzlichsten Gl√ºckw√ºnsche aussprechen und w√ºnsche Ihnen viel Erfolg und gute Nerven auch bei Ihrer Aufgabe. Herzlichen Dank.  Applaus'\n\n\nOverall, the transcriptions work well. The first sentence above, however, shows that we still can expect misinterpretations."
  },
  {
    "objectID": "notebooks/corpus-analysis-notebook.html",
    "href": "notebooks/corpus-analysis-notebook.html",
    "title": "Social Media Lab",
    "section": "",
    "text": "Among a variety of possibilities, we can, for example, look at the frequencies of the words contained in the corpus or examine the corpus for recurring themes it contains.\nFirst we need to import all the required libraries once again. The Natural Language Toolkit (NLTK) gives us access to a variety of natural language processing functions (e.g.¬†tokenisation, stop word removal, part-of-speech tagging, ‚Ä¶).\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport requests\nimport pandas as pd\n\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n\n\nWhen analysing word frequencies, we can use stop word lists to ignore words that occur frequently but are not relevant to us. We can easily download such a list. However, this can also be individually adapted to the purpose.\n\n# Retrieve Stopwords from Github\nsw_json = requests.get('https://github.com/stopwords-iso/stopwords-de/raw/master/stopwords-de.json')\n\nNow we can tokenise the existing text, remove the stop words or punctuation marks they contain, convert the words to lower case, or use bi-grams in addition to single-word tokens.\nWe then sum up the occurrences of the individual words and make the results available in a DataFrame.\n\ndef word_freq(text, punctuation=False, stop_words = False, lowercasing = False, bigrams = False):\n\n    if punctuation:\n        # Tokenizing, removing punctuation\n        tokens = RegexpTokenizer(r'\\w+').tokenize(text) # https://regexr.com/\n    else:\n        # Tokenizing, w/o removing punctuation\n        # tokens = text.split()\n        tokens = word_tokenize(text)\n\n    if stop_words:\n        # Removing Stopwords\n        tokens = [w for w in tokens if not w.lower() in stop_words]\n\n    if lowercasing:\n        # Lower-Casing\n        tokens = [w.lower() for w in tokens]\n\n    if bigrams:\n        # Converting text tokens into bigrams\n        tokens = nltk.bigrams(tokens)\n\n    # Creating Data Frame\n    freq = nltk.FreqDist(tokens) # display(freq)\n    df = pd.DataFrame.from_dict(freq, orient='index')\n    df.columns = ['Frequency']\n    df.index.name = 'Term'\n\n    # Here we calculate the total number of tokens in our Frequency List\n    total_tokens = sum(freq.values()) # sum([2,3,4,5,6])\n\n    # Here we add a new column `Relative` (*100 for percentage)\n    df['Relative'] = (df['Frequency'] / total_tokens) * 100\n\n    return df\n\n\nfrom pathlib import Path\nimport os\n\n#@markdown Do you want bigrams included?\nbigrams = True #@param {type:\"boolean\"}\n\n#@markdown Should all words get lower cased before counting the occurances?\nlowercasing = True #@param {type:\"boolean\"}\n\n#@markdown Do you want to exclude stopwords in your result list?\nstopwords = True #@param {type:\"boolean\"}\n\n#@markdown Do you want to remove punctuation before counting the occurances?\npunctuation = True #@param {type:\"boolean\"}\n\n\n# Load stopwords file if necessary\nif stopwords:\n    stopwords = sw_json.json()\n\n# Read source file and concat all texts\ntext = ' '.join(list(df[text_column]))\n\n# Call word_freq() with specified parameters\ndf_freq = word_freq(text, punctuation = punctuation, stop_words = stopwords, lowercasing = lowercasing, bigrams = bigrams)\n\n# Sort results for descending values\ndf_freq = df_freq.sort_values(\"Relative\", ascending = False)\n\ndisplay(df_freq[0:10])\n\n\n  \n    \n\n\n\n\n\n\nFrequency\nRelative\n\n\nTerm\n\n\n\n\n\n\n(j√ºdisches, leben)\n5\n1.259446\n\n\n(allerheiligen, allerseelen)\n4\n1.007557\n\n\n(ilse, aigner)\n3\n0.755668\n\n\n(bayerischer, landtag)\n3\n0.755668\n\n\n(klare, haltung)\n2\n0.503778\n\n\n(w√ºnschen, einfach)\n2\n0.503778\n\n\n(vaters, freundschaftliche)\n2\n0.503778\n\n\n(tod, vaters)\n2\n0.503778\n\n\n(g√ºnter, tod)\n2\n0.503778\n\n\n(schwiegervater, g√ºnter)\n2\n0.503778\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWordcloud\nOne way to visualise word frequencies and recurring themes of texts are word clouds. These basically show the most frequently occurring words in the text (similar to the table created earlier), but more frequently occurring words are depicted larger than less frequently occurring words.\nFirst, we have to install the necessary library wordcloud.\n\n!pip install -q wordcloud\n\nThe actual implementation of this approach is relatively simple. We need to combine all the texts into a single text, as we did in the previous step with the frequency analysis, and pass it to the imported library.\n\nfrom wordcloud import WordCloud, STOPWORDS\n\ndef generate_wordcloud(text, path):\n\n    text = ' '.join(list(text))\n\n    # Generate a word cloud image\n    wordcloud = WordCloud(background_color=\"white\",width=1920, height=1080).generate(text)\n\n    # Dazugeh√∂rige Grafik erstellen\n    plt.imshow(wordcloud, interpolation=\"bilinear\") # Aufl√∂sung/Interpolation der Grafik\n    plt.axis(\"off\")\n    plt.figtext(0.5, 0.1, wordcloud_subcaption, wrap=True, horizontalalignment='center', fontsize=12)\n    plt.savefig(path, dpi=300)\n    plt.show()\n\nOnce again, we have the option of adjusting various parameters. Remember to specify the right file path, file name and column of your text data!\n\n#@markdown Input for additional stopwords; whitespace separated\nstopwords_extension_wc = '' #@param {type: \"string\"}\n\n#@markdown Subcaption for the wordcloud, leave blank to ignore\nwordcloud_subcaption = 'Markus S\\xF6der' #@param {type: \"string\"}\n\nNow all we have to do is load the stop word file, add our own additions and then trigger the creation of the word cloud using the function we created at the beginning.\nThe result image is saved in the defined data_path.\n\nimport matplotlib.pyplot as plt\nimport requests\n\n# Retrieve Stopwords from Github\nr = requests.get('https://github.com/stopwords-iso/stopwords-de/raw/master/stopwords-de.json')\nstop_words = r.json()\n\n# Convert input into list\nstopwords_extension_wc_list = stopwords_extension_wc.split(' ')\nstop_words.extend(stopwords_extension_wc_list)\n\n# Stopw√∂rter in die WordCloud laden\nSTOPWORDS.update(stop_words)\n\n\ngenerate_wordcloud(df[text_column], 'wordcloud.png')"
  },
  {
    "objectID": "notebooks/2023_12_11_GPT_Text_Classification4.html",
    "href": "notebooks/2023_12_11_GPT_Text_Classification4.html",
    "title": "Run the extraction of multiple variables.",
    "section": "",
    "text": "system_prompt = \"\"\"\nYou're an expert in detecting calls-to-action (CTAs) from texts.\n**Objective:**\nDetermine the presence or absence of explicit and implicit CTAs within German-language content sourced from Instagram texts such as posts, stories, video transcriptions, and captions related to political campaigns from the given markdown table.\n**Instructions:**\n1. Examine each user input as follows:\n2. Segment the content into individual sentences.\n3. For each sentence, identify:\n   a. Explicit CTA: Direct requests for an audience to act which are directed at the reader, e.g., \"beide Stimmen CDU!\", \"Am 26. September #FREIEW√ÑHLER in den #Bundestag w√§hlen.\"\n   b. Explicit CTA: A clear direction on where or how to find additional information, e.g. \"Mehr dazu findet ihr im Wahlprogramm auf fdp.de/vielzutun\", \"Besuche unsere Website f√ºr weitere Details.\"\n   c. Implicit CTA: Suggestions or encouragements that subtly propose an action directed at the reader without a direct command, e.g., \"findet ihr unter dem Link in unserer Story.\"\n4. Classify whether an online or offline action is referrenced.\n5. CTAs should be actions that the reader or voter can perform directly, like voting for a party, clicking a link, checking more information, etc. General statements, assertions, or suggestions not directed at the reader should not be classified as CTAs.\n5. Return boolean variables for Implicit CTAs (`Implicit`), Explicit CTAs (`Explicit`), `Online`, and `Offline` as a JSON objet.\n**Formatting:**\nOnly return the JSON object, nothing else. Do not repeat the text input.\n\"\"\"\n\nThe following code snippet uses my gpt-cost-estimator package to simulate API requests and calculate a cost estimate. Please run the estimation whne possible to asses the price-tag before sending requests to OpenAI!\nNote: This code block adds some logic to deal with multiple variables contained in the JSON object: {\"Implicit\": false, \"Explicit\": false, \"Online\": false, \"Offline\": false}. We add the columns Implicit, Explicit, Online, and Offline accordingly. To classify different variables the code need to be modified accordingly. ChatGPT can help with this task!\nFill in the MOCK, RESET_COST, SAMPLE_SIZE, COLUMNS and MODEL variables as needed (see comments above each variable.)\n\nfrom tqdm.auto import tqdm\nimport json\n\n#@markdown Do you want to mock the OpenAI request (dry run) to calculate the estimated price?\nMOCK = False # @param {type: \"boolean\"}\n#@markdown Do you want to reset the cost estimation when running the query?\nRESET_COST = True # @param {type: \"boolean\"}\n#@markdown Do you want to run the request on a smaller sample of the whole data? (Useful for testing). Enter 0 to run on the whole dataset.\nSAMPLE_SIZE = 5 # @param {type: \"number\", min: 0}\n\n#@markdown Which model do you want to use?\nMODEL = \"gpt-3.5-turbo-0613\" # @param [\"gpt-3.5-turbo-0613\", \"gpt-4-1106-preview\", \"gpt-4-0613\"] {allow-input: true}\n\n#@markdown Which variables did you define in your Prompt?\nCOLUMNS = [\"Implicit\", \"Explicit\", \"Online\", \"Offline\"] # @param {type: \"raw\"}\n\n# This method extracts the four variables from the response.\ndef extract_variables(response_str):\n    # Initialize the dictionary\n    extracted = {}\n\n    for column in COLUMNS:\n      extracted[column] = None\n\n    try:\n        # Parse the JSON string\n        data = json.loads(response_str)\n\n        for column in COLUMNS:\n          # Extract variables\n          extracted[column] = data.get(column, None)\n\n        return extracted\n\n    except json.JSONDecodeError:\n        # Handle JSON decoding error (e.g., malformed JSON)\n        print(\"Error: Response is not a valid JSON string.\")\n        return extracted\n    except KeyError:\n        # Handle cases where a key is missing\n        print(\"Error: One or more keys are missing in the JSON object.\")\n        return extracted\n    except Exception as e:\n        # Handle any other exceptions\n        print(f\"An unexpected error occurred: {e}\")\n        return extracted\n\n\n# Initializing the empty column\nif COLUMN not in df.columns:\n  df[COLUMN] = None\n\n# Reset Estimates\nCostEstimator.reset()\nprint(\"Reset Cost Estimation\")\n\nfiltered_df = df.copy()\n\n# Skip previously annotated rows\nfiltered_df = filtered_df[pd.isna(filtered_df[COLUMN])]\n\nif SAMPLE_SIZE &gt; 0:\n  filtered_df = filtered_df.sample(SAMPLE_SIZE)\n\nfor index, row in tqdm(filtered_df.iterrows(), total=len(filtered_df)):\n    try:\n        p = row['Text']\n        response = run_request(system_prompt, p, MODEL, MOCK)\n\n        if not MOCK:\n          # Extract the response content\n          # Adjust the following line according to the structure of the response\n          r = response.choices[0].message.content\n          extracted = extract_variables(r)\n\n          for column in COLUMNS:\n            df.at[index, column] = extracted[column]\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        # Optionally, handle the error (e.g., by logging or by setting a default value)\n\nprint()\n\nReset Cost Estimation\nCost: $0.0191 | Total: $0.0838\n\n\n\n\n\n\ndf[~pd.isna(df['Implicit'])]\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nshortcode\nText\nText Type\nPolicy Issues\nCall\nImplicit\nExplicit\nOnline\nOffline\n\n\n\n\n442\n442\nCxxXJBtAHhv\nFriedrich Merz ist nicht gerade bekannt f√ºr se...\nCaption\n['Asylbewerberleistungsgesetz', 'Zahnsanierung...\nNone\nFalse\nFalse\nFalse\nFalse\n\n\n453\n453\nCxvqTwmtlJK\nDamit es uns nicht so ergeht wie den Indianern...\nCaption\nNaN\nNone\nFalse\nTrue\nFalse\nTrue\n\n\n494\n494\nCxs9ujENMqI\nüîπ#Krankenh√§userüîπ#Geburtsstationen und üîπ#Hebamm...\nCaption\n['Krankenh√§user', 'Geburtsstationen', 'Hebamme...\nNone\nFalse\nTrue\nFalse\nTrue\n\n\n839\n839\nCxWF0mcqrhg\nUnterwegs im oberbayerischen Moosburg: Herzlic...\nCaption\nNaN\nNone\nFalse\nTrue\nFalse\nTrue\n\n\n1818\n1818\nCxvKsBBos0j\n9801 Bayerische Staatsregierung MISSION 7272 9...\nOCR\nNaN\nNone\nFalse\nFalse\nFalse\nFalse"
  },
  {
    "objectID": "notebooks/2023_12_11_GPT_Text_Classification2.html",
    "href": "notebooks/2023_12_11_GPT_Text_Classification2.html",
    "title": "Run the Few-Shot request.",
    "section": "",
    "text": "system_prompt = \"\"\"\nYou are an advanced classifying AI. Your task is to classify the sentiment of a text. Sentiment can be either ‚Äòpositive‚Äô, ‚Äònegative‚Äô, or ‚Äòneutral‚Äô.\n**Examples:**\n\"Wir sind EIN Volk! üá©üá™ In Leipzig nahm es den Anfang, breitete sich aus wie ein Lauffeuer und ebnete den Weg f√ºr die deutsche Einheit. Was damals viel Arbeit war, zahlte sich aus. Was heute noch Arbeit ist, wird sich auszahlen. Ein geeintes Deutschland ist keine Selbstverst√§ndlichkeit und wir sind dankbar f√ºr die Demokratie, den Rechtsstaat und unsere freiheitliche Gesellschaft. Und wir arbeiten t√§glich daf√ºr, dass uns diese Werte erhalten bleiben.\": positive\n\"FREIE W√ÑHLER Wir FREIE W√ÑHLER k√§mpfen f√ºr eine fl√§chendeckende Gesundheitsversorgung auch auf dem Land. HUBERT AJUANGER\": neutral\n\"Die #Gr√ºnen sind mit daf√ºr verantwortlich, dass die #Ampel-Regierung in Berlin meilenweit an der Lebenswirklichkeit der Menschen vorbei regiert. Ausgerechnet unter einem gr√ºnen Klimaminister l√§sst die Akzeptanz f√ºr #Klimaschutz in der Gesellschaft nach. Mit uns wird es keine Gr√ºnen in der Bayerischen Staatsregierung geben.\": negative\n\"\"\"\n\n\nprompt = \"\"\"\nPlease classify the following social media comment into either ‚Äònegative‚Äô, ‚Äòneutral‚Äô, or ‚Äòpositive‚Äô. Your answer MUST be one of [‚Äònegative‚Äô, ‚Äòneutral‚Äô, ‚Äòpositive‚Äô], and it should be presented in lowercase.\nText: [TEXT]\n\"\"\"\n\nThe following code snippet uses my gpt-cost-estimator package to simulate API requests and calculate a cost estimate. Please run the estimation whne possible to asses the price-tag before sending requests to OpenAI! Make sure run_request and system_prompt are defined before this block by running the two blocks above (see Setup for GPT)!\nFill in the MOCK, RESET_COST, COLUMN, SAMPLE_SIZE, and MODEL variables as needed (see comments above each variable.)\n\nfrom tqdm.auto import tqdm\n\n#@markdown Do you want to mock the OpenAI request (dry run) to calculate the estimated price?\nMOCK = False # @param {type: \"boolean\"}\n#@markdown Do you want to reset the cost estimation when running the query?\nRESET_COST = True # @param {type: \"boolean\"}\n#@markdown What's the column name to save the results of the data extraction task to?\nCOLUMN = 'Sentiment' # @param {type: \"string\"}\n#@markdown Do you want to run the request on a smaller sample of the whole data? (Useful for testing). Enter 0 to run on the whole dataset.\nSAMPLE_SIZE = 25 # @param {type: \"number\", min: 0}\n\n#@markdown Which model do you want to use?\nMODEL = \"gpt-3.5-turbo-0613\" # @param [\"gpt-3.5-turbo-0613\", \"gpt-4-1106-preview\", \"gpt-4-0613\"] {allow-input: true}\n\n\n# Initializing the empty column\nif COLUMN not in df.columns:\n  df[COLUMN] = None\n\n# Reset Estimates\nCostEstimator.reset()\nprint(\"Reset Cost Estimation\")\n\nfiltered_df = df.copy()\n\n# Skip previously annotated rows\nfiltered_df = filtered_df[pd.isna(filtered_df[COLUMN])]\n\nif SAMPLE_SIZE &gt; 0:\n  filtered_df = filtered_df.sample(SAMPLE_SIZE)\n\nfor index, row in tqdm(filtered_df.iterrows(), total=len(filtered_df)):\n    try:\n        p = prompt.replace('[TEXT]', row['Text'])\n        response = run_request(system_prompt, p, MODEL, MOCK)\n\n        if not MOCK:\n          # Extract the response content\n          # Adjust the following line according to the structure of the response\n          r = response.choices[0].message.content\n\n          # Update the 'new_df' DataFrame\n          df.at[index, COLUMN] = r\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        # Optionally, handle the error (e.g., by logging or by setting a default value)\n\nprint()\n\nReset Cost Estimation\nCost: $0.0010 | Total: $0.0278\n\n\n\n\n\n\ndf[~pd.isna(df['Sentiment'])].sample(5)\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nshortcode\nText\nText Type\nPolicy Issues\nSentiment\n\n\n\n\n1833\n1833\nCxunhdYNvw3\ntanten\nOCR\nNaN\nneutral\n\n\n2299\n2299\nCxJAr3Ht7mh\nEIN JAHR FEMINISTISCHE REVOLUTION IM IRAN LASS...\nOCR\nNaN\nneutral\n\n\n369\n369\nCx2gzYdIv5d\nWir gratulieren Sven Schulze, der gestern in M...\nCaption\nNaN\npositive\n\n\n1886\n1886\nCxqbrYztMdC\nBerliner Senat; nachdem er rausgefunden hat, d...\nOCR\nNaN\nnegative\n\n\n290\n290\nCx7ruIdiOXb\n#TagderdeutschenEinheit \\n\\nUnser #Bayern hat ...\nCaption\n['LosvonBerlin', 'Bayernpartei']\nnegative\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# Save Results\ndf.to_csv('/content/drive/MyDrive/2023-12-01-Export-Posts-Text-Master.csv')\n\n\nimport matplotlib.pyplot as plt\n\n# Count the occurrences of each sentiment\nsentiment_counts = df['Sentiment'].value_counts()\n\n# Create a bar chart\nsentiment_counts.plot(kind='bar')\n\n# Adding labels and title\nplt.xlabel('Sentiment')\nplt.ylabel('Count')\nplt.title('Sentiment Counts')\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "notebooks/2023_12_01_GPT_Text_Exploration.html",
    "href": "notebooks/2023_12_01_GPT_Text_Exploration.html",
    "title": "Data Import",
    "section": "",
    "text": "Using GPT for Information Extraction\nThe focus of this chapter lies in demonstrating how GPT can be employed in a loop to analyze text documents. This methodology aligns with the principles of topic modeling but extends further by leveraging the advanced capabilities of the language model. Our approach involves the iterative processing of text, where GPT aids in identifying, categorizing, and interpreting the underlying themes and sentiments expressed in social media texts.\nThe GPT application presents a significant difference compared to traditional topic modeling. While topic modeling often aims to automatically uncover hidden thematic structures within a text corpus, our approach with GPT is based on a different assumption: We presuppose that there is already a specific theme or a particular question in mind according to which we want to organize and analyze the documents. This approach allows us to navigate through the vast amounts of text in social media in a targeted and efficient manner, identifying specific insights and patterns that are directly related to our predefined areas of interest.\nThe following workflow outlines how we could use this information extraction process to create a topic list. Using the list we can classify each document.\n\n\n\nAn example for a GPT based ‚ÄúTopic Modeling‚Äù approach. I have used this approach in a current research project, the process is not perfect yet.\n\n\n\n!pip install -q openai backoff gpt-cost-estimator\n\n\n\nSetup for the OpenAI API\nWe‚Äôre using the new Colab Feature to store keys safely within the Colab Environment. Click on the key on the left to add your API key and enable it for this notebook. Enter the name fpr your API-Key in the api_key_name variable below.\n\nimport openai\nfrom openai import OpenAI\nfrom google.colab import userdata\nimport backoff\nfrom gpt_cost_estimator import CostEstimator\n\napi_key_name = \"openai-lehrstuhl-api\"\napi_key = userdata.get(api_key_name)\n\n\n# Initialize OpenAI using the key\nclient = OpenAI(\n    api_key=api_key\n)\n\n\n\n@CostEstimator()\ndef query_openai(model, temperature, messages, mock=True, completion_tokens=10):\n    return client.chat.completions.create(\n                      model=model,\n                      temperature=temperature,\n                      messages=messages,\n                      max_tokens=600)\n\n# We define the run_request method to wrap it with the @backoff decorator\n@backoff.on_exception(backoff.expo, (openai.RateLimitError, openai.APIError))\ndef run_request(system_prompt, user_prompt, mock):\n  messages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": user_prompt}\n  ]\n\n  return query_openai(\n          model=\"gpt-3.5-turbo-0613\",\n          temperature=0.0,\n          messages=messages,\n          mock=mock\n        )\n\nNext, we create a system prompt describing what we want to extract. For further examples of prompts and advice on prompt engineering see e.g.¬†the prompting guide and further resources linked at the bottom of the page.\nFor the initial example we use social media content shared by politicans and parties. We know, that some of these texts mention policy issues, let‚Äôs try to extract these issues across all documents.\nNote: The extracted issues are not going to be consistent, because each document is sent as a singular request to the API, thus the previous issues are not going to be used as context.\nModify the following system prompt to extract other types of information. What else could you extract?\n\nLocations (based on names)\nNames (of persons or places)\nMentions of Companies\n‚Ä¶\n\nDo not forget the Prompt Archive when experimenting. Share your successfull prompt with us!\n\nsystem_prompt = \"\"\"\nYou are a helpful assistant, an expert for German politics.\n**Objective:** Extract policy issues from German language social media texts. Policy issues refer to specific topics or subjects that are the focus of public or governmental debate, analysis, and decision-making. Elections themselves and party slogans or their performance are no policy issues.\n**Instructions:** Return each policy issues referenced in user message as a comma-seperated list. Return 'None' if no policy issues are referenced.\n**Formatting:** Return a comma-seperated list.\n\"\"\"\n\n\n\nRunning the request.\nThe following code snippet uses my gpt-cost-estimator package to simulate API requests and calculate a cost estimate. Please run the estimation whne possible to asses the price-tag before sending requests to OpenAI! Make sure ‚Äòrun_request‚Äô and ‚Äòsystem_prompt‚Äô are defined before this block by running the two blocks above!\nSet the following variables:\n\nMOCK: Do you want to mock the OpenAI request (dry run) to calculate the estimated price?\nRESET_COST: Do you want to reset the cost estimation when running the query?\nCOLUMN: What‚Äôs the column name to save the results of the data extraction task to?\nSAMPLE_SIZE: Do you want to run the request on a smaller sample of the whole data? (Useful for testing). Enter 0 to run on the whole dataset.\n\n\nfrom tqdm.auto import tqdm\n\nMOCK = True\nRESET_COST = True \nCOLUMN = 'Policy Issues'\nSAMPLE_SIZE = 0 \n\n# Initializing the empty column\nif COLUMN not in new_df.columns:\n  new_df[COLUMN] = None\n\nif RESET_COST:\n  # Reset Estimates\n  CostEstimator.reset()\n  print(\"Reset Cost Estimation\")\n\nfiltered_df = new_df.copy()\n\n# Skip previously annotated rows\nfiltered_df = filtered_df[pd.isna(filtered_df['Policy Issues'])]\n\nif SAMPLE_SIZE &gt; 0:\n  filtered_df = filtered_df.sample(SAMPLE_SIZE)\n\nfor index, row in tqdm(filtered_df.iterrows(), total=len(filtered_df)):\n    try:\n        response = run_request(system_prompt, row['Text'], MOCK)\n\n        if not MOCK:\n          # Extract the response content\n          # Adjust the following line according to the structure of the response\n          r = response.choices[0].message.content\n\n          # Convert the string 'r' to a list if it's not 'None', otherwise keep it as None\n          if r != 'None':\n              r = r.split(', ')\n          else:\n              r = None\n\n          # Update the 'new_df' DataFrame\n          new_df.at[index, 'Policy Issues'] = r\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        # Optionally, handle the error (e.g., by logging or by setting a default value)\n\nprint()\n\nReset Cost Estimation\n\n\n\n\n\n\n# Save Results\nnew_df.to_csv('/content/drive/MyDrive/2023-12-01-Export-Posts-Text-Master.csv')\n\nNext we create a set of Policy Issues. Sets are similar to lists in that they are used to store multiple items, but each unique item in a set appears only once, regardless of how many times it is added, as sets inherently enforce uniqueness and do not allow duplicates. Unlike lists, sets are unordered, meaning they do not record element position or order of insertion. This property makes sets highly efficient for checking membership and eliminating repeated entries. We create the list policy_issues to generate a word cloud.\n\nunique_policy_issues = set()\npolicy_issues = []\n\nfor issues in new_df['Policy Issues']:\n    if issues is not None:\n        unique_policy_issues.update(issues)\n\n        for issue in issues:\n            policy_issues.append(issue)\n\nprint(unique_policy_issues)\n\n{'faires Verfahren', 'Stra√üen', 'EU-Kommission', 'Zusammenhalt unserer Gesellschaft', 'migrationswende', 'bayernliebe', 'Thema Migration nicht unkontrolliert weiterlaufen lassen', 'Sprache', 'FREIHEITf√ºrBayern', 'Freistaat Bayern', 'rezession', 'Hilfe', 'Mehrwertsteuer', 'tv', 'L√ºgen', 'Bayern stark und stabil bleibt', 'Baumfreifl√§chen', 'BAf√∂G-H√∂chstsatz', 'Online Antrag', 'Zur√ºckweisungen an Binnengrenzen', 'Autoverbot', 'wohnungskonzerne enteignen', 'Infrastrukturausbau', 'Reformierbarkeit des √∂ffentlich-rechtlichen Rundfunks', 'bayerische Antidiskriminierungsstelle', 'Baden-W√ºrttemberg', 'Strompreiszonen', 'Streit und Chaos', 'linke Inhalte', 'Soldatenberuf', 'bezahlbare Energie', 'Verkaufszahlen bei Heizungssystemen', 'Forschung und Entwicklung', '√ñlheizungen', 'Situation an den Aussengrenzen', 'Leitungen', 'Verbotsorgien', 'Kriminalit√§t', 'CDU/CSU', 'Sprachen und Kultur', 'deutsche Universit√§t', 'bezahlbares Bayern f√ºr alle', 'Souver√§nit√§t', 'wiedervereinigung', 'Landwirte', 'Rechtsstaat', 'moderne Lernumgebung', 'Iran', 'Hebammen', 'Tierschutz', 'Baumwipfelpfad', 'politische Einfl√ºsse', 'bayrisches Brauchtum', 'Mitbestimmung', 'Linksextremisten', 'landtagbayern', 'BayernsOpposition', 'ddr', 'ltw', 'Parteidisziplin', 'Planungs- und Genehmigungsprozesse', 'Wirtschaft f√∂rdern', 'Energiepreis', 'Personalschl√ºssel', 'Bestandsgeb√§ude', 'gleichwertige Lebensverh√§ltnisse', 'Energiepreise', 'Sparen', 'bezahlbarer Wohnraum f√ºr alle', 'Jugendzentren', 'Wassersparen', 'niedrigere Standards', 'Sorgen und N√∂te', 'Migrationswende', 'junge M√§nner', 'Haus- und Fach√§rzte', 'Landwirtschaftspolitik', 'unserezukunft', 'Faschist*innen', 'wegenmorgen', 'konservative Werte', 'innenminister', 'Politik', 'Bundeswehr', 'Startchancen f√ºr Kinder', 'Kulturlandschaft', 'umsteuern', 'Klimaschutzziele erreichen', 'F√ºhrerscheinrichtlinie', 'Kita-Betreuungspl√§tze', 'politischer Wechsel', 'Kaufnebenkosten', 'Obergrenze f√ºr Bargeldzahlungen', 'heimatliebe', 'Zivilist*innen', 'Glaubw√ºrdigkeit', 'Mehrwertsteuer f√ºr die Gastronomie', 'unkontrollierte Zuwanderung', 'LOSvonBerlin', 'Anti-Atom-Ideologie', 'Ausbau der Erneuerbaren Energien', 'Mehr Platz f√ºr das Rad und die √ñffis', 'Biodiversit√§t', 'linke Medien', 'afdw√§hlen', 'Sozialwohnungen', 'bayerischerlandtag', 'Grenzen', 'Migrationskatastrophe', 'rechtsradikale Parteien', 'Ego-Show', 'Bildungswende', 'Fach- und Erg√§nzungskr√§fte', 'Terroristen der Hamas', 'anpackenstattankleben', 'Nachtfahrverbot', 'unsereverantwortung', 'illegale Einwanderung', 'Hotel', 'erfolg', 'Deutschlandticket', 'Unterschlei√üheim', 'Wirtschafts- und Standortpolitik', 'Startchancen-Programm', 'Personal- und Sachmittel', 'Landtag Bayern', 'Grenzschutz', 'Dampfheizkraftwerk', 'Gefl√ºchtete', 'Bayernpartei', 'Exportwirtschaft', 'Rechte der B√ºrger', 'Zivilgesellschaft', 'Ernsthaftigkeit', 'B√ºrgerlicheMitte', 'pseudowissenschaftliche Gender-Ideologie', 'sichere Herkunftsstaaten', 'haust√ºrwahlkampf', 'Bayern lebt es sich einfach besser', 'Wahlklatsche', 'Mittelstand', 'Diesel', 'F√∂rderk√ºrzungen', 'Deutschlandpakt', 'Stigmatisierung', 'Befreiung land- und forstwirtschaftlicher Fahrzeuge von der KfZ-Steuer', 'Mobilit√§tsmix', 'Innere & transnationale Sicherheit', 'LOSvonRom', 'Hetze', '8Oktober', 'MINT-F√§cher', 'Stillstand', 'Beteiligung', 'Bevormundung aus Berlin', 'Finanzierungen', 'Technologieoffenheit bewahren', 'illegale Migration', 'islamistischer Terror', 'Auto-Verbot', 'oberfranken', 'Ampel-Regierung', 'GesunderMenschenverstand', 'Perspektiven', 'Arztsitze', 'Terrorangriffe der Hamas auf Israel', 'SPD', 'Bau einer weiteren Betonpiste', 'Bezirksvorsitzender f√ºr Unterfranken', 'sommertour', 'Sach- statt Geldleistungen', 'deutschegeschichte', 'ehrenamt', 'Sprachfeststellungstests', 'Klarheit bei Heizungsgesetz', 'opposition', 'bayerischen Steuerzahler entlasten', 'Kinderbetreuung', 'station√§re Versorgung', '√úberforderung', 'LTWBayern', 'Selbstbestimmungsgesetz', 'Interessen', 'landespolitische Fragen', 'Ausl√§nder', 'Unternehmenssteuern', 'CO2-Maut f√ºr LKW', '√ñffentlich-rechtlicher Rundfunk', 'Bezirkstage', 'missionunion', 'Abgabe', 'Weltoffenheit', 'Waldbauern', 'Elster Zertifikat', 'Hohenwart', 'Meinungen aus', 'Terrororganisation', 'Aufbaugeneration', 'rechtskonform', 'Meisterausbildung', 'digitale Strategie', 'wichtige Themen', 'energiewende', 'Briefwahl', 'Umweltschutz', 'christlich', 'Kaputt gesparte Kommunen', 'Lebensmittel- und Energiepreise', 'kommende Wahlen', 'Chancengerechtigkeit', 'Entlastung', 'Geldleistungen', 'Bezahlbare Wohnungen und faire Mieten f√ºr 7 Mio. Mieter', 'Unterst√ºtzung', 'Wirtschaftswachstum', 'Heizungsverbote', 'Haushaltspolitik', 'j√ºdischer Staat', 'Wasserstoff-Region', 'Flurneuordnung', 'eigene Zukunft', 'Fachkr√§fte', 'Kontrolle', 'Biomassedeckel', 'Verkehr', 'Mehrwertsteuer auf Grundnahrungsmittel', 'Zuwanderung', 'Zahnsanierungen', 'soziale Leistungen', 'Entwicklungshilfe', 'Benachteiligungen des l√§ndlichen Raums', 'Digitale Bildung', 'Chancenbereitung', 'Abschiebung', 'Einfluss der ideologisierten und politisierten ‚ÄûKlimaforschung‚Äú', 'LosvonBerlin', 'Tanz', 'Unterst√ºtzung f√ºr Familien', 'Weltsicht', 'Kahlschlag in der Krankenhauslandschaft', 'Heizgesetz', 'Klimasparbuch', 'Zahnarzt', 'grundsatzprogramm', 'St√§dten', 'Steuergeschenke f√ºr Konzerne', 'Gesundheitspolitik', 'steuerliche Entlastungen', 'Anpacken', 'Wiedereinzug in den Bayerischen Landtag', 'Teamgeist', 'Numerus Clausus', 'Inklusion', 'Verteidigungspolitik', 'Heimatliebe', 'ausr√ºsten', 'Ehrenamtlicher Einsatz', 'gef√§hrdungslage', 'Gender', 'Beschr√§nkungen der Bargeldnutzung', 'Stabilit√§t', 'M√ºnchen', 'Dr. Markus B√ºchler', 'bevormundung', 'selbstbestimmtes Europa', 'Asylbewerberleistungsgesetz', 'wirtschaftspolitik', 'Waldstilllegungen', 'beschr√§nkt g√ºltiger F√ºhrerschein ab 60', 'Bayerns W√§lder', 'Gr√§uel', 'Abschieben', 'islamistischer Terror der Hamas', 'Arbeit', 'MWST in der Gastronomie', 'bezahlbar', '8. Oktober', 'Katastrophenschutz', 'Nahverkehrsangebote', 'Mitmach-Aktionen', 'zivilgesellschaftliche Kr√§fte', 'Fernsehbosse', 'Brenner-Nordzulauf', 'Einigkeit', 'Gewalt', 'Parteiarbeit', 'Abtreibungen', 'Tanken', 'Privateigentumsschutz', 'politische und gesellschaftliche (Fehl-)Entwicklungen', 'Expertenkommission', 'Bayern selbst entscheiden k√∂nnen', 'Ideen', 'aktueller Rechtsruck', 'Asyl-Lobbyisten', 'Biotechnologie', 'starke Stimme', 'Klimaschutz', 'Kontoverbindung', 'bezahlbares Wohnen', 'b√ºrgerliche Koalition', 'Klimawandel', 'Migranten', 'ungewollt schwangere Frauen', 'NGO', 'zeitgem√§√üe F√ºhrung', 'gew√∂hnlicher Aufenthalt', 'Tourismus', 'B√ºrger', 'Interessen Italiens', 'Terror', 'Mietpreissteigerungen', 'Impfzwang', 'Batteriespeicher', 'Kahlschlag', 'L√§rm', 'fr√ºhzeitiges Erlernen der deutschen Sprache', 'Bayern liebt', 'internationale Gemeinschaft', 'Maximilianeum', 'EUCH bewegen', 'Ausbildungsreform f√ºr die Kinderpflege', 'innovative Bauverfahren', 'Ehrenamt', 'Bildungsoffensive f√ºr Deutschland', 'antisemitische Sachverhalte', 'wirtschaft', 'Zusammenarbeit mit dem Regime', 'Pharmazie', 'mittelfranken', 'Abschiebungen', 'Ampel-Chaos', 'gesellschaftliche Unterst√ºtzung', 'Frieden in Europa', '√úbernahme nach dem Studium', 'Stimmung gegen√ºber Gefl√ºchteten', 'Miete', 'Natur erhalten', 'Familien mit Kindern', 'BayernZuerst', 'Bezahlbare Wohnungen und faire Mieten', 'Mitarbeitenden', 'digitale Beantragung', 'Opfer rechter Gewalt', 'Wasserversorgung', 'Pflegegeld', 'Wohnungsoffensive', 'F√ºrDieZukunft', 'Gesetz', 'praktische √úbungsstunde', 'Verkehrstr√§ger', 'Ideologiefreiheit an den Hochschulen', 'Jugendparlament', 'neonazistische Gruppen', 'Hessenweiterf√ºhren', 'Fahrverbote', 'Sachleistungen', 'Rechtsrutsch', 'Schutz vor Gefahren', 'Landwirtinnen', 'CO2-Bepreisung', 'Modlareuth', 'Bayernwahl', 'Weniger B√ºrokratie und eine digitale Verwaltung', 'Petition', 'Herausforderungen unserer Zeit', 'deutsche Richterbund', 'ltwby23', 'Klimafreundliches Heizen', 'Kunst', 'zweite Legislatur', 'Gesundheitswesen', 'H2-Heizung', 'K√ºnstliche Intelligenz', 'Universit√§t', 'Migrationsabkommen', 'E-Fuels', 'Politiker', 'bayerische St√§dtebauf√∂rderung', 'Rassismus', 'w√§hlherzstatthetze', 'L√§nderfinanzausgleich reformieren', 'Medizin', 'Krimbr√ºcke', 'landtagswahl', 'Wasserstoff massiv ausbauen', 'F√ºhrerschein f√ºr schwere PKWs', 'demokratische Werte', 'politisch Verfolgte', 'Verantwortung', 'Grenzpolizei', 'Leistung', 'Gute Pflege', 'recht', 'Leistungen', 'gr√ºne Klimakonto', '√Ñrmsten der Bev√∂lkerung', 'gute Schulen', 'Gefl√ºchtetenpolitik', 'Soldatischer Dienst', 'Stolz', 'Elterngeld', 'regionale Spezialit√§ten', 'soziale Sicherheit', 'klare Positionen', 'LKW', 'eigenst√§ndiges Fahren ab 16 Jahren', 'Bayerns Opposition', 'Klimakollaps', 'Tempolimit', 'einheit', 'CO2-Bilanz', 'heimat', 'friedliches Europa', 'EU-Au√üengrenzen', 'Gr√ºnen', 'Glasindustrie', 'ErbschaftsteuerAbschaffen', 'pro-pal√§stinensische Terrororganisationen', 'Finanzierung', 'Umwelt', 'Schutz von J√ºdinnen und Juden', 'Krankenhausreform', 'EU-Eliten', 'Windkraftausbau', 'DIELINKE', 'Bildungs- und Betreuungseinrichtungen', 'Einheimische Bev√∂lkerung', 'steigende Mieten', 'Kostenlosen und ticketfreien √ñPNV', 'kleidung', 'Wochenmarkt', 'Verantwortungsvolle Regierung', 'unabh√§ngige Justiz', 'Solarenergie-Anlage f√ºr Balkon oder Dach', 'Pflege', 'H√ºrden', 'Mehrwertsteuer auf Speisen in der Gastronomie', 'kathaunterwegs', 'Motor f√ºr Deutschland und Europa', 'Integration', 'Lehrst√ºhle', 'Wiedervereinigung', 'M√§nnern und Kindern in Israel', 'Asylrecht aush√∂hlen', 'Bildungsangebote von Verb√§nden', 'kostenloser Nahverkehr', 'H√ºrden f√ºr den F√ºhrerscheinerwerb', 'individuelle Mobilit√§t', 'Halbzeitbilanz der Ampel', 'Gewerbe', 'Planungshoheit der L√§nder', 'Riedenburg', 'Geb√§udeenergiegesetz', '√∂konomie', 'Diskriminierung bei Nichtverwendung der ‚ÄûGendersprache‚Äú', 'Aiwanger', 'Sonderaufnahmeprogramme', 'Landtagswahlen', 'Taurusmarschflugk√∂rper', 'Partei der Mitte', 'fr√ºhkindliche Bildung', 'Geiseln', 'Mobilit√§t f√ºr alle', 'Vergesellschaftung gro√üer profitorientierter Wohnungskonzerne', 'Apotheke', 'politische Bildung', 'Landr√§te', 'Wirtschaftlichkeit', 'Absenkung der Mehrwertsteuer', '√ñkomodellregionen', 'Leichenmisshandlung', 'R√ºckf√ºhrungsabkommen', 'Unsicherheiten f√ºr Studierende', 'Sympathiekundgebungen f√ºr den Terror in Israel', 'bayerischerrundfunk', 'Corona-Bu√ügelder', 'Parteien', 'Dorferneuerung', 'Kernfusion', 'Werte', 'Krankenhausversorgung auf dem Land', 'Sauerlach', 'B√ºrgerinteresse', 'Bayern wird Wasserstoffland Nummer 1', 'Krankenh√§user', 'mauerfall', 'Genehmigungsprozesse', 'Schwarz-Gr√ºn', 'Atomkraftwerken', 'Hofsterben', 'Ausl√§nder-Gewalt', 'Versorgung', 'Drittstaatsangeh√∂rige', 'Festung Europa', 'Unterfranken', 'Heimat', 'nat√ºrlicher Rohstoff Holz', 'L√∂sungen', 'Abgabelast pro gefahrenen km', 'gegen gr√ºne Ideologie', 'Gegenpositionen zum herrschenden Zeitgeist', 'Hilfe und Arbeitsmigration', 'Sitzen bleiben', 'Familienbetriebe st√§rken', 'Landtagskandidat*innen', 'fossile Energietr√§ger', 'europ√§ische Regelung', 'CSU-Versprechen im Wahlkampf', 'pal√§stinensische Terroristen', 'Gr√ºne', 'Klimakonto', 'Menschenrechtsverletzungen', 'Verkehrsbelastung', 'autorit√§re Gesundheitspolitik', 'Israel', 'wirtschaftliche Entwicklung des Freistaates', 'staatliche Verwaltung', 'tradition', 'gestiegene Kosten f√ºr Heizung', 'Zukunftsvertrag zur Landwirtschaft in Bayern', 'K√ºrzungshammer', 'Soziale Politik', 'gr√ºne', 'Maghreb-Staaten', 'F√∂rderrunde', 'gesellschaftliches Wohlergehen', 'Bewusstsein', 'Gewinnung von Arbeits- und Fachkr√§ften', 'Bandenkriminalit√§t', 'Stellenabbau', 'F√∂rdergelder', 'Pflegeversorgung in der Heimat sicherstellen', 'Richtungsentscheidung', 'bezahlbare Wohnungen', 'Gelder f√ºr Freiwilligendienste', 'bayerische Volkspartei', 'Geburtsstationen', 'landtag', 'Bayerntour', 'konsequente R√ºckf√ºhrung krimineller Straft√§ter', 'Gesundheitsversorgung', 'Bus und Bahn', 'Schweden', 'Regeln', 'kostenlose Kitas', 'Wahl am 8.10.', 'bayerische Grenzpolizei', 'Populistische Politik', 'Klimakleber', 'Leichensch√§ndigung', 'vereint', 'EU-Sanktionen', 'Innere Sicherheit', 'konservative Opposition', 'Breitbandversorgung', 'Lebensqualit√§t', 'Ausbildungskosten', 'bezahlbarkeit', 'Vertrauen in demokratische Institutionen', 'Minderheit im eigenen Land', 'Wasserstoffdrehkreuz', 'CO2-Preis', 'Erdgasheizungen', 'Erbschaftsteuer', 'Abkommen', 'landtagswahlen', 'Mobilfunk', 'Fakten', 'Lebensverh√§ltnisse', 'Bayernliebe', 'DRG-Fallpauschal-Finanzierung abschaffen', 'Parteivorsitzende', 'Demokrat*innen', 'Demonstranten', 'Investition in Ausstattung der Schulen', 'EU stoppen', 'russischer Angriffskrieg gegen die Ukraine', 'politische Gefangene', 'FREIHEITf√ºrBAYERN', 'Wirtschaftspolitik', 'Forschung an KI', 'geb√ºhrenfreie Kitas', 'Gesundheit', 'Familienpolitik', 'Ideologie', 'Handeln', 'Jugend', 'Rechtsstaatlichkeit', 'Startchancen', 'sinkender Strompreis', 'kostenfreie Bildung', 'Forschung', 'Gefahrenstellen', 'Schule und Berufsleben', 'Kommunen', 'Haft', 'Schlaganfall-Versorgung', 'straff√§llig', 'Heizungsgesetz', 'Grenze', 'ltwby', 'Selbstbestimmung', 'Geld', 'tagderdeutscheneinheit', 'l√§ndliche R√§ume', 'Respekt', 'B√ºrgerenergie-Genossenschaften', 'Antisemitismus', 'EU-Asylkompromiss', 'Automobilindustrie', 'Markus S√∂der und die CSU', 'Immobilienhaie', 'Kandidierenden', 'Sanierungsbedarf', 'Kurs', 'Vernichtung', 'Wissenschaft', 'Kitapl√§tze', 'Wirtschafts- und Sozialpolitik', 'Rente', 'EWERG eG', 'Gr√ºne raus aus der Regierung', 'Strompreise von Umlagen und Steuern', 'Arbeitsvertrag', 'Politik f√ºr die eigenen Leute', 'PolitikF√ºrUnsereZukunft', 'Beseitigung von Weltraumschrott', 'Rundfunkrat', 'k√ºnftige Generationen', 'GEAS', 'Bezahlung in der Pflege', 'Spaltung', 'Verbeamtung', 'international', 'n√ºrnberg', 'Strompreise', 'faire Bezahlung von Pflegekr√§ften', 'rechte Politik', 'Wirtschaftszweig', 'Kandidatinnen und Kandidaten', 'Satellitendaten', 'Verkehrsentlastung', 'Nationaler Sicherheitsrat', 'begleitetes Fahren', 'starkes und bezahlbares Bayern', 'Migrationspolitik', 'NGOs', 'Umwidmung von Parkpl√§tzen', 'heimische obst- und nahrungsmittel', 'Seenotrettung', 'Betonfundamente', 'CSU', 'Technologie-Offenheit', 'Datenschutz', '√∂ffentlich-rechtlichen Medien', 'Erhalt der heimischen Lebensmittelproduktion', 'politikmitverstand', 'Landwirtschaft', 'Nationale Raumfahrtstrategie', 'Hightech', 'Arbeitslosenquote', 'Artenvielfalt', 'Kassenleistungen', 'Wirtschaft', 'Wohlstand', 'PIN', 'Zukunftskurs', 'einheitliche Regelungen', 'EU', 'Wohnung', 'staatliche Betriebskostenf√∂rderung', 'neoliberale Wirtschafts- und Finanzpolitik', 'antisemitische Einstellungen', 'Lebensmittel', 'Dorfentwicklung', 'Wasserstoff', 'CO2-Bindung', 'Biomasse', 'handlungsf√§higer Staat', 'Verg√ºtung', 'starke Bildungspolitik', 'Judenhass', 'Privatsph√§re', 'Wasserstoff-Tankstelle', 'Schienen', 'sauberes und bezahlbares Zuhause', 'Geschichte', 'bayerisches Familiengeld', 'Betriebskostenf√∂rderung', 'gemeinn√ºtzige Arbeit', 'LTWBy', 'L√§rmschutz', 'F√∂rderung von Ideen zur Verbesserung von Unterricht und Schule', 'Elektrolyseur', 'Kostenlose Kitas', 'versorgungsrelevant', 'WHO', 'HolDirDeineZukunftZur√ºck', 'Rundfunkbeitrag', 'Photovoltaik', 'Gemeinschaftsschule', 'Mieten', 'Bau- und Wohnwirtschaft', 'Energie', 'Abgase', 'Wissenschaftsfreiheit', 'Terror der Hamas', 'BAf√∂G Reform', 'Gedichte', 'Machen statt Niedermachen', 'H√ºterin der B√ºrgerrechte', 'Chancen', 'Tariftreue-Paket', 'fl√§chendeckende Notfallversorgung', 'Umweltfreundliche Mobilit√§tsformen st√§rken', 'Kliniken', 'Bildungssystem', 'Feindbilder', 'Landes-Antidiskriminierungsgesetz', 'Radio', 'Abrechnung der Arztkosten', 'Anma√üungen des EuGH', 'Eigentum', 'Eltern- und Sch√ºler*innenvertretungen', 'Windkraft', 'Senioren', 'Ungleichheit', 'Liberalismus in Europa', 'Ausbau des mobilen Internets', 'Lauterbach', 'Abwehr dieses Terrorangriffs', 'Politik f√ºr Leistung und Eigentum', 'teambayern', 'Verbindungsachsen', 'exzellente Ausbildung', 'Demokratie-Dialog', 'Wasserstoffnetz', 'krise', 'Wohnraum', 'politik', 'Energieversorgung', 'Eigentum in Familienbesitz sch√ºtzen', 'FreistaatBayern', 'praktische Berufe', 'Werkswohnungen', 'Europawahl 2024', 'Quiz-Spiel', 'Nancy Faeser', 'GemeinsamStark', 'Haft f√ºr Schutzsuchende', 'Ordnung und Sicherheit', 'Lieferung von schweren Waffen', 'Biotech-Standort', 'Grenzschutzoffensive Bayern', 'sittenwidrig', 'Energiewende', 'Abh√§ngigkeit vom Ausland', 'Arbeit im Rentenalter', 'Engagement f√ºr Demokratie', 'queere Menschen', 'Holzwachstum', 'Durchforstungsholz', 'Ingolstadt', 'Pflegekrise', 'Grundnahrungsmittel', 'Versorgungssicherheit', 'attraktive Bedingungen f√ºr deutsche Weltraumunternehmen', 'Bildungspolitik', 'Effizienz', 'Stra√üengro√üprojekt', 'Demoskopen', 'Mutter', 'LTW', 'linke', 'Pflegegesellschaft', 'Rechtsterrorismus', 'Hass und Hetze', 'Kartellamt', 'Zwang', 'Rundfunkgeb√ºhren abschaffen', 'Arbeitsmarktpolitik', 'Merz', 'Patient', 'Partei', 'Krisen', 'GR√úN', 'heimatmitherz', 'Frieden', 'b√ºrokratischer Mehraufwand', 'Netto', 'bezahlbare Lebensmittel', 'Desinformation', 'Schulen', 'bildungsgerechtigkeit', 'Freilassung aller Geiseln', 'Oberbayern', 'MedizinischeVersorgung', 'Energie-Mix', 'Lebenshaltungskosten', 'Postleitzahl', 'Geothermie', 'Bayern', 'Wirtschaftsfreundlichere Rahmenbedingungen', 'starke Wirtschaft und bezahlbare Energie', 'regierung', 'geschichte', 'Wille', 'Europawahlen', 'ltw2023', 'Anstalten', '2023', 'Lieferkettengesetz', 'Verwaltung', 'Bildung und Forschung', 'sexuelle Gewalt', 'HerzStattHetze', 'Manifest f√ºr Freiheit in Europa', 'Fremdbestimmung', 'Steuersenkungen', 'Menschen in Israel', 'Schwaben', 'rechte Gewalt', 'Mietpreisbremse', 'w√ºrdevoll', 'EnergiewendeMitVerstand', 'L√§ndlichen Raum st√§rken', 'CDUParteitag', 'Zukunftsvertrag f√ºr die Landwirtschaft', 'DeineStimmeZ√§hlt', 'Gegenwehr gegen√ºber einer √ºbergriffigen EU-B√ºrokratie', 'Bruder', 'freistaatbayern', 'B√ºrgergeldreform', 'Familie', 'st√§ndige Hetze von S√∂der', 'F√∂rderung innovativer Start-ups', 'demokratie', 'W√§rmewende', 'rechte Szene', 'gute Pflege', 'Friedensbewegung', 'Windr√§der', 'Prinzipien', 'T√§tern', 'Verbindungen', 'Sturm', 'Tauruslieferung', 'Kernkraft', 'Kleinkraftr√§der', 'soziale Gerechtigkeit', 'Hass und Antisemitismus', 'Drogenlegalisierung', 'einigkeit', 'klimaschutz', 'radikale Bewegungen in √ñsterreich und Deutschland', 'wiederverwenden', 'Familien', 'Mangel an Kita- und Pflegepl√§tzen', 'Kraft der Vernunft', 'faire Mieten', 'Land Israel', 'Region', 'Rundfunk', 'Heizungspolitik', 'Migrationskrise', 'DIE LINKE', 'Sozialepolitik', 'Wohnungsbau', 'Fl√ºchtende', 'Bewirtschaftete W√§lder', 'gute L√∂hne', 'Ganztagspl√§tze', 'sicherer Strom', 'Grenzen kontrollieren', 'Gegenrechts', 'Chatkontrolle', 'Technologieoffenheit', 'Menschen in Armut', 'Weiden', 'F√∂rderung des √ñkolandbaus und der Biologischen Vielfalt', 'Europ√§ische Staaten', 'Innenministerin Faeser', 'Zugangscode', 'Strompreis', 'Forschungspolitik', 'Mullah Regime', 'Verteidigung Israels', 'Situation in den Aufnahmekommunen', 'Klimakrise', 'Opfer', 'Ausbildungsst√§tte', 'Rechtsmittel', 'Streit', 'Bildungsprotest2023', 'heimatbayern', 'katrinebnersteiner', 'Kraft', 'Forschungsbedingungen', 'Verwaltungsrat', 'EU-G√§ngelung', 'Impuls', 'Produktionsverlagerung', 'Industrie', 'kriminelle Ausl√§nder', 'Staatswald', 'Nazis', 'sachsenanhalt', 'Babys in Bayern', 'Spitzenkandidat', '√úberl√§nge von 16,50 m auf 17,40 m zulassen', 'Einwanderungs- und Asylpolitik', 'Spritpreisbremse', 'F√∂rdermittelk√ºrzungen', 'gemeinsame Sprache', 'Immatrikulation', 'U18-Wahl', 'mobilit√§t', 'None', 'Kinder- und Jugendplan', 'Krisenverordnung', 'Pflegekonzepte', 'B√ºrokratieabbau', 'BayernSPD', 'bildung', 'Tradition', 'oberbayern', 'w√§hlen', 'Zugang zur Justiz', 'W√§lder', 'Gericht', 'Beste Bildung und weniger Unterrichtsausfall', 'Visionen', 'Terrorism', 'Asylrecht', 'Lernmittelfreiheit', 'Pr√§ventionsangebote', 'schlechter √ñPNV', 'Studis', 'humanit√§re Verantwortung', 'Ampelpl√§ne', 'Asylpolitik', 'br', 'Mopedf√ºhrerschein', 'Abschaffung der ungerechten Erbschaftssteuer', 'Demokratief√∂rderung', 'Land√§rzte', 'traditionelle Studieng√§nge', 'Enteignung', 'AfD-Wahlergebnisse', 'Holz√∂fen', 'Nationale Sicherheit', 'bayerische Bezirkstage', 'Bildungswende jetzt', 'Erbschaftssteuer', 'Volksentscheid', 'Landtags- und Bezirkstagswahl', 'Intoleranz', 'Abschaffung des Asylrechts', 'deutschland', 'ErbschaftssteuerAbschaffen', 'Gerechtigkeit', 'Gr√∂√üenwahn', 'Gewalt und Terror in Israel', 'Familienkasse', 'Atomwaffen', 'Wahl', 'GegenRechts', 'Wochen', 'R√ºcktritt', 'bayerische Staatsangeh√∂rigkeit', 'ehemalige SED-Partei', 'Wohnen', 'Mitgliedsstaaten', 'Kultursommer mit Links', 'arbeitspl√§tze', 'Kernkraftwerke', 'CSU Parteivorstand', 'Terrorismus', 'Anti-Demokrat*innen', 'Sorgen der B√ºrger ernst nehmen', 'bayerische Interessen im Bund und in Europa', 'die das Klima sch√ºtzt', 'soziale Politik f√ºr Bayern', 'Tarifbindung', 'B√ºrger*innen-Energiegenossenschaft', 'Abschaffung der CO2-Steuer', 'linksextremistischen Gruppen', 'bayerische Staatlichkeit', 'EEG-F√∂rderung', 'extremisten', 'Leistung und Eigentum', 'Durchhalteverm√∂gen', 'oberpfalz', 'gerechter Freistaat', 'Verbrechen an unschuldigen Frauen', 'Grundsicherung', 'Verl√§sslichkeit und Kompetenz statt Beliebigkeit und Populismus', 'Los von Berlin', 'ambulante Anlaufstellen', 'AKW-Verteufelung', 'rechnen', 'Herangehensweisen', 'Bezahlbares Wohnen f√ºr 7 Mio. Mieter', 'Existenzrecht des j√ºdischen Staates Israel', 'staatliche Grundfinanzierung von Universit√§ten und Hochschulen ohne ideologische Vorgaben', 'St√§rke', 'Kraftstoff', 'Wasserstoffinfrastruktur', 'vern√ºnftige Mitte', 'Vertr√§ge mit Staaten in Nordafrika und T√ºrkei', 'Landtagswahlen in Hessen und Bayern', 'Tempolimit auf Autobahnen', 'p√§dagogische Qualit√§t von Kitas', 'Gewerkschaften', 'Polizei', 'Russland', 'FlurNatur-Struktur und Landschaftselemente', 'Arbeitsmigration', 'Verbrennungsmotoren', 'Arbeits- und Fachkr√§fte', 'B√ºrokratie', 'Kostenlose Kitas f√ºr 780 000 Kinder', 'schlechte Bildung', 'Enteignen', 'Steuers√§tze', 'soziale Probleme', 'Einreisekontrolle an den EU-Au√üengrenzen', 'israelische St√§dte und D√∂rfer', 'FDP', 'Rechtspopulismus', 'Krieg', 'Steuermodelle', 'Schule', 'alleinerziehend', 'buntes Kinderprogramm', 'VPN', 'Anstand', 'Staatsanwaltschaften', 'Arbeitskr√§ftemangel', 'LandtagBayern', 'Verteilungsfragen', 'Souver√§nit√§tsverlust', 'Menschenrechte', 'Oberpfalz', 'Einkommensteuer', 'kostenfreie Kitas', 'lebenswertes Bayern', 'Investitionen in die Zukunft', 'sichere Stromversorgung', 'Staatsr√§son', 'sozialepolitikf√ºrdich', 'Gute Pflege f√ºr 2,7 Mio. Senioren', 'Anerkennung', 'Kinder', 'ingolstadt', 'schlanker und effizienter Staat', 'F√∂rderung', 'ausbau', 'Anpacken f√ºr Bayern', 'Pers√∂nlichkeitsrechte', 'Zuwanderungspolitik', 'Umweltsch√ºtzer', 'erneuerbare Energien', 'Krankenhaus', 'Deutsche Stromkunden', 'Lehrer', 'Antragsprozess', 'Italianisierung', 'Gegenrassismus', 'inklusives Bildungssystem', 'Stromversorgung', 'Bundesregierung', 'Krankenhaus-Milliarde', 'MitDir', 'heimatschutz', 'Die Konfrontation', 'Toleranz', 'Freibetr√§ge', 'fernsehen', 'freiberufliche Apotheken', 'Leerstandsabgabe', 'Stromleitungen', 'Zahlen', 'Freiheitf√ºrS√ºdTirol', 'lpt2023', 'Deutschland-Pakt gegen unkontrollierte Zuwanderung', 'Lehrerinnen', 'Mehrsprachigkeit', 'Digitalministerin', 'Vereine und das Ehrenamt st√§rken', 'anpacken', 'Bevormundung', 'Identit√§t und Nation', 'Kostenlose Bildung', 'Biotopen', 'Sprit sparen', 'Regierungsform', 'Katharina und Ludwig', 'Versorgungsstra√üen', '√ñPNV', 'Blockabfertigungen', 'Lohnersatzleistungen f√ºr pflegende Angeh√∂rige', 'Ladenschlussgesetz', 'Einb√ºrgerung', 'sozialpolitik', 'Rathausplatz', 'Belebung von Ortszentren und Dorferneuerung', 'Mietendeckel', 'FREIE W√ÑHLER', 'Privatversicherte', 'Standort', 'Wiedervereinigung Deutschlands', 'Mindestlohn', 'R√ºckf√ºhrung von kriminellen Straft√§tern', 'Insolvenzen', 'Mehr Personal und bessere Zusammenarbeit und Vernetzung', 'Einstellung von Richtern und Staatsanw√§lten', 'vereinbarkeit', 'Gendern', 'Erhaltung von D√∂rfern', 'Kinderzukunftsprogramm', 'Pelletheizung', 'Sicherheitsvorkehrungen', 'gute Bildung', 'Gesellschaft', 'landwirtschaft', 'Einzelleistungsverg√ºtung', 'Verdoppelung Kapazit√§t', 'Habeck', 'Naturpark', '√úbergriffigkeiten der EU-Eliten', 'Druck', 'Hausbesitzer', 'Verkehrspolitik', 'Gastronomie', 'starke Wirtschaft', 'Agieren und Finanzierung pal√§stinensischer und propal√§stinensischer Terrororganisationen', 'kostenlosem √ñPNV f√ºr Kinder und Jugendliche', 'Ganzjahrestourismus', 'Schule f√ºr alle', 'Erdbeobachtungen', 'Vergesslichkeit', 'Pessimismus', 'Online Ausweis', 'klimaneutraler Wohnraum', 'Kinder und Jugendliche', 'innovativ', 'Ausbau der Windkraft', 'Landespflegegeld', 'besseres Europa', 'Sozialpolitik', 'Asyl', 'ampel', 'individuelle F√∂rderung', 'BP', 'wahlprogramm', 'Menschen mit Behinderung', 'LINKE', 'internationale Wettbewerbsf√§higkeit', 'mangelndem Wohnraum', 'Kinder und Jugendliche mit Migrationshintergrund', 'Wasserstoff-Gipfel', 'fl√§chendeckend', 'junge Gr√ºne Abgeordnete', 'Herausforderungen in der Migrationspolitik', 'Zwangsimpfungen', 'Freiheit f√ºr Bayern', 'F√∂rderungen f√ºr Holz- und Pellets-Heizungen', 'Kindergrundsicherung', 'bayerische Arbeitspl√§tze', 'Innovation', 'Naturschutz', 'sozialer Aufstieg', 'aktuelle Lage', 'Mittelmeer', 'Gelder', 'innenministerherrmann', 'CO2-Einsparung im deutschen Strommix', 'gr√ºne AKW-Heuchelei', 'Mobilit√§t egal wo du hin willst', 'Italien', 'Deutschland', 'bayern', 'Berliner Senat', 'Gesundheitsreform', 'kostenloses Mittagessen', 'sauberer Strom', 'progressive', 'Mehrwertsteuer in der Gastronomie', 'Steuererh√∂hung', 'Finanzierung islamistischer Organisationen', 'Staat Israel', 'Begleitetes Fahren ab 15 Jahren', 'Ignoranz', 'Musik', 'kostenfreier Schulweg', 'Windrad', 'Kapazit√§ten', 'energie', 'konfrontation', 'Stromerzeugung', 'Studieren', 'wissenschaft', 'Menschen vor Ort', 'Haushaltsmittel zur Kofinanzierung der Gemeinschaftsaufgabe Agrarstruktur und K√ºstenschutz', 'Jugendhaus', 'Streuobstpakt', 'freiheit', 'staatlich subventionierter Industriestrompreis', 'Freie W√§hler', 'Medienbildung', 'Antisemitismus-Beauftragter der Bayerischen Staatsregierung', 'Amberg', 'Bev√∂lkerung', 'Rechtsextremismus', 'Trinkwasserschutz', 'dritter Nationalpark', 'Digitalisierung', 'Fachsch√ºler*innen', 'Asylsystem', 'den Gr√ºnen und den linkslastigen Medien', 'Vielfalt', 'Humanit√§t', 'Augsburg', 'neue Stromleitungen', 'Jom Kippur', 'Prost', 'station√§re Grenzkontrollen', 'gr√ºnklingelt', 'nachhaltigere Raumfahrt', 'Hackschnitzel', 'kostenloser Meister', 'kostenlosen √ñPNV', 'optimistische zukunftsorientierte Politik', 'Sparerpauschbetrag', 'Bargeldnutzung', 'Soziale Gerechtigkeit', 'M√§rkte', 'bayerisches F√∂rderprogramm', 'Umgang mit Unternehmen', 'h√∂herer Mindestlohn', 'Lebensgrundlagen', 'Handwerk', 'Kooperationsvertr√§ge mit der Bundespolizei', 'Fl√§chenverbrauch', 'gleiche Chancen', 'psychische Gesundheit', 'Steuersenkung', 'Ausbildung', 'Grundwasserschutz', 'Institutionen', 'national strukturierteres Abschiebeverfahren', 'angriff', 'Oberfranken', 'Kommunale Krankenh√§user erhalten', 'Anti-Auto-Haltung', 'R√ºckf√ºhrung', 'Wohnungsnot', 'finanzielle F√∂rderung von Grundschulen', 'kostenfreie Meisterausbildung', 'Berlin', 'medizinische Versorgung', '√∂kologie', 'Modernit√§t', 'Innenentwicklung und die Vermeidung von Fl√§chenverbrauch', 'brauchtum', 'Propagandafernsehen', 'Zeltlager', 'jungeunion', 'Mobilit√§tswende', 'Nutzung', 'Sozialdemokratie', 'Bayerischer Landtag', 'Genehmigungen', 'Soziale Politik F√ºr Dich', 'Fachkr√§ftemangel', 'Drogenkonsumr√§ume', 'dezentrale Bevorratung in Bayern und Deutschland', 'unterfranken', 'Geburtshilfe', 'Azubis', 'Fallpauschalensystem', 'gute Pflege f√ºr 2,7 Mio. Senioren', 'Kitas', 'Druck auf die Ampel', 'rechtsrutschstoppen', 'Abdeckungs-Offensive', 'Kommerzialisierung', 'Solaranlage', 'rechte Ausschreitungen', 'bezahlbares Bayern', 'Landschaftswasserhaushalt', 'Bildungsorganisationen', 'echte Beteiligung', 'Mauerfall', 'Kampf gegen Rechts', 'Konkurrenz', 'AnpackenF√ºrBayern', 'moedlareuth', 'klimasch√§dlicher Flugverkehr', 'Finanzen', 'Wahlalter 16', 'wichtige soziale Themen', 'Tempolimit auf deutschen Autobahnen', 'Umweltzerst√∂rungen', 'Vorhaben', 'fachkr√§ftemangel', 'Stellenwert in der Gesellschaft', 'Weltraummanagement', 'Fichtenbestand', 'Ver√§nderungen', 'Dieselfahrverbot', 'Umwelt- und Naturschutz', 'Energie- und Industriepolitik', 'Technik', 'Ticketfreiheit', 'Unterst√ºtzung der Schulen bei der Umsetzung von Programmen', 'Chancengleichheit', 'Erh√∂hung der LKW Maut', 'Europawahlprogramm', 'Schutz der Zivilbev√∂lkerung', 'Schleuserkriminalit√§t', 'selbstverwaltete Justiz', 'EU-Gerichtshof', 'Verbotspolitik', 'LTWby23', 'Antragsberechtigung', 'generationengerechte Politik', 'demokratieverteidigen', 'Todesstrafe', 'J√§hrlichen Stellenaufbau bis 2029 verl√§ngern', 'Holzheizung', 'klarer Kurs', 'Exekutive', 'Asyl-Migration', 'Schutzgrund', 'Recht', 'Nationalismus', 'Wende in der Migrationspolitik', 'Stromnetzausbau', 'Brauchtum', 'lesen', 'Unterdr√ºckung im Iran', 'Waldsch√§dlinge', 'Technologie', 'Ferienangebote', 'Landes- und B√ºndnisverteidigung', 'AUSSENGRENZEN', 'Entf√ºhrungen', 'Tag der Deutschen Einheit', 'Markus S√∂der', 'herrmann', 'Benzin', 'Bayern-Energie', 'Klimaneutralit√§t', 'Politsystem', 'Autonomie f√ºr S√ºd-Tirol', 'Erzeugerpreise', 'deutsche Staatsb√ºrgerschaft', 'Heizen', 'inneresicherheit', 'Brandmauern', 'B√ºrgergeld', 'bezahlbare Mieten', 'Arbeitnehmerrechte', 'B√ºrgerinnen und B√ºrger', 'CO2', 'Gr√ºnen Partei', 'Kinderhaus', 'v√∂lkerrechtswidriger Angriff', 'Student', 'Zinsen', 'bezahlbarer Wohnraum', 'Wohnungsmangel', 'Safe Abortion Day', 'Einkommen', 'Diskutieren wir', 'Europ√§ische Kommission', 'freistaat', 'Frieden und Freiheit', 'Kulturkampf', 'Spoken words', 'Produktion ins Ausland', 'M√∂dlareuth', 'Einheit', 'tvtipp', 'l√§ndlicher Raum', 'angehobene Altersgrenzen', 'Schulsozialarbeit', 'Schutz', 'allestimmengr√ºn', 'zukunft', 'Migration', 'Gr√ºnen wollen das ganze Land bevormunden', 'Grundschule', 'zweitehand', 'Massenmigration', 'Anpacken f√ºr unsere B√ºrger', 'organisiertes', 'B√ºrgerrechte', 'CO2-neutraler Kraftstoff', 'zielgerichtete Leistungen', 'Ampel', 'Life Science Campus', 'Steuerfreibetr√§ge im Monat pro Arbeitnehmer auf 2000 Euro', 'europ√§ische Zukunft', 'schwaben', 'R√ºckf√ºhrungen', 'FSJ-Pl√§tze', 'Wasserkraft', 'Gr√ºne in der Landesregierung', 'Rechtsruck', 'Selbstbewusstsein', 'gemeinsames Lernen', 'D√ºrre', 'Wohnsitz', 'sicherheit', 'Wirtschaftsstrompreis', 'Heimatbewusstsein', 'Bodentruppen', 'DeutscheGeschichte', 'Bus', 'Verbote', 'Abschaffung Erbschaftssteuer', 'F√∂rderung der l√§ndlichen Entwicklung', 'Krankenhausversorgung', 'Legislative', 'Existenzrecht Israels', 'regensburg', 'durchgr√ºntes Berlin', 'Finanzierung des √ñPNVs', 'Heimatvertriebene', 'chrupalla', 'LTW23', 'Landschaftspfleger', 'Sommer', 'illegale Einreisen', 'heimische Energiewelt', 'Neonazi-Strukturen', 'Demokratiebildung', 'bundesweite Grenzpolizei', 'Deregulierung', 'Inflation', 'Zukunftsfinanzierungsgesetz', 'Herz statt Hetze', 'Bezahlbare Energie', 'gesellschaftliche Teilhabe', 'volle Unterst√ºtzung f√ºr die Ukraine', 'bayerische Interessen', 'Elternhaus', 'bessere Taktung', 'Abgaben', 'antisemitische Propaganda', 'l√§ndliche Krankenh√§user', 'Grundversorgung', 'Bayerns erneuerbare Energie', 'Entlastungen', 'Aufl√∂sung des √∂ffentlich-rechtlichen Rundfunks', 'gr√ºne Dogmen', 'effektiver Grenzschutz', 'Erdgas', 'Kriminelle Straftaten', 'Rendite', 'Regensburg', 'Diskriminierung', 'AfD-Erfolgswelle', 'Mobilit√§t', 'Erneuerbare', 'Windenergie', 'Kernwegenetzbau', 'Energieversorgung in Bayern', 'Vitalit√§t', 'staatliche Wohnheime', 'Kinder und Jugendliche in den Fokus', 'Engagement f√ºr die Heimat', 'L√∂hne', 'Hochschule', 'Revolutionsgarden', 'Weltfriedenstag', 'Covid-Ma√ünahmen', 'Bildung f√ºr Bayern', 'Barrierefreiheit', 'Zwangsgeb√ºhren', 'bezahlbare und saubere Energie', 'Baupolitik', 'Wahnsinn des Nationalsozialismus', 'Genuss', 'einseitigen W√§rmepumpen-Tr√§ume der Ampel', 'Selbstregierung', 'kostenlose Meisterausbildung', 'Hisbollah', 'Menschen mit Fluchtgeschichte', 'Zukunftsvertrag zwischen der Staatsregierung und dem Bayerischen Bauernverband', 'Mittelstand sch√ºtzen', 'Kernfusions-Kraftwerk', 'Erhalt aller Schulstandorte', 'Landtagswahl', 'legale Zuwanderung', 'Gerichte', 'L√§nderfinanzausgleich', 'rechts', 'Mut', 'LTWby2023', 'b√ºcher', 'Remigration', 'Bayerische Grenzpolizei', 'Freiheit', 'hohe Energiepreise', 'Klimakatastrophe', 'Existenz- und Altersabsicherung', 'Vernunft statt Ideologie', 'Zusammenleben', 'Asylantr√§ge', 'ortsnahe Versorgung', 'Kontinuit√§t', 'Zahlungen an Pal√§stinenser', 'kostendeckende Schulstarthilfe', 'Aufnahmestopp f√ºr junge M√§nner', 'Inntal', 'Mieterschutz', 'Normalverdiener', 'KEINE dritte Startbahn am Flughafen M√ºnchen', 'gendern', 'bayernsOpposition', '√Ñrztemangel', 'Zeitenwende', 'Haltung', 'LTW2023', 'Holzheizungen', 'Hilfe f√ºr Betroffene von Terrorismus', 'Sicherheit und Ordnung', '√úbergriffe des italienischen Staates', 'Online-Petition', 'Wirtschaftsminister', 'Theoriestunden', 'Schwangerschaftsabbr√ºche', 'Erneuerbare Energien', 'Terrorangriff der Hamas', 'Rechenschaft', 'br24wahl', 'St√§rkung von Landschaften', 'Studierende', 'Artenschutz', 'Grundwasser', 'Wasserschutz', 'Auflagen', 'Zusammenhalt', 'Situation der Studierenden in Bayern', 'LudwigUntrwegs', 'T√ºrkei', 'N√ºrnberg', 'Deutsche Einheit', 'Energiewende vor Ort', 'Hitze', 'Wertsch√§tzung f√ºr √§ltere Menschen', 'Bezahlbare Wohnungen', 'Wahlprogramm', 'Bildung f√ºr alle unsere Kinder', 'nachhaltigkeit', 'Vorschriften', 'starke Infrastruktur', 'Organisationsbereiche', 'Gazastreifen', 'Steigerwaldzentrum', 'Au√üenpolitik', 'saubere Energie', 'Renten', 'Keine Gr√ºnen in der Regierung', 'duales Studium als Bildungsweg st√§rken', 'L√∂hne in Ostdeutschland', 'augsburg', 'Medikamente', 'anf√§ngliche Fehler', 'Bund ID Konto', 'Unternehmen', 'Sicherheit bei Lebensplanung', 'Schnitzel', 'Mitglieder-Anteile', 'Richter', 'Stallbauvorschriften', 'Bildung unabh√§ngig vom Geldbeutel', 'Bayern bleibt', 'Pflegekr√§fte', 'remigration', 'Klima', 'deutschlandfest', 'Aufgaben unserer Zeit', 'fl√§chendeckende Gesundheitsversorgung', 'Chaos', 'Abschiebe-Zahlen', 'Ethos', 'Kampagne finanziert sich', 'Schutzversprechen f√ºr j√ºdisches Leben in Bayern', 'Energie und Treibstoffe', 'Steuerliche F√∂rderungen', 'Bauern', 'Lehrst√ºhle f√ºr ‚ÄûGenderforschung‚Äú', 'zukunftsf√§higes Bildungssystem', 'Wasserstoff-Land Nummer 1 werden', 'Multimillllion√§re', 'sozialpolitische Ma√ünahmen', 'W√§rmepumpe', 'brzahlbares Wohnen', 'Niederbayern', 'Wohnen als Grundrecht', 'B√ºrgermeister', 'Chancengerechtigkeit in Deutschland', 'Schutz des ungeborenen Lebens', 'Ukraine', 'den Automobilstandort Deutschland st√§rken', 'Reform des Gesundheitswesens', 'Energiegewinnung', 'Landkreis M√ºnchen', '√§rztliche Versorgung', 'Akutsprechstunden', 'K√ºrzungspolitik', 'Vertrauen', 'Entlastungen f√ºr Sparerinnen und Sparer', 'Wohnungs- und Mietmarkt', 'Misstrauen', 'Erbschaftssteuer abschaffen', 'Steuerpolitik', 'Volksbegehren', 'EEG-Umlage', 'Ausbildungs- oder Studienstart', 'Zusatzleistungen', 'Bavarian Fusion Cluster', 'Investorenbetriebene Medizinische Versorgungszentren (MVZ)', 'Spitzenmedizin', 'Wahlkampf', 'Kirchen', 'Energiekosten', 'Hessen', 'Verbrennerverbot', 'Marktwirtschaft', 'Landtags- und Kommunalwahlen', 'Stil', 'Verwaltungsdirektor', 'deutsche Staatsr√§son', 'Nationalsozialismus', 'Europa', 'Grundlastf√§higkeit', 'saubere und bezahlbare Energie', 'Bus und Bahn ins ganze Land bringen', 'Asylbewerber*innen', 'Hilfe und F√ºrsorge des Staates und der Gesellschaft', 'Hilfe in sozialen Fragen', 'Elektrolyseur-F√∂rderprogramm', 'Familiengeld', 'Opposition', 'niederbayern', 'Sicherheit des Staates Israel', 'F√ºhrungspositionen', 'Wohnungsgemeinn√ºtzigkeit', 'Medien', 'soziale Herkunft', 'Fernsehen', 'CDU', 'Anpacken statt Ank√ºndigen', 'GEZ abschaffen', 'Arbeitspl√§tze', 'soziale Politik', 'Wasserentnahme', 'Kinderarmut', 'Filz', 'saisonalit√§t', 'Gesellschaften', 'staatliche Energieunternehmen', 'Heimatenergie', 'S√ºden', 'kreislauf', 'Energiepolitik', 'Bildung f√ºr alle', 'Finanzminister', 'steigende Preise', 'LOSvonBERLIN', 'Wahlerfolge f√ºr Boris Rhein und die CDU Hessen', 'mittelschicht', 'Deckelung der Mieten', 'Mitte-rechts', 'volle Unterst√ºtzung f√ºr Israel', 'Benachteiligung von Menschen mit psychischen Problemen', 'TeamBayern', 'beste Bildung', 'Altparteien', 'n√§chste Wahlen', 'g√ºnstiger √ñPNV', 'CO2-freier Strom', 'Bezirkstag', 'Hamas-Terror', 'Schienenverkehr ausbauen', 'Heimische Produktion von Arzneimittel st√§rken', 'Anteile', 'St√§rkung der Demokratie', 'Ruhegehalt', 'Vergabeverfahren von Medizin-Studienpl√§tzen', 'DeshalbAfD', 'Energiesparen', 'Holznutzung', 'dezentrale Unterbringungen', 'BayerischerLandtag', 'Verh√§ltnis zur Partei', 'Identit√§t', 'Abw√§rme', 'Wohnraumf√∂rderung', 'Grenzschutzbayern', 'Bildung', 'Freie und selbstbestimmte Entscheidung der Patienten', 'junge Generation', 'politiker', 'klimaneutrales Wirtschaften', 'Auto', 'Arbeitszeitflexibilisierung', 'H2-Tankstellen', 'AfD verspricht', 'umsetzen', 'Allgemeinbildung', 'ltwbayern', 'Bezahlbare und saubere Energie', 'Einkommensunterschiede', 'infostand', 'forschung', 'unserlandzuerst', 'Migrationsstopp', 'Freiheit und weniger Kontrollen f√ºr bayerische Landwirte', 'Bundestag', 'Sprachkurse', 'Hessenwahl', 'afd', 'gute Ganztagsbetreuung', 'Stromsteuer', 'Hamas', 'primetime', 'Jugendherberge', 'FREIEW√ÑHLER', 'Veranstaltung', 'wehrhafte Demokratie', 'Ursula von der Leyen', 'Mehr Kontrolle und Steuerung bei der Migration', 'Freistaat', 'Angriffe', 'Mieterinnen und Mieter', 'Abschottungswahnsinn', 'Medizinische Versorgung', 'Solidarit√§t', 'Borkenk√§fer', 'Hightech Agenda Bayern', 'Erh√∂hung der Steuerfreibetr√§ge im Monat pro Arbeitnehmer auf 2000 Euro', 'Kommunen bei der Aufnahme von Gefl√ºchteten unterst√ºtzen', 'Gesetzesvolksentscheid', 'Selbstverteidigung', 'Chancenbudget', 'Transportwege', 'familienfreundlicher', 'Wahlniederlage', 'Bayerischer H√§rtefallfonds', 'm√ºnchen', 'Tempo 90 f√ºr Fahranf√§nger', 'Belastungs-Stopp', 'Stromspeicher', 'Seniorinnen und Senioren', 'Artenschwund', 'AfD', 'strukturschwachen Kommunen', 'Vereine', 'Festanstellung', 'Krankenhausinvestitionen', 'Zukunft', 'Gewalttaten', 'Hausbau', 'schlafen', 'Notendruck', 'afdbayern', 'Energieversorgung der Zukunft', 'Kraftstoffpreis', 'Solidarit√§t mit Israel', 'BAf√∂G', 'Strom und Lebensmittel', 'Cannabis-Legalisierung', 'n√§chste Landtagswahl', 'H√§user', 'Profite mit unserer Miete', 'Fr√ºhe Hilfen', 'Regierungsbildung', 'individuelle Mobilit√§t erm√∂glichen', 'LandtagswahlBayern', 'Bezahlbare und saubere Energie f√ºr Bayern', 'leistungsorientierte Bezahlung von Lehrkr√§ften', 'Kinder eine Zukunft', 'Investitionen', 'Durchforstung', 'Binnengrenzen', 'Radwege', 'Vernichtung von Kultur', 'Verkehrssicherheit', 'Unterschriften Sammeln', 'W√§rmenetz', 'Steuererleichterung f√ºr Agrardiesel', 'gr√ºne Politik', 'Wartezeiten f√ºr Therapiepl√§tze', 'Nahversorgung im l√§ndlichen Raum st√§rken', 'kompetente Politik', 'Urteile', 'Waffenrecht', 'illegale Zuwanderung', 'mangelnde Mobilit√§t', 'online', 'St√§rkung von Regionen', 'Spitzenkandidaten', 'hightech', 'Steuern', 'Au√üenwirtschaft', 'startups', 'Benutzername & Passwort', 'Mobilit√§t junger Menschen', 'illegaleMigration', 'gute gesundheitliche Versorgung', 'Direktkandidatin f√ºr den Bayerischen Landtag', 'm√∂rderischer Angriff auf unseren Bundesvorsitzenden', 'Bauvorschriften', 'Grenzkontrollen', 'Alternative zu Deutschland', 'Bescheid', 'Heizungs-Lotsen', 'Massenzuwanderung', 'Asylbewerber', 'steigende Mietpreise', 'Grenzsicherung', 'Rechtsruck in Bayern', 'kostenlose Kitapl√§tze', 'Claudia K√∂hler', 'gesellschaftlicher Zusammenhalt', 'Integrationsf√§higkeit', 'Milit√§r', 'selbstbestimmt', 'Demokratie', 'juxcsu', 'bayernwahl', 'Staatssekret√§r f√ºr Inneres', 'S√ºd-Tirol-Autonomie', 'Landtagswahl in Bayern', 'Verweisung nichtdeutscher Staatsb√ºrger', 'Armut', 'Mittelfranken', 'Studium', 'Hightech Agenda', 'Sicherheitsbeh√∂rden', 'politikmitherz', 'Windpark', 'Gesundheitsschutz', 'Wiedereinzug', 'Aktivrente', 'geringes Einkommen', 'ausl√§ndische Fachkr√§fte', 'Landtag', 'Terror stoppen', 'gerechte Politik', 'Erosionsschutz', 'Speicher', 'regionale Wertsch√∂pfung', 'Unterbringung', 'marktwirtschaftliches Gewissen', 'Austausch mit den B√ºrgerinnen und B√ºrgern', 'wohnen', 'Umbau zu stabileren W√§ldern', 'innenministerium', 'Bahn', 'Gesundheitsvorsorge', 'Entwicklung in Deutschland', 'Gemeinschaftsaufgabe ‚ÄûVerbesserung der Agrarstruktur und des K√ºstenschutzes‚Äú', 'Sicherheit', 'duales Studium verbessern', 'Integrationsgrenze', 'Ganztag', 'S√ºd-Tiroler Freiheit', 'CO2-Steuer abschaffen', 'Steuerverschwendung', 'Teambayern', 'Einwanderung', 'Kostenlose Kitas f√ºr 780.000 Kinder', 'interessierte B√ºrger', 'terroristischer √úberfall', 'Klimapolitik', 'israelisches Volk', 'Dreigliedriges Schulsystem'}\n\n\nLet‚Äôs quickly generate a wordcloud to check for patterns. See the Simple Corpus Analysis Notebook for more information.\n\n!pip install -q wordcloud\n\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport requests\n\n# Retrieve Stopwords from Github\nr = requests.get('https://github.com/stopwords-iso/stopwords-de/raw/master/stopwords-de.json')\nstop_words = r.json()\n\n# Stopw√∂rter in die WordCloud laden\nSTOPWORDS.update(stop_words)\n\ndef generate_wordcloud(text):\n    text = ' '.join(list(text))\n\n    # Generate a word cloud image\n    wordcloud = WordCloud(background_color=\"white\",width=1920, height=1080).generate(text)\n\n    # Dazugeh√∂rige Grafik erstellen\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.figtext(0.5, 0.1, \"Policy Issues\", wrap=True, horizontalalignment='center', fontsize=12)\n    plt.show()\n\ngenerate_wordcloud(policy_issues)\n\n\n\n\nNow we‚Äôre ready to pass the list to GPT to extract a manageable amount of topics. Note the list might be too long to fit into the GPT context window. In this case we have to split the list into several shorter lists and iterate over them.\nThis time we are not interested in a specific formatting for the response. We want to print the result for human interpretation.\n\nsystem_prompt = \"\"\"\nYou are a helpful assistant, an expert for German politics. Derive 15 topics of policy issues from this list of keywords provided by the user. Concentrate on overarching topics and avoid overlapping topics. Provide a set of 10 keywords per topic.\n\"\"\"\n\n\nkeyword_string = \", \".join(unique_policy_issues)\nresponse = run_request(system_prompt, row['Text'], False)\n\nCost: $0.0010 | Total: $0.3431\n\n\n\nprint(response.choices[0].message.content)\n\n1. Sicherheit\n- Polizei\n- Kriminalit√§t\n- Terrorismus\n- √úberwachung\n- Grenzkontrollen\n\n2. Wirtschaftswachstum\n- Industrie\n- Arbeitspl√§tze\n- Investitionen\n- Innovation\n- Export\n\n3. Arbeitslosenquote\n- Arbeitsmarkt\n- Arbeitslosengeld\n- Arbeitsvermittlung\n- Qualifikationen\n- Arbeitslosenversicherung\n\n4. Regierung\n- Politik\n- Parteien\n- Regierungsbildung\n- Koalitionen\n- Opposition\n\n5. Bayern-Power\n- Regionalpolitik\n- Infrastruktur\n- Bildung\n- Kultur\n- Tourismus\n\n6. Ampel-Frust\n- Politikverdrossenheit\n- Koalitionsstreitigkeiten\n- Stillstand\n- Kompromisse\n- Unzufriedenheit\n\n7. Familiengeld\n- Familienpolitik\n- Kinderbetreuung\n- Elternzeit\n- Kindergeld\n- Unterst√ºtzung\n\n8. Pflegegeld\n- Pflegepolitik\n- Altenpflege\n- Pflegeversicherung\n- Pflegeheim\n- Angeh√∂rigenpflege\n\n9. Meisterausbildung\n- Berufsausbildung\n- Fachkr√§ftemangel\n- Handwerk\n- Aufstiegschancen\n- Weiterbildung\n\n10. Briefwahl\n- Wahlrecht\n- Wahlbeteiligung\n- Demokratie\n- Wahlkampf\n- Stimmabgabe"
  }
]